{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modules\n",
    "\n",
    "In PyTorch, most of what you need to define your neural network is inside `torch.nn`. The central class for representing neural networks that is equipped with the necessary methods for automatic differentiation and training neural networks is the `torch.nn.Module` class. All pre-defined modules (also called layers, blocks, models, etc) are a subclass of `torch.nn.Module`. First, we will introduce some basic pre-defined modules that you will use most often. Then, we will go over defining a custom module, which can constitutes an arbitrary model that you have designed and want to implement in code.\n",
    "\n",
    "First, we will use a simple linear layer as an example and go over some key methdos and attributes of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "linear_module = nn.Linear(in_features=3, out_features=4, bias=True)\n",
    "\n",
    "print(linear_module)\n",
    "print(20*'-')\n",
    "\n",
    "print('Parameters:')\n",
    "for name, param in linear_module.named_parameters():\n",
    "    print('parameter name:', name)\n",
    "    print('parameter shape:', param.shape)\n",
    "    print('parameter value:', param)\n",
    "print(20*'-')\n",
    "\n",
    "\n",
    "print('Weight:')\n",
    "print(linear_module.weight)\n",
    "print(20*'-')\n",
    "\n",
    "print('Bias:')\n",
    "print(linear_module.bias)\n",
    "print(20*'-')\n",
    "\n",
    "\n",
    "# Moving the module to GPU:\n",
    "\n",
    "linear_module.to('cuda')\n",
    "\n",
    "print('Weight device:')\n",
    "print(linear_module.weight.device)\n",
    "print(20*'-')\n",
    "\n",
    "# Freezing the module so it is not trainable anymore:\n",
    "linear_module.requires_grad_(False)\n",
    "\n",
    "print('Is the weight trainable?')\n",
    "print(linear_module.weight.requires_grad)\n",
    "print(20*'-')\n",
    "\n",
    "# Making it trainable again:\n",
    "linear_module.requires_grad_(True)\n",
    "\n",
    "# Let's try to pass an input\n",
    "\n",
    "input_tensor = torch.tensor([[1.0, 2.0, 3.0]]).to('cuda')\n",
    "output = linear_module(input_tensor)\n",
    "print('Output:')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Except the specific parameter names like weight and bias, the rest of the methods above are the methods of the super class `nn.Module` and you can call or accesss them for any instance of the `nn.Module` class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a fully connected network\n",
    "\n",
    "Now, let's use some pre-defined modules to create a fully connected network. We will use linear modules and some activation modules like ReLU, Sigmoid, Tanh, LeakyReLU. There are more that you can find in PyTorch documentation online or by a simple google search or asking your favorite AI chatbot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\"\"\"\n",
    "You can create a neural network that is simply a sequence of layers by using the nn.Sequential class.\n",
    "YOu can define linear layers and activation layers from nn as well.\n",
    "\"\"\"\n",
    "\n",
    "fully_connected_network = nn.Sequential(\n",
    "    nn.Linear(in_features=3, out_features=32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=32, out_features=64),\n",
    "    nn.LeakyReLU(negative_slope=0.01),\n",
    "    nn.Linear(in_features=64, out_features=128),\n",
    "    nn.Tanh(),\n",
    "    nn.Linear(in_features=128, out_features=1),\n",
    "    nn.Sigmoid(),\n",
    ")\n",
    "\n",
    "# Let's try to pass an input tensor to this model and check the output shape:\n",
    "\n",
    "batch_size = 10\n",
    "\n",
    "input_tensor = torch.randn(batch_size, 3)\n",
    "\n",
    "output_tensor : torch.Tensor = fully_connected_network(input_tensor)\n",
    "\n",
    "print('Input tensor shape:', input_tensor.shape)\n",
    "print('Does the input have gradients?', input_tensor.requires_grad)\n",
    "print('Does the output have gradients?', output_tensor.requires_grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's inspect the model:\n",
    "\n",
    "print(fully_connected_network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting a certain module by indexing:\n",
    "print(fully_connected_network[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Going over the parameters:\n",
    "for name, param in fully_connected_network.named_parameters():\n",
    "    print('parameter name:', name)\n",
    "    print('parameter shape:', param.shape)\n",
    "    print(20*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Can we get them all in one package instead of using a loop?\n",
    "\"\"\"\n",
    "\n",
    "# STATE DICT: a dictionary containing the name and content of the parameters of the model.\n",
    "\n",
    "state_dict = fully_connected_network.state_dict()\n",
    "\n",
    "for key, value in state_dict.items():\n",
    "    print('key:', key)\n",
    "    print('value shape:', value.shape)\n",
    "    print(20*'-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "For a model with a defined architecture, you can save and load the state_dict to and from a file.\n",
    "\"\"\"\n",
    "\n",
    "# Saving the state_dict to a file:\n",
    "torch.save(state_dict, 'state_dict.pth')\n",
    "\n",
    "# Loading the state_dict from a file:\n",
    "loaded_state_dict = torch.load('state_dict.pth')\n",
    "\n",
    "# Loading the state_dict to the model:\n",
    "fully_connected_network.load_state_dict(loaded_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember these methods as these are how model parameters can be saved for publication and submission purposes as well. As mentioned before, you can use these methods on all modules, specially your customized modules! We will go over them in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining custom modules\n",
    "\n",
    "In order to define an arbitrary model in PyTorch, you need to define as a class that inherits (has all the methods and attributes of, and will have some more) from `nn.Module`. Then, you will have to implement at least two methods for your model to be able to train it and use it as any module.\n",
    "\n",
    "- `.__init__(self, ...)`\n",
    "    - First, you have to call `super().__init__()` to run the initialization of the superclass, which is `nn.Module`.\n",
    "\n",
    "    - Then, you define all the components of your model. You can use pre-defined modules like `nn.Linear`, `nn.Sequential`, `nn.Conv2d`, and many more. If you want to know if PyTorch has a certain model or layer pre-defined and ready to use, you can do a simple search and find out. \n",
    "\n",
    "    - To define a flexible number of modules, use `nn.ModuleList` and `nn.ModuleDict` and fill them in with your desired modules. You can then do everything with them like a normal list or dictionary (by indexing them with their location or key). **BE CAREFUL** to not define a normal list or dictionary for containing your modules, as they will not be detectable for backpropagation and training them. The elements of `nn.ModuleList` and `nn.ModuleDict` and be any `nn.Module`, including another `nn.ModuleList` or `nn.ModuleDict`. Try to keep things soft-coded, flexible, and not too complicated. You should be able to modify the size of your model (like number of layers, neurons, etc) by just changing the arguments you pass in the instantiation, and without any need to change the code of the model class.\n",
    "\n",
    "    - You can define custom parameters using `nn.Parameter`, or a list or dictionary of parameters using `nn.ParameterList` and `nn.ParameterDict`. You will find these useful for defining models with flexible sizes. You can fill in the ParameterList or ParameterDict in a loop whose size depends on how to define and initialize your model. You can use these parameters like any other tensor in elementwise calculations or matrix and tensor multiplications. Make sure to not store them in normal lists or dictionaries, because they will not be properly connected to the rest of the model and backpropagation will not be able to detect them! You will probably not need to define custom paramteres and use them, but keep this option in mind just in case.\n",
    "\n",
    "- `.forward(self, ...)`\n",
    "    - Here, you will use the modules and parameters you have defined in `__init__`, as well as any calculations you can, to define the forward pass of the model. You cannot define anything in this method. All model components that you want to train should have been defined. Then, you can access them as attributes of `self` and pass your input to them.\n",
    "\n",
    "\n",
    "PyTorch will take care of the rest. We will teach you how to utilize a dataset and write the code to train a model next week. \n",
    "\n",
    "For now, let's go over some examples of defining a layer with arbitrary components and operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "# A 2-layer network with a hidden layer\n",
    "class Custom_Module_1(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_features: int,\n",
    "            out_features: int,\n",
    "            hidden_dim: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_features, hidden_dim)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_dim, out_features)\n",
    "        self.activation2 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.activation2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# creating an instance of the model:\n",
    "\n",
    "custom_model_1_example_1 = Custom_Module_1(\n",
    "    in_features = 3, \n",
    "    out_features = 1, \n",
    "    hidden_dim = 32\n",
    "    )\n",
    "\n",
    "batch_size, in_features = 10, 3\n",
    "\n",
    "input_tensor = torch.randn(batch_size, in_features)\n",
    "\n",
    "output_tensor = custom_model_1_example_1(input_tensor)\n",
    "\n",
    "print('Input tensor shape:', input_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "What if I want two hidden layers?\n",
    "\"\"\"\n",
    "\n",
    "# The noob way is to rewrite the code of your class like this:\n",
    "class Custom_Module_2(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_features: int,\n",
    "            out_features: int,\n",
    "            hidden_dim1: int,\n",
    "            hidden_dim2: int,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(in_features, hidden_dim1)\n",
    "        self.activation1 = nn.ReLU()\n",
    "        self.layer2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
    "        self.activation2 = nn.ReLU()\n",
    "        self.layer3 = nn.Linear(hidden_dim2, out_features)\n",
    "        self.activation3 = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.activation1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.activation2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.activation3(x)\n",
    "        return x\n",
    "\n",
    "# creating an instance of the model:\n",
    "\n",
    "custom_model_2_example_1 = Custom_Module_2(\n",
    "    in_features = 3, \n",
    "    out_features = 1, \n",
    "    hidden_dim1 = 32,\n",
    "    hidden_dim2 = 64\n",
    "    )\n",
    "\n",
    "batch_size, in_features = 10, 3\n",
    "input_tensor = torch.randn(batch_size, in_features)\n",
    "output_tensor = custom_model_2_example_1(input_tensor)\n",
    "print('Input tensor shape:', input_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The professional way is to use nn.ModuleList\n",
    "\n",
    "# If you want to have a variable number of hidden layers, you can use nn.ModuleList to create a list of layers.\n",
    "\n",
    "class Custom_Module_3(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_features: int,\n",
    "            out_features: int,\n",
    "            hidden_dims: list,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        dims = [in_features] + hidden_dims + [out_features]\n",
    "\n",
    "        # define a for loop inside a list. This is called list comprehension.\n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.Linear(dims[i], dims[i+1])\n",
    "            for i in range(len(dims)-1)\n",
    "            ])\n",
    "        \n",
    "        # define a for loop inside a list with conditionals. \n",
    "        self.activations = nn.ModuleList([\n",
    "            nn.ReLU() if i != len(dims)-2 else nn.Sigmoid()\n",
    "            for i in range(len(dims)-1)\n",
    "            ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        for layer, activation in zip(self.layers, self.activations):\n",
    "            x = layer(x)\n",
    "            x = activation(x)\n",
    "        return x\n",
    "    \n",
    "# Now your model is more flexible and you won't need to modify the code if you want to add more hidden layers.\n",
    "\n",
    "# creating an instance of the model:\n",
    "custom_model_3_example_1 = Custom_Module_3(\n",
    "    in_features = 3, \n",
    "    out_features = 1, \n",
    "    hidden_dims = [32, 64]\n",
    "    )\n",
    "\n",
    "# Checking the model with an input tensor:\n",
    "batch_size, in_features = 10, 3\n",
    "input_tensor = torch.randn(batch_size, in_features)\n",
    "output_tensor = custom_model_3_example_1(input_tensor)\n",
    "print('Input tensor shape:', input_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's try this with nn.Sequential:\n",
    "\"\"\"\n",
    "\n",
    "class Custom_Module_4(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_features: int,\n",
    "            out_features: int,\n",
    "            hidden_dims: list,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        dims = [in_features] + hidden_dims + [out_features]\n",
    "\n",
    "        # define an empty list\n",
    "        layers = []\n",
    "        for i in range(len(dims)-1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "            if i != len(dims)-2:\n",
    "                layers.append(nn.ReLU())\n",
    "            else:\n",
    "                layers.append(nn.Sigmoid())\n",
    "\n",
    "        # The * notation unpacks the elements in the list and passes them as arguments to the nn.Sequential class.\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "        # Because you give the list to nn.Sequential, they will be detectable for training.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "    \n",
    "\n",
    "# creating an instance of the model:\n",
    "custom_model_4_example_1 = Custom_Module_4(\n",
    "    in_features = 3, \n",
    "    out_features = 1, \n",
    "    hidden_dims = [32, 64]\n",
    "    )\n",
    "\n",
    "# Checking the model with an input tensor:\n",
    "batch_size, in_features = 10, 3\n",
    "input_tensor = torch.randn(batch_size, in_features)\n",
    "output_tensor = custom_model_4_example_1(input_tensor)\n",
    "print('Input tensor shape:', input_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's try using nn.ModuleDict this time:\n",
    "\"\"\"\n",
    "\n",
    "class Custom_Module_5(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_features: int,\n",
    "        out_features: int,\n",
    "        hidden_dims: list,\n",
    "        ):\n",
    "        super().__init__()\n",
    "\n",
    "        dims = [in_features] + hidden_dims + [out_features]\n",
    "\n",
    "        self.n_layers = len(dims) - 1\n",
    "\n",
    "        self.layers = nn.ModuleDict({\n",
    "            'linear_modules': nn.ModuleList([\n",
    "                nn.Linear(dims[i], dims[i+1])\n",
    "                for i in range(self.n_layers)\n",
    "            ]),\n",
    "\n",
    "            'activation_modules': nn.ModuleList([\n",
    "                nn.ReLU() if i != len(dims)-2 else nn.Sigmoid()\n",
    "                for i in range(self.n_layers)\n",
    "            ])\n",
    "        })\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(self.n_layers):\n",
    "            x = self.layers['linear_modules'][i](x)\n",
    "            x = self.layers['activation_modules'][i](x)\n",
    "        return x\n",
    "                \n",
    "# creating an instance of the model:\n",
    "\n",
    "custom_model_5_example_1 = Custom_Module_5(\n",
    "    in_features = 3, \n",
    "    out_features = 1, \n",
    "    hidden_dims = [32, 64]\n",
    "    )\n",
    "\n",
    "# Checking the model with an input tensor:\n",
    "batch_size, in_features = 10, 3\n",
    "input_tensor = torch.randn(batch_size, in_features)\n",
    "output_tensor = custom_model_5_example_1(input_tensor)\n",
    "print('Input tensor shape:', input_tensor.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
