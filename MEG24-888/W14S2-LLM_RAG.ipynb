{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "roxXiOJRPZ6v"
      },
      "source": [
        "# Large Language Model (LLM) and Retriviel-Augmented Generation (RAG)\n",
        "\n",
        "Building upon your understanding of Transformers, this recitation introduces the concept of Large Language Models (LLMs), which are essentially Transformer-based models trained on vast amounts of text data to perform a wide range of language tasks. These models, such as GPT or BERT, rely on the attention mechanism to model long-range dependencies and contextual understanding in text. While training such models is computationally intensive, their ability to generalize across tasks makes them extremely powerful.\n",
        "\n",
        "In addition to LLMs, we will briefly touch on Retrieval-Augmented Generation (RAG), a practical technique that enhances the performance of LLMs by retrieving relevant external information at inference time. Although RAG was not covered in class, it's worth noting as an effective way to ground language generation in external knowledge sources, especially when the model's parameters alone might not store all the necessary information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuOAv8v7Pb77"
      },
      "source": [
        "# I. LLM Inputs: Tokens and Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dULTdfaPgWm"
      },
      "source": [
        "Similar to other natural language processing models, a large language model takes a sentence (a sequence of words or specifially tokens) as input."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knLSNg0DPlw-"
      },
      "source": [
        "### Vocabulary – From Text to Tokens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4S-7varlPpcW"
      },
      "source": [
        "Given an input sentence, we use the vocabulary to map each word to discrete token IDs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJ4ctVPoPtBN"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"https://raw.githubusercontent.com/Zhefan-Xu/24789-intermediate-Deep-LearningTA/main/tokenization.png\" width=\"1200\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaRi6bkyPxEf"
      },
      "source": [
        "However, those discrite IDs are not suitable for neural network learning.\n",
        "\n",
        "We need a mapping from the discrete IDs to high dimensional embedding space.\n",
        "\n",
        " - **Vocabulary:** A collection of all unique tokens (e.g., words, subwords, or characters) used by the model. Each token is assigned a unique integer ID.\n",
        " - **Embedding matrix:** A learnable matrix of size (vocabulary size × embedding dimension), where each row corresponds to the embedding vector of a token. This matrix transforms the discrete token IDs into continuous vectors that serve as input to the neural network."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uCw-MZi4RIZf"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"https://raw.githubusercontent.com/Zhefan-Xu/24789-intermediate-Deep-LearningTA/main/embedding_matrix.png\" width=\"500\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwtHJIyqP1vb"
      },
      "source": [
        "### How to build such a vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weDgZAcDP5Fg"
      },
      "source": [
        "There exists several ways to build the vocabulary based on the vocab levels:\n",
        " - **character-level:** Each character is treated as a token (e.g., \"apple\" → [\"a\", \"p\", \"p\", \"l\", \"e\"])\n",
        "     - pros:\n",
        "         - Handles out-of-vocabulary (OOV) words easily — any input can be tokenized.\n",
        "         -  Very small vocabulary size.\n",
        "     - cons:\n",
        "         - Long input sequences (inefficient for models).\n",
        "         - Harder to capture semantic meaning at the character level.\n",
        " - **subword-level:** Words are split into frequent sub-units (e.g., \"unhappiness\" → [\"un\", \"happiness\"])\n",
        "     - pros:\n",
        "         - Balances vocabulary size and flexibility.\n",
        "         - Handles rare and new words better than word-level tokenization.\n",
        "     - cons:\n",
        "         - Still possible to get long sequences for very rare or unusual words.\n",
        "         - Requires learning a subword vocabulary (e.g., BPE, WordPiece).\n",
        " - **word-level:** Split text by whitespace and punctuation (e.g., \"I like apples.\" → [\"I\", \"like\", \"apples\"])\n",
        "     - pros:\n",
        "         - Intuitive and human-readable.\n",
        "         - Shorter sequence lengths for typical text.\n",
        "     - cons:\n",
        "         - Fixed vocabulary → cannot handle unseen words (OOV problem).\n",
        "         - Large vocabulary size → more memory and harder to train."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzNnDX0mQE6U"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"https://raw.githubusercontent.com/Zhefan-Xu/24789-intermediate-Deep-LearningTA/main/vocabulary.png\" width=\"1200\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5BEW2qGPP-Gu"
      },
      "source": [
        "Byte-Pair Encoding (BPE) is a commonly used algorithm (e.g. it is used in GPT-2 and GPT-3) for constructing subword-level vocabularies. It iteratively merges the most frequent pairs of characters or character sequences to form new tokens. You can find more implementation details [here](https://huggingface.co/learn/llm-course/chapter6/5)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkMOh6kkQF14"
      },
      "source": [
        "# II. LLM Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipVYig8RQKI-"
      },
      "source": [
        "The LLM is based on the Transformer architecture, which utilizes self-attention mechanisms to efficiently process and generate sequences of text. This architecture allows the model to capture long-range dependencies and contextual relationships within the input data, making it highly effective for natural language understanding and generation tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_Igwb5NQQAy"
      },
      "source": [
        "Here shows how a vanilla encoder-decoder transformer LLM works:\n",
        " - **Encoder:** Processes the input sequence of embeddings and produces a sequence of hidden states that capture contextual information.\n",
        " - **Decoder:** Generates the output sequence by attending to the encoder’s hidden states (cross-attention) and its own previously generated tokens (self-attention).\n",
        " - **Embedding matrix:** Maps the decoder’s output embeddings back to the vocabulary space to predict the next token."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6_tZnYDQVEq"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"https://raw.githubusercontent.com/Zhefan-Xu/24789-intermediate-Deep-LearningTA/main/LLM_architectures.png\" width=\"1000\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0rmhRcjQaW4"
      },
      "source": [
        "### Achitecture choice: Encoder-Decoder Model vs. Decoder Only Model vs. Non-causal Decoder Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68phL8iERhWa"
      },
      "source": [
        "With researcher exploring different architectures of LLM, there are mainly three kinds of popular transformer architectures:\n",
        "- **Encoder-Decoder models:** These models use a separate encoder and decoder stack. The encoder processes the input sequence into a contextual representation, and the decoder generates the output sequence based on this representation and previously generated tokens. This architecture is commonly used in tasks like machine translation. Example: T5, BART.\n",
        "- **Decoder-Only (causal decoder) models:** These models rely solely on a stack of decoder layers with causal (unidirectional) attention, meaning each token can only attend to previous tokens. This setup is optimized for autoregressive generation tasks, such as language modeling and text completion. Example: GPT series.\n",
        "- **Non-causal decoder models:** These models also use decoder-only stacks but allow bidirectional (non-causal) attention during training. Unlike causal models, they can attend to all tokens in a sequence simultaneously, making them better suited for tasks that require full context understanding. Example: BERT-style models adapted into decoder-only settings, like some variants used for masked language modeling or denoising tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8isIrq5Qf5z"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"https://raw.githubusercontent.com/Zhefan-Xu/24789-intermediate-Deep-LearningTA/main/encoder_decoder.png\" width=\"800\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--aaWKh0QnkO"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"https://raw.githubusercontent.com/Zhefan-Xu/24789-intermediate-Deep-LearningTA/main/comparison.png\" width=\"800\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pizOSTjQo_c"
      },
      "source": [
        "Overall, the **decoder-only** architecture delivers the best performance for language generation tasks and is the foundation of most state-of-the-art LLMs today.\n",
        "\n",
        "However, different architectures may still offer advantages in specific domains. You can find a more detailed comparison in [paper](https://arxiv.org/abs/2204.05832)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6ZFKYvtQs5D"
      },
      "source": [
        "# III. LLM Outputs: Text Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HlQKAENQwYW"
      },
      "source": [
        "LLMs generate text using an autoregressive approach, where each token is predicted based on previously generated tokens. The example below illustrates this process:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzVnF98qRqpe"
      },
      "source": [
        "$$\n",
        "P(\\text{``I\", ``eat\", ``the\", ``apple\"]}) =\n",
        "P(\\text{``apple\"} \\mid \\text{[``I\", ``eat\", ``the\"]}) \\cdot\n",
        "P(\\text{``the\"} \\mid \\text{[``I\", ``eat\"]}) \\cdot\n",
        "P(\\text{``eat\"} \\mid \\text{[``I\"]}) \\cdot\n",
        "P(\\text{``I\"})\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qWue6EqRtx3"
      },
      "source": [
        "The probability of the predicted word at time step t is computed as follows, and a word is then randomly sampled from this probability distribution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-csOddFcRwhE"
      },
      "source": [
        "This formula is based on the standard softmax function, with the only modification being that each logit is divided by the temperature hyperparameter (T). The temperature controls the randomness of the output distribution:\n",
        "- **When T->$\\infty$**, he softmax becomes very peaked, approaching a one-hot distribution. The model becomes highly confident and almost always selects the token with the highest logit (i.e., greedy decoding).\n",
        "- **When T->0**, the logits are effectively flattened, and the softmax approaches a uniform distribution, making the output nearly random.\n",
        "- Intermediate values of T allow for a trade-off between exploration and determinism."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yx4aMhJVR4Db"
      },
      "source": [
        "What is the problem with vanilla softmax sampling?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElxCnTovRzou"
      },
      "source": [
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"https://raw.githubusercontent.com/Zhefan-Xu/24789-intermediate-Deep-LearningTA/main/generation.png\" width=\"400\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C919_QlaPUPG"
      },
      "source": [
        "## What is RAG?\n",
        "\n",
        "RAG stands for Retrieval Augmented Generation.\n",
        "\n",
        "It was introduced in the paper [*Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks*](https://arxiv.org/abs/2005.11401).\n",
        "\n",
        "Each step can be roughly broken down to:\n",
        "\n",
        "* **Retrieval** - Seeking relevant information from a source given a query. For example, getting relevant passages of Wikipedia text from a database given a question.\n",
        "* **Augmented** - Using the relevant retrieved information to modify an input to a generative model (e.g. an LLM).\n",
        "* **Generation** - Generating an output given an input. For example, in the case of an LLM, generating a passage of text given an input prompt."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1qNcwx7p1t2"
      },
      "source": [
        "## Why RAG?\n",
        "\n",
        "The main goal of RAG is to improve the generation outputs of LLMs.\n",
        "\n",
        "Two primary improvements can be seen as:\n",
        "1. **Preventing hallucinations** - LLMs are incredible but they are prone to potential hallucination, as in, generating something that *looks* correct but isn't. RAG pipelines can help LLMs generate more factual outputs by providing them with factual (retrieved) inputs. And even if the generated answer from a RAG pipeline doesn't seem correct, because of retrieval, you also have access to the sources where it came from.\n",
        "2. **Work with custom data** - Many base LLMs are trained with internet-scale text data. This means they have a great ability to model language, however, they often lack specific knowledge. RAG systems can provide LLMs with domain-specific data such as medical information or company documentation and thus customized their outputs to suit specific use cases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG7bK5U7p1t2"
      },
      "source": [
        "\n",
        "## What kind of problems can RAG be used for?\n",
        "\n",
        "RAG can help anywhere there is a specific set of information that an LLM may not have in its training data (e.g. anything not publicly accessible on the internet).\n",
        "\n",
        "For example you could use RAG for:\n",
        "* **Customer support Q&A chat** - By treating your existing customer support documentation as a resource, when a customer asks a question, you could have a system retrieve relevant documentation snippets and then have an LLM craft those snippets into an answer. Think of this as a \"chatbot for your documentation\". Klarna, a large financial company, [uses a system like this](https://www.klarna.com/international/press/klarna-ai-assistant-handles-two-thirds-of-customer-service-chats-in-its-first-month/) to save $40M per year on customer support costs.\n",
        "* **Email chain analysis** - Let's say you're an insurance company with long threads of emails between customers and insurance agents. Instead of searching through each individual email, you could retrieve relevant passages and have an LLM create strucutred outputs of insurance claims.\n",
        "* **Company internal documentation chat** - If you've worked at a large company, you know how hard it can be to get an answer sometimes. Why not let a RAG system index your company information and have an LLM answer questions you may have? The benefit of RAG is that you will have references to resources to learn more if the LLM answer doesn't suffice.\n",
        "* **Textbook Q&A** - Let's say you're studying for your exams and constantly flicking through a large textbook looking for answers to your quesitons. RAG can help provide answers as well as references to learn more.\n",
        "\n",
        "All of these have the common theme of retrieving relevant resources and then presenting them in an understandable way using an LLM.\n",
        "\n",
        "From this angle, you can consider an LLM a calculator for words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydzQk2e3p1t3"
      },
      "source": [
        "## Key terms\n",
        "\n",
        "| Term | Description |\n",
        "| ----- | ----- |\n",
        "| **Token** | A sub-word piece of text. For example, \"hello, world!\" could be split into [\"hello\", \",\", \"world\", \"!\"]. A token can be a whole word,<br> part of a word or group of punctuation characters. 1 token ~= 4 characters in English, 100 tokens ~= 75 words.<br> Text gets broken into tokens before being passed to an LLM. |\n",
        "| **Embedding** | A learned numerical representation of a piece of data. For example, a sentence of text could be represented by a vector with<br> 768 values. Similar pieces of text (in meaning) will ideally have similar values. |\n",
        "| **Embedding model** | A model designed to accept input data and output a numerical representation. For example, a text embedding model may take in 384 <br>tokens of text and turn it into a vector of size 768. An embedding model can and often is different to an LLM model. |\n",
        "| **Similarity search/vector search** | Similarity search/vector search aims to find two vectors which are close together in high-demensional space. For example, <br>two pieces of similar text passed through an embedding model should have a high similarity score, whereas two pieces of text about<br> different topics will have a lower similarity score. Common similarity score measures are dot product and cosine similarity. |\n",
        "| **Large Language Model (LLM)** | A model which has been trained to numerically represent the patterns in text. A generative LLM will continue a sequence when given a sequence. <br>For example, given a sequence of the text \"hello, world!\", a genertive LLM may produce \"we're going to build a RAG pipeline today!\".<br> This generation will be highly dependant on the training data and prompt. |\n",
        "| **LLM context window** | The number of tokens a LLM can accept as input. For example, as of March 2024, GPT-4 has a default context window of 32k tokens<br> (about 96 pages of text) but can go up to 128k if needed. A recent open-source LLM from Google, Gemma (March 2024) has a context<br> window of 8,192 tokens (about 24 pages of text). A higher context window means an LLM can accept more relevant information<br> to assist with a query. For example, in a RAG pipeline, if a model has a larger context window, it can accept more reference items<br> from the retrieval system to aid with its generation. |\n",
        "| **Prompt** | A common term for describing the input to a generative LLM. The idea of \"[prompt engineering](https://en.wikipedia.org/wiki/Prompt_engineering)\" is to structure a text-based<br> (or potentially image-based as well) input to a generative LLM in a specific way so that the generated output is ideal. This technique is<br> possible because of a LLMs capacity for in-context learning, as in, it is able to use its representation of language to breakdown <br>the prompt and recognize what a suitable output may be (note: the output of LLMs is probable, so terms like \"may output\" are used). |\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCOXy3e1p1t3"
      },
      "source": [
        " ## What we're going to build\n",
        "\n",
        "We're going to build RAG pipeline which enables us to chat with a PDF document, specifically an open-source [nutrition textbook](https://pressbooks.oer.hawaii.edu/humannutrition2/), ~1200 pages long.\n",
        "\n",
        "You could call our project NutriChat!\n",
        "\n",
        "We'll write the code to:\n",
        "1. Open a PDF document (you could use almost any PDF here).\n",
        "2. Format the text of the PDF textbook ready for an embedding model (this process is known as text splitting/chunking).\n",
        "3. Embed all of the chunks of text in the textbook and turn them into numerical representation which we can store for later.\n",
        "4. Build a retrieval system that uses vector search to find relevant chunks of text based on a query.\n",
        "5. Create a prompt that incorporates the retrieved pieces of text.\n",
        "6. Generate an answer to a query based on passages from the textbook.\n",
        "\n",
        "The above steps can broken down into two major sections:\n",
        "1. Document preprocessing/embedding creation (steps 1-3).\n",
        "2. Search and answer (steps 4-6).\n",
        "\n",
        "And that's the structure we'll follow.\n",
        "\n",
        "It's similar to the workflow outlined on the NVIDIA blog which [details a local RAG pipeline](https://developer.nvidia.com/blog/rag-101-demystifying-retrieval-augmented-generation-pipelines/).\n",
        "\n",
        "<img src=\"https://github.com/mrdbourke/simple-local-rag/blob/main/images/simple-local-rag-workflow-flowchart.png?raw=true\" alt=\"flowchart of a local RAG workflow\" />"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gw8Bht2NYYb"
      },
      "outputs": [],
      "source": [
        "# Change to runtime to GPU-T4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjVKFH1Z_66R"
      },
      "outputs": [],
      "source": [
        "# DONOT CHANGE THIS CODE\n",
        "# Mount your drive\n",
        "\n",
        "import os\n",
        "import gdown\n",
        "import subprocess\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "FILE_ID=\"1wc6HtzE8JSyrwmQoNKXwXJ8-MUr7Azn9\"\n",
        "FILE_NAME=\"gemma-transformers-1.1-2b-it-v1.tar\"\n",
        "\n",
        "model_name=\"gemma-transformers-1.1-2b-it-v1\"\n",
        "\n",
        "if not os.path.exists(os.path.join(\"/content/drive/MyDrive/\",model_name)):\n",
        "   os.mkdir(os.path.join(\"/content/drive/MyDrive/\",model_name))\n",
        "\n",
        "   #Download File\n",
        "   gdown.download(f\"https://drive.google.com/uc?id={FILE_ID}\", os.path.join(\"/content/drive/MyDrive/\",model_name,FILE_NAME), quiet=False)\n",
        "\n",
        "# untar the file\n",
        "if not \"tokenizer_config.json\" in os.listdir(os.path.join(\"/content/drive/MyDrive/\",model_name)):\n",
        "   print(\"Untaring model tar File\")\n",
        "   subprocess.run(['tar', '-xvf', os.path.join(\"/content/drive/MyDrive/\",model_name,FILE_NAME), '-C', os.path.join(\"/content/drive/MyDrive/\",model_name)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ehPrNLNHp1t3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "if \"COLAB_GPU\" in os.environ:\n",
        "    print(\"[INFO] Running in Google Colab, installing requirements.\")\n",
        "    !pip install -U torch # requires torch 2.1.1+ (for efficient sdpa implementation)\n",
        "    !pip install PyMuPDF # for reading PDFs with Python\n",
        "    !pip install tqdm # for progress bars\n",
        "    !pip install sentence-transformers # for embedding models\n",
        "    !pip install accelerate # for quantization model loading\n",
        "    !pip install bitsandbytes # for quantizing models (less storage space)\n",
        "    !pip install flash-attn --no-build-isolation # for faster attention mechanism = faster LLM inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJAxjErwp1t4"
      },
      "source": [
        "## 1. Document/Text Processing and Embedding Creation(20 Points)\n",
        "\n",
        "Ingredients:\n",
        "* PDF document of choice.\n",
        "* Embedding model of choice.\n",
        "\n",
        "Steps:\n",
        "1. Import PDF document.\n",
        "2. Process text for embedding (e.g. split into chunks of sentences).\n",
        "3. Embed text chunks with embedding model.\n",
        "4. Save embeddings to file for later use (embeddings will store on file for many years or until you lose your hard drive)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39ndCCALp1t4"
      },
      "outputs": [],
      "source": [
        "## DONOT CHANGE THIS CODE\n",
        "\n",
        "# Download PDF file\n",
        "import os\n",
        "import requests\n",
        "\n",
        "# Get PDF document\n",
        "pdf_path = os.path.join(\"/content/drive/MyDrive/\",model_name,\"human-nutrition-text.pdf\")\n",
        "\n",
        "# Download PDF if it doesn't already exist\n",
        "if not os.path.exists(pdf_path):\n",
        "  print(\"File doesn't exist, downloading...\")\n",
        "\n",
        "  # The URL of the PDF you want to download\n",
        "  url = \"https://pressbooks.oer.hawaii.edu/humannutrition2/open/download?type=pdf\"\n",
        "\n",
        "  # The local filename to save the downloaded file\n",
        "  filename = pdf_path\n",
        "\n",
        "  # Send a GET request to the URL\n",
        "  response = requests.get(url)\n",
        "\n",
        "  # Check if the request was successful\n",
        "  if response.status_code == 200:\n",
        "      # Open a file in binary write mode and save the content to it\n",
        "      with open(filename, \"wb\") as file:\n",
        "          file.write(response.content)\n",
        "      print(f\"The file has been downloaded and saved as {filename}\")\n",
        "  else:\n",
        "      print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
        "else:\n",
        "  print(f\"File {pdf_path} exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq0T_faqp1t4"
      },
      "source": [
        "PDF acquired!\n",
        "\n",
        "We can import the pages of our PDF to text by first defining the PDF path and then opening and reading it with PyMuPDF (`import fitz`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pwOztMHtp1t4"
      },
      "outputs": [],
      "source": [
        "import fitz # (pymupdf, found this is better than pypdf for our use case, note: licence is AGPL-3.0, keep that in mind if you want to use any code commercially)\n",
        "from tqdm.auto import tqdm # for progress bars, requires !pip install tqdm\n",
        "\n",
        "def text_formatter(text: str) -> str:\n",
        "    \"\"\"Performs minor formatting on text.\"\"\"\n",
        "    cleaned_text = text.replace(\"\\n\", \" \").strip() # note: this might be different for each doc (best to experiment)\n",
        "\n",
        "    # Other potential text formatting functions can go here\n",
        "    return cleaned_text\n",
        "\n",
        "# Open PDF and get lines/pages\n",
        "# Note: this only focuses on text, rather than images/figures etc\n",
        "def open_and_read_pdf(pdf_path: str) -> list[dict]:\n",
        "    \"\"\"\n",
        "    Opens a PDF file, reads its text content page by page, and collects statistics.\n",
        "\n",
        "    Parameters:\n",
        "        pdf_path (str): The file path to the PDF document to be opened and read.\n",
        "\n",
        "    Returns:\n",
        "        list[dict]: A list of dictionaries, each containing the page number\n",
        "        (adjusted), character count, word count, sentence count, token count, and the extracted text\n",
        "        for each page.\n",
        "    \"\"\"\n",
        "    doc = fitz.open(pdf_path)  # open a document\n",
        "    pages_and_texts = []\n",
        "    for page_number, page in tqdm(enumerate(doc)):  # iterate the document pages\n",
        "        text = page.get_text()  # get plain text encoded as UTF-8\n",
        "        text = text_formatter(text)\n",
        "        pages_and_texts.append({\"page_number\": page_number - 41,  # adjust page numbers since our PDF starts on page 42\n",
        "                                \"page_char_count\": len(text),\n",
        "                                \"page_word_count\": len(text.split(\" \")),\n",
        "                                \"page_sentence_count_raw\": len(text.split(\". \")),\n",
        "                                \"page_token_count\": len(text) / 4,  # 1 token = ~4 chars, see: https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them\n",
        "                                \"text\": text})\n",
        "    return pages_and_texts\n",
        "\n",
        "pages_and_texts = open_and_read_pdf(pdf_path=pdf_path)\n",
        "pages_and_texts[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIBbZguDp1t5"
      },
      "source": [
        "Now let's get a random sample of the pages. How does sample look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5sh4DKJp1t5"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "random.sample(pages_and_texts, k=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vbd8B4tp1t5"
      },
      "source": [
        "### Stats on Text.\n",
        "\n",
        "Let's perform a rough exploratory data analysis (EDA) to get an idea of the size of the texts (e.g. character counts, word counts etc) we're working with.\n",
        "\n",
        "The different sizes of texts will be a good indicator into how we should split our texts.\n",
        "\n",
        "Many embedding models have limits on the size of texts they can ingest, for example, the [`sentence-transformers`](https://www.sbert.net/docs/pretrained_models.html) model [`all-mpnet-base-v2`](https://huggingface.co/sentence-transformers/all-mpnet-base-v2) has an input size of 384 tokens.\n",
        "\n",
        "This means that the model has been trained in ingest and turn into embeddings texts with 384 tokens (1 token ~= 4 characters ~= 0.75 words).\n",
        "\n",
        "Texts over 384 tokens which are encoded by this model will be auotmatically reduced to 384 tokens in length, potentially losing some information.\n",
        "\n",
        "We'll discuss this more in the embedding section.\n",
        "\n",
        "For now, let's turn our list of dictionaries into a DataFrame and explore it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yLJuMsXrp1t5"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(pages_and_texts)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s7uERBNip1t5"
      },
      "outputs": [],
      "source": [
        "# Get stats\n",
        "df.describe().round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gyYE7jmJp1t5"
      },
      "source": [
        "Okay, looks like our average token count per page is 287.\n",
        "\n",
        "For this particular use case, it means we could embed an average whole page with the `all-mpnet-base-v2` model (this model has an input capacity of 384)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwLyDf68p1t5"
      },
      "source": [
        "### Further text processing (splitting pages into sentences)\n",
        "\n",
        "The ideal way of processing text before embedding it is still an active area of research.\n",
        "\n",
        "A simple method I've found helpful is to break the text into chunks of sentences.\n",
        "\n",
        "As in, chunk a page of text into groups of 5, 7, 10 or more sentences (these values are not set in stone and can be explored).\n",
        "\n",
        "But we want to follow the workflow of:\n",
        "\n",
        "`Ingest text -> split it into groups/chunks -> embed the groups/chunks -> use the embeddings`\n",
        "\n",
        "Some options for splitting text into sentences:\n",
        "\n",
        "1. Split into sentences with simple rules (e.g. split on \". \" with `text = text.split(\". \")`, like we did above).\n",
        "2. Split into sentences with a natural language processing (NLP) library such as [spaCy](https://spacy.io/) or [nltk](https://www.nltk.org/).\n",
        "\n",
        "Why split into sentences?\n",
        "\n",
        "* Easier to handle than larger pages of text (especially if pages are densely filled with text).\n",
        "* Can get specific and find out which group of sentences were used to help within a RAG pipeline.\n",
        "\n",
        "> **Resource:** See [spaCy install instructions](https://spacy.io/usage).\n",
        "\n",
        "Let's use spaCy to break our text into sentences since it's likely a bit more robust than just using `text.split(\". \")`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "853xoua7p1t5"
      },
      "outputs": [],
      "source": [
        "from spacy.lang.en import English # see https://spacy.io/usage for install instructions\n",
        "\n",
        "nlp = English()\n",
        "\n",
        "# Add a sentencizer pipeline, see https://spacy.io/api/sentencizer/\n",
        "nlp.add_pipe(\"sentencizer\")\n",
        "\n",
        "# Create a document instance as an example\n",
        "doc = nlp(\"This is a sentence. This another sentence.\")\n",
        "assert len(list(doc.sents)) == 2\n",
        "\n",
        "# Access the sentences of the document\n",
        "list(doc.sents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7UkZLy5p1t5"
      },
      "source": [
        "We don't necessarily need to use spaCy, however, it's an open-source library designed to do NLP tasks like this at scale.\n",
        "\n",
        "So let's run our small sentencizing pipeline on our pages of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VHEbnK65p1t5"
      },
      "outputs": [],
      "source": [
        "for item in tqdm(pages_and_texts):\n",
        "    item[\"sentences\"] = list(nlp(item[\"text\"]).sents)\n",
        "\n",
        "    # Make sure all sentences are strings\n",
        "    item[\"sentences\"] = [str(sentence) for sentence in item[\"sentences\"]]\n",
        "\n",
        "    # Count the sentences\n",
        "    item[\"page_sentence_count_spacy\"] = len(item[\"sentences\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4rqCb3YBp1t6"
      },
      "outputs": [],
      "source": [
        "# Inspect an example\n",
        "random.sample(pages_and_texts, k=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcXZ3q3Kp1t6"
      },
      "source": [
        "Wonderful!\n",
        "\n",
        "Now let's turn out list of dictionaries into a DataFrame and get some stats."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-uZ0LI-vp1t6"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(pages_and_texts)\n",
        "df.describe().round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uDQbz2hp1t6"
      },
      "source": [
        "For our set of text, it looks like our raw sentence count (e.g. splitting on `\". \"`) is quite close to what spaCy came up with.\n",
        "\n",
        "Now we've got our text split into sentences, how about we gorup those sentences?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3Kcvbs5p1t6"
      },
      "source": [
        "### Chunking our sentences together\n",
        "\n",
        "Let's take a step to break down our list of sentences/text into smaller chunks.\n",
        "\n",
        "As you might've guessed, this process is referred to as **chunking**.\n",
        "\n",
        "Why do we do this?\n",
        "\n",
        "1. Easier to manage similar sized chunks of text.\n",
        "2. Don't overload the embedding models capacity for tokens (e.g. if an embedding model has a capacity of 384 tokens, there could be information loss if you try to embed a sequence of 400+ tokens).\n",
        "3. Our LLM context window (the amount of tokens an LLM can take in) may be limited and requires compute power so we want to make sure we're using it as well as possible.\n",
        "\n",
        "Something to note is that there are many different ways emerging for creating chunks of information/text.\n",
        "\n",
        "For now, we're going to keep it simple and break our pages of sentences into groups of 10 (this number is arbitrary and can be changed, I just picked it because it seemed to line up well with our embedding model capacity of 384).\n",
        "\n",
        "On average each of our pages has 10 sentences.\n",
        "\n",
        "And an average total of 287 tokens per page.\n",
        "\n",
        "So our groups of 10 sentences will also be ~287 tokens long.\n",
        "\n",
        "This gives us plenty of room for the text to embedded by our `all-mpnet-base-v2` model (it has a capacity of 384 tokens).\n",
        "\n",
        "To split our groups of sentences into chunks of 10 or less, let's create a function which accepts a list as input and recursively breaks into down into sublists of a specified size."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3pv8TsHp1t6"
      },
      "outputs": [],
      "source": [
        "# Define split size to turn groups of sentences into chunks\n",
        "num_sentence_chunk_size = 10\n",
        "\n",
        "# Create a function that recursively splits a list into desired sizes\n",
        "def split_list(input_list: list,\n",
        "               slice_size: int) -> list[list[str]]:\n",
        "    \"\"\"\n",
        "    Splits the input_list into sublists of size slice_size (or as close as possible).\n",
        "\n",
        "    For example, a list of 17 sentences would be split into two lists of [[10], [7]]\n",
        "    \"\"\"\n",
        "    return [input_list[i:i + slice_size] for i in range(0, len(input_list), slice_size)]\n",
        "\n",
        "# Loop through pages and texts and split sentences into chunks\n",
        "for item in tqdm(pages_and_texts):\n",
        "    item[\"sentence_chunks\"] = split_list(input_list=item[\"sentences\"],\n",
        "                                         slice_size=num_sentence_chunk_size)\n",
        "    item[\"num_chunks\"] = len(item[\"sentence_chunks\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NGFTI24pp1t6"
      },
      "outputs": [],
      "source": [
        "# Sample an example from the group (note: many samples have only 1 chunk as they have <=10 sentences total)\n",
        "random.sample(pages_and_texts, k=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKDRgScpp1t6"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame to get stats\n",
        "df = pd.DataFrame(pages_and_texts)\n",
        "df.describe().round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6IlvqhNp1t6"
      },
      "source": [
        "Note how the average number of chunks is around 1.5, this is expected since many of our pages only contain an average of 10 sentences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YrcBau6Ep1t6"
      },
      "source": [
        "### Splitting each chunk into its own item\n",
        "\n",
        "We'd like to embed each chunk of sentences into its own numerical representation.\n",
        "\n",
        "So to keep things clean, let's create a new list of dictionaries each containing a single chunk of sentences with relative information such as page number as well statistics about each chunk."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzO7LAFyp1t7"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Split each chunk into its own item\n",
        "pages_and_chunks = []\n",
        "for item in tqdm(pages_and_texts):\n",
        "    for sentence_chunk in item[\"sentence_chunks\"]:\n",
        "        chunk_dict = {}\n",
        "        chunk_dict[\"page_number\"] = item[\"page_number\"]\n",
        "\n",
        "        # Join the sentences together into a paragraph-like structure, aka a chunk (so they are a single string)\n",
        "        joined_sentence_chunk = \"\".join(sentence_chunk).replace(\"  \", \" \").strip()\n",
        "        joined_sentence_chunk = re.sub(r'\\.([A-Z])', r'. \\1', joined_sentence_chunk) # \".A\" -> \". A\" for any full-stop/capital letter combo\n",
        "        chunk_dict[\"sentence_chunk\"] = joined_sentence_chunk\n",
        "\n",
        "        # Get stats about the chunk\n",
        "        chunk_dict[\"chunk_char_count\"] = len(joined_sentence_chunk)\n",
        "        chunk_dict[\"chunk_word_count\"] = len([word for word in joined_sentence_chunk.split(\" \")])\n",
        "        chunk_dict[\"chunk_token_count\"] = len(joined_sentence_chunk) / 4 # 1 token = ~4 characters\n",
        "\n",
        "        pages_and_chunks.append(chunk_dict)\n",
        "\n",
        "# How many chunks do we have?\n",
        "len(pages_and_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gXaJPCNLp1t7"
      },
      "outputs": [],
      "source": [
        "# View a random sample\n",
        "random.sample(pages_and_chunks, k=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OaL2we3Ep1t7"
      },
      "source": [
        "Excellent!\n",
        "\n",
        "Now we've broken our whole textbook into chunks of 10 sentences or less as well as the page number they came from.\n",
        "\n",
        "This means we could reference a chunk of text and know its source.\n",
        "\n",
        "Let's get some stats about our chunks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5Q5hxtnp1t7"
      },
      "outputs": [],
      "source": [
        "# Get stats about our chunks\n",
        "df = pd.DataFrame(pages_and_chunks)\n",
        "df.describe().round(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0JVNHeVRp1t7"
      },
      "source": [
        "Hmm looks like some of our chunks have quite a low token count.\n",
        "\n",
        "How about we check for samples with less than 30 tokens (about the length of a sentence) and see if they are worth keeping?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXnst6W-Uxsw"
      },
      "outputs": [],
      "source": [
        "# Show random chunks with under 30 tokens in length\n",
        "min_token_length = 30\n",
        "for row in df[df[\"chunk_token_count\"] <= min_token_length].sample(5).iterrows():\n",
        "    print(f'Chunk token count: {row[1][\"chunk_token_count\"]} | Text: {row[1][\"sentence_chunk\"]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsKt1nQbp1t8"
      },
      "outputs": [],
      "source": [
        "pages_and_chunks_over_min_token_len = df[df[\"chunk_token_count\"] > min_token_length].to_dict(orient=\"records\")\n",
        "pages_and_chunks_over_min_token_len[:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Y2QAaTSp1t8"
      },
      "source": [
        "Smaller chunks filtered!\n",
        "\n",
        "Time to embed our chunks of text!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3tQAFYPHp1t8"
      },
      "source": [
        "### Embedding our text chunks(Tokenization Process)\n",
        "\n",
        "While humans understand text, machines understand numbers best.\n",
        "\n",
        "An [embedding](https://vickiboykis.com/what_are_embeddings/index.html) is a broad concept.\n",
        "\n",
        "But one of my favourite and simple definitions is \"a useful numerical representation\".\n",
        "\n",
        "The most powerful thing about modern embeddings is that they are *learned* representations.\n",
        "\n",
        "Meaning rather than directly mapping words/tokens/characters to numbers directly (e.g. `{\"a\": 0, \"b\": 1, \"c\": 3...}`), the numerical representation of tokens is learned by going through large corpuses of text and figuring out how different tokens relate to each other.\n",
        "\n",
        "Ideally, embeddings of text will mean that similar meaning texts have similar numerical representation.\n",
        "\n",
        "> **Note:** Most modern NLP models deal with \"tokens\" which can be considered as multiple different sizes and combinations of words and characters rather than always whole words or single characters. For example, the string `\"hello world!\"` gets mapped to the token values `{15339: b'hello', 1917: b' world', 0: b'!'}` using [Byte pair encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding) (or BPE via OpenAI's [`tiktoken`](https://github.com/openai/tiktoken) library). Google has a tokenization library called [SentencePiece](https://github.com/google/sentencepiece).\n",
        "\n",
        "Our goal is to turn each of our chunks into a numerical representation (an embedding vector, where a vector is a sequence of numbers arranged in order).\n",
        "\n",
        "Once our text samples are in embedding vectors, us humans will no longer be able to understand them.\n",
        "\n",
        "However, we don't need to.\n",
        "\n",
        "The embedding vectors are for our computers to understand.\n",
        "\n",
        "We'll use our computers to find patterns in the embeddings and then we can use their text mappings to further our understanding.\n",
        "\n",
        "Enough talking, how about we import a text embedding model and see what an embedding looks like.\n",
        "\n",
        "To do so, we'll use the [`sentence-transformers`](https://www.sbert.net/docs/installation.html) library which contains many pre-trained embedding models.\n",
        "\n",
        "Specifically, we'll get the `all-mpnet-base-v2` model (you can see the model's intended use on the [Hugging Face model card](https://huggingface.co/sentence-transformers/all-mpnet-base-v2#intended-uses))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aYD16hx_p1t8"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n",
        "                                      device=\"cpu\") # choose the device to load the model to (note: GPU will often be *much* faster than CPU)\n",
        "\n",
        "# Create a list of sentences to turn into numbers\n",
        "sentences = [\n",
        "    \"The Sentences Transformers library provides an easy and open-source way to create embeddings.\",\n",
        "    \"Sentences can be embedded one by one or as a list of strings.\",\n",
        "    \"Embeddings are one of the most powerful concepts in machine learning!\",\n",
        "    \"Learn to use embeddings well and you'll be well on your way to being an AI engineer.\"\n",
        "]\n",
        "\n",
        "# Sentences are encoded/embedded by calling model.encode()\n",
        "embeddings = embedding_model.encode(sentences)\n",
        "embeddings_dict = dict(zip(sentences, embeddings))\n",
        "\n",
        "# See the embeddings\n",
        "for sentence, embedding in embeddings_dict.items():\n",
        "    print(\"Sentence:\", sentence)\n",
        "    print(\"Embedding:\", embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7gT1hWUp1t8"
      },
      "source": [
        "Woah! That's a lot of numbers.\n",
        "\n",
        "How about we do just once sentence?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NXEyzp4ep1t8"
      },
      "outputs": [],
      "source": [
        "# Encode the single_sentence\n",
        "\n",
        "single_sentence = \"Yo! How cool are embeddings?\"\n",
        "single_embedding = embedding_model.encode(single_sentence)\n",
        "print(f\"Sentence: {single_sentence}\")\n",
        "print(f\"Embedding:\\n{single_embedding}\")\n",
        "print(f\"Embedding size: {single_embedding.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brBm4L2-p1t8"
      },
      "source": [
        "Nice! We've now got a way to numerically represent each of our chunks.\n",
        "\n",
        "Our embedding has a shape of `(768,)` meaning it's a vector of 768 numbers which represent our text in high-dimensional space, too many for a human to comprehend but machines love high-dimensional space.\n",
        "\n",
        "> **Note:** No matter the size of the text input to our `all-mpnet-base-v2` model, it will be turned into an embedding size of `(768,)`. This value is fixed. So whether a sentence is 1 token long or 1000 tokens long, it will be truncated/padded with zeros to size 384 and then turned into an embedding vector of size `(768,)`. Of course, other embedding models may have different input/output shapes.\n",
        "\n",
        "How about we add an embedding field to each of our chunk items?\n",
        "\n",
        "Let's start by trying to create embeddings on the CPU, we'll time it with the `%%time` magic to see how long it takes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgJo2b5zp1t8"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Send the model to the GPU\n",
        "embedding_model.to(\"cuda\") # requires a GPU installed, for reference on my local machine, I'm using a NVIDIA RTX 4090\n",
        "\n",
        "# Create embeddings one by one on the GPU\n",
        "for item in tqdm(pages_and_chunks_over_min_token_len):\n",
        "    item[\"embedding\"] = embedding_model.encode(item[\"sentence_chunk\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLe2bt5ep1t8"
      },
      "source": [
        "Woah! Looks like the embeddings get created much faster (~10x faster on my machine) on the GPU!\n",
        "\n",
        "You'll likely notice this trend with many of your deep learning workflows. If you have access to a GPU, especially a NVIDIA GPU, you should use one if you can.\n",
        "\n",
        "But what if I told you we could go faster again?\n",
        "\n",
        "You see many modern models can handle batched predictions.\n",
        "\n",
        "This means computing on multiple samples at once.\n",
        "\n",
        "Those are the types of operations where a GPU flourishes!\n",
        "\n",
        "We can perform batched operations by turning our target text samples into a single list and then passing that list to our embedding model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcQwvgOip1t8"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Turn text chunks into a single list\n",
        "text_chunks = [item[\"sentence_chunk\"] for item in pages_and_chunks_over_min_token_len]\n",
        "# Embed all texts in batches\n",
        "text_chunk_embeddings = embedding_model.encode(text_chunks,\n",
        "                                               batch_size=32, # you can use different batch sizes here for speed/performance, I found 32 works well for this use case\n",
        "                                               convert_to_tensor=True) # optional to return embeddings as tensor instead of array\n",
        "\n",
        "text_chunk_embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EX9IZoylp1t9"
      },
      "source": [
        "### Save embeddings to file\n",
        "\n",
        "Since creating embeddings can be a timely process (not so much for our case but it can be for more larger datasets), let's turn our `pages_and_chunks_over_min_token_len` list of dictionaries into a DataFrame and save it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4aTtZCLp1t9"
      },
      "outputs": [],
      "source": [
        "# Save embeddings to file\n",
        "text_chunks_and_embeddings_df = pd.DataFrame(pages_and_chunks_over_min_token_len)\n",
        "embeddings_df_save_path = os.path.join(\"/content/drive/MyDrive/\",model_name,\"text_chunks_and_embeddings_df.csv\")\n",
        "text_chunks_and_embeddings_df.to_csv(embeddings_df_save_path, index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BxV7PGcNp1t9"
      },
      "source": [
        "And we can make sure it imports nicely by loading it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyZf54GIp1t9"
      },
      "outputs": [],
      "source": [
        "# Import saved file and view\n",
        "embeddings_df_save_path = os.path.join(\"/content/drive/MyDrive/\",model_name,\"text_chunks_and_embeddings_df.csv\")\n",
        "text_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)\n",
        "text_chunks_and_embedding_df_load.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2oAjIysFZhI"
      },
      "source": [
        "## 2. RAG - Search and Answer(50)\n",
        "\n",
        "We discussed RAG briefly in the beginning but let's quickly recap.\n",
        "\n",
        "RAG stands for Retrieval Augmented Generation.\n",
        "\n",
        "Which is another way of saying \"given a query, search for relevant resources and answer based on those resources\".\n",
        "\n",
        "Let's breakdown each step:\n",
        "* **Retrieval** - Get relevant resources given a query. For example, if the query is \"what are the macronutrients?\" the ideal results will contain information about protein, carbohydrates and fats (and possibly alcohol) rather than information about which tractors are the best for farming (though that is also cool information).\n",
        "* **Augmentation** - LLMs are capable of generating text given a prompt. However, this generated text is designed to *look* right. And it often has some correct information, however, they are prone to hallucination (generating a result that *looks* like legit text but is factually wrong). In augmentation, we pass relevant information into the prompt and get an LLM to use that relevant information as the basis of its generation.\n",
        "* **Generation** - This is where the LLM will generate a response that has been flavoured/augmented with the retrieved resources. In turn, this not only gives us a potentially more correct answer, it also gives us resources to investigate more (since we know which resources went into the prompt).\n",
        "\n",
        "The whole idea of RAG is to get an LLM to be more factually correct based on your own input as well as have a reference to where the generated output may have come from.\n",
        "\n",
        "This is an incredibly helpful tool.\n",
        "\n",
        "Let's say you had 1000s of customer support documents.\n",
        "\n",
        "You could use RAG to generate direct answers to questions with links to relevant documentation.\n",
        "\n",
        "Or you were an insurance company with large chains of claims emails.\n",
        "\n",
        "You could use RAG to answer questions about the emails with sources.\n",
        "\n",
        "One helpful analogy is to think of LLMs as calculators for words.\n",
        "\n",
        "With good inputs, the LLM can sort them into helpful outputs.\n",
        "\n",
        "How?\n",
        "\n",
        "It starts with better search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSucZxAQFff-"
      },
      "source": [
        "### Similarity search\n",
        "\n",
        "Similarity search or semantic search or vector search is the idea of searching on *vibe*.\n",
        "\n",
        "If this sounds like woo, woo. It's not.\n",
        "\n",
        "Perhaps searching via *meaning* is a better analogy.\n",
        "\n",
        "With keyword search, you are trying to match the string \"apple\" with the string \"apple\".\n",
        "\n",
        "Whereas with similarity/semantic search, you may want to search \"macronutrients functions\".\n",
        "\n",
        "And get back results that don't necessarily contain the words \"macronutrients functions\" but get back pieces of text that match that meaning.\n",
        "\n",
        "> **Example:** Using similarity search on our textbook data with the query \"macronutrients function\" returns a paragraph that starts with:\n",
        ">\n",
        ">*There are three classes of macronutrients: carbohydrates, lipids, and proteins. These can be metabolically processed into cellular energy. The energy from macronutrients comes from their chemical bonds. This chemical energy is converted into cellular energy that is then utilized to perform work, allowing our bodies to conduct their basic functions.*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PkcLgDixFf91"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Import texts and embedding df\n",
        "text_chunks_and_embedding_df = pd.read_csv(os.path.join(\"/content/drive/MyDrive/\",model_name,\"text_chunks_and_embeddings_df.csv\"))\n",
        "\n",
        "# Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n",
        "text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
        "\n",
        "# Convert texts and embedding df to list of dicts\n",
        "pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
        "\n",
        "# Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)\n",
        "embeddings = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
        "embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q9iffbOiFjdL"
      },
      "outputs": [],
      "source": [
        "text_chunks_and_embedding_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntJ49iplFqtm"
      },
      "source": [
        "Nice!\n",
        "\n",
        "Now let's prepare another instance of our embedding model. Not because we have to but because we'd like to make it so you can start the notebook from the cell above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17rFLyZ2Fl_D"
      },
      "outputs": [],
      "source": [
        "device=\"cuda\"\n",
        "from sentence_transformers import util, SentenceTransformer\n",
        "\n",
        "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n",
        "                                      device=device) # choose the device to load the model to"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPOUjsPQF0Bp"
      },
      "source": [
        "Embedding model ready!\n",
        "\n",
        "Time to perform a semantic search.\n",
        "\n",
        "Let's say you were studying the macronutrients.\n",
        "\n",
        "And wanted to search your textbook for \"macronutrients functions\".\n",
        "\n",
        "Well, we can do so with the following steps:\n",
        "1. Define a query string (e.g. `\"macronutrients functions\"`) - note: this could be anything, specific or not.\n",
        "2. Turn the query string in an embedding with same model we used to embed our text chunks.\n",
        "3. Perform a [dot product](https://pytorch.org/docs/stable/generated/torch.dot.html) or [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) function between the text embeddings and the query embedding (we'll get to what these are shortly) to get similarity scores.\n",
        "4. Sort the results from step 3 in descending order (a higher score means more similarity in the eyes of the model) and use these values to inspect the texts.\n",
        "\n",
        "Easy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a21ECRMrF0ha"
      },
      "outputs": [],
      "source": [
        "# 1. Define the query\n",
        "# Note: This could be anything. But since we're working with a nutrition textbook, we'll stick with nutrition-based queries.\n",
        "query = \"macronutrients functions\"\n",
        "print(f\"Query: {query}\")\n",
        "\n",
        "# 2. Embed the query to the same numerical space as the text examples\n",
        "# Note: It's important to embed your query with the same model you embedded your examples with.\n",
        "query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
        "\n",
        "# 3. Get similarity scores with the dot product (we'll time this for fun)\n",
        "from time import perf_counter as timer\n",
        "\n",
        "start_time = timer()\n",
        "dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
        "end_time = timer()\n",
        "\n",
        "print(f\"Time take to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
        "\n",
        "# 4. Get the top-k results (we'll keep this to 5)\n",
        "top_results_dot_product = torch.topk(dot_scores, k=5)\n",
        "top_results_dot_product"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t0M6BrUKF9Gn"
      },
      "source": [
        "We can get pretty far by just storing our embeddings in `torch.tensor` for now.\n",
        "\n",
        "However, for *much* larger datasets, we'd likely look at a dedicated vector database/indexing libraries such as [Faiss](https://github.com/facebookresearch/faiss).\n",
        "\n",
        "Let's check the results of our original similarity search.\n",
        "\n",
        "[`torch.topk`](https://pytorch.org/docs/stable/generated/torch.topk.html) returns a tuple of values (scores) and indicies for those scores.\n",
        "\n",
        "The indicies relate to which indicies in the `embeddings` tensor have what scores in relation to the query embedding (higher is better).\n",
        "\n",
        "We can use those indicies to map back to our text chunks.\n",
        "\n",
        "First, we'll define a small helper function to print out wrapped text (so it doesn't print a whole text chunk as a single line)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApKB7ithF9iw"
      },
      "outputs": [],
      "source": [
        "# Define helper function to print wrapped text\n",
        "import textwrap\n",
        "\n",
        "def print_wrapped(text, wrap_length=80):\n",
        "    wrapped_text = textwrap.fill(text, wrap_length)\n",
        "    print(wrapped_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFs5KnMaGHdm"
      },
      "source": [
        "Now we can loop through the `top_results_dot_product` tuple and match up the scores and indicies and then use those indicies to index on our `pages_and_chunks` variable to get the relevant text chunk.\n",
        "\n",
        "Sounds like a lot but we can do it!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ALoUOgopGH30"
      },
      "outputs": [],
      "source": [
        "print(f\"Query: '{query}'\\n\")\n",
        "print(\"Results:\")\n",
        "# Loop through zipped together scores and indicies from torch.topk\n",
        "for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):\n",
        "    print(f\"Score: {score:.4f}\")\n",
        "    # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
        "    print(\"Text:\")\n",
        "    print_wrapped(pages_and_chunks[idx][\"sentence_chunk\"])\n",
        "    # Print the page number too so we can reference the textbook further (and check the results)\n",
        "    print(f\"Page number: {pages_and_chunks[idx]['page_number']}\")\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MrNvm7XjGMFP"
      },
      "outputs": [],
      "source": [
        "import fitz\n",
        "\n",
        "# Open PDF and load target page\n",
        "pdf_path = os.path.join(\"/content/drive/MyDrive/\",model_name,\"human-nutrition-text.pdf\") # requires PDF to be downloaded\n",
        "doc = fitz.open(pdf_path)\n",
        "page = doc.load_page(5 + 41) # number of page (our doc starts page numbers on page 41)\n",
        "\n",
        "# Get the image of the page\n",
        "img = page.get_pixmap(dpi=300)\n",
        "\n",
        "# Optional: save the image\n",
        "#img.save(\"output_filename.png\")\n",
        "doc.close()\n",
        "\n",
        "# Convert the Pixmap to a numpy array\n",
        "img_array = np.frombuffer(img.samples_mv,\n",
        "                          dtype=np.uint8).reshape((img.h, img.w, img.n))\n",
        "\n",
        "# Display the image using Matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(13, 10))\n",
        "plt.imshow(img_array)\n",
        "plt.title(f\"Query: '{query}' | Most relevant page:\")\n",
        "plt.axis('off') # Turn off axis\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RMAtrcfmO6W"
      },
      "source": [
        "We could repeat this workflow for any kind of query we'd like on our textbook.\n",
        "\n",
        "And it would also work for other datatypes too.\n",
        "\n",
        "We could use semantic search on customer support documents.\n",
        "\n",
        "Or email threads.\n",
        "\n",
        "Or company plans.\n",
        "\n",
        "Or our old journal entries.\n",
        "\n",
        "Almost anything!\n",
        "\n",
        "The workflow is the same:\n",
        "\n",
        "`ingest documents -> split into chunks -> embed chunks -> make a query -> embed the query -> compare query embedding to chunk embeddings`\n",
        "\n",
        "And we get relevant resources *along with* the source they came from!\n",
        "\n",
        "That's the **retrieval** part of Retrieval Augmented Generation (RAG)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pmHaWBdmPcj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
