{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Regularization\n","\n","This week you have started learning about the basics of convolution and some regularization techniques like data augmentation, L1/L2 regularization, dropout, and batchnorm. In this notebook, we will go over these concepts in PyTorch and How to implement them."]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["# For interactive plotting:\n","import os\n","import matplotlib.pyplot as plt\n","import ipywidgets as widgets\n","try:\n","    from google.colab import output\n","    output.enable_custom_widget_manager()\n","except ImportError:\n","    pass\n","try:\n","    %matplotlib widget\n","except:\n","    os.system('pip install ipympl -qq')\n","    %matplotlib widget\n","\n","\n","import torch\n","from torch import nn, optim\n","\n","from torchvision import datasets\n","from torch.utils.data import Dataset\n","from torchvision.transforms import v2\n","\n","# helper function to inspect a tensor:\n","def print_tensor_info(\n","        name: str, \n","        tensor, # torch.Tensor\n","        ):\n","    print(f'{name}')\n","    print(20*'-')\n","    if not isinstance(tensor, torch.Tensor):\n","        print(f'It is {type(tensor).__name__}!')\n","        print(20*'='+'\\n')\n","        return\n","    # print name, shhape, dtype, device, require_grad\n","    print(f'shape: {tensor.shape}')\n","    print(f'dtype: {tensor.dtype}')\n","    print(f'device: {tensor.device}')\n","    print(f'requires_grad: {tensor.requires_grad}')\n","    print(20*'='+'\\n')\n","\n","\n","# helper class to visualize image data interactively in Jupyter Notebook\n","class ImageDataViz:\n","    \"\"\"\n","    An interactive image data visualzation tool inside Juptyer Notebook.\n","    Make sure to use the magic command: %matplotlib widget\n","    \"\"\"\n","    def __init__(self, data: Dataset):\n","        self.data = data\n","        self.n_samples = len(data)\n","        self.index = widgets.IntSlider(\n","            value=0, \n","            min=0, \n","            max=self.n_samples-1, \n","            step=1, \n","            description='Index', \n","            continuous_update=True,\n","            layout=widgets.Layout(width='40%'),\n","        )\n","\n","    def update(self, index: int):\n","        x, y = self.data[index]\n","        image = x.moveaxis(0, -1).squeeze().numpy()\n","        self.img.set_data(image)\n","        self.ax.set_title(f'Label: {y}')\n","\n","    def show(self):\n","        self.fig, self.ax = plt.subplots()\n","        x, y = self.data[0]\n","        image = x.moveaxis(0, -1).squeeze().numpy()\n","        self.img = self.ax.imshow(image)\n","        self.ax.axis('off')\n","        self.ax.set_title(f'Label: {y}')\n","        widgets.interact(self.update, index=self.index)"]},{"cell_type":"markdown","metadata":{},"source":["# Initialization\n","\n","This is a simple but very important part of training a neural network. Using it is pretty easy. Choose an initializer [here](https://pytorch.org/docs/stable/nn.init.html) and pass in the paramter you want it to initialize, as well as the hyperparameters of the initializer. Try different initializations to find out which one work better. Here's an example:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["w = torch.empty(3, 5)\n","nn.init.xavier_normal_(w)\n","print_tensor_info('w', w)\n","print(w)"]},{"cell_type":"markdown","metadata":{},"source":["## Image data augmentation\n","\n","A very simple but important technique to prevent neural networks from overfitting to a small training dataset is data augmentation. Data augmentation is any transformation applied to input data that preserves the correct output. In PyTorch, many transforms are available in `torchvision.transforms.v2` that we imported as `v2`. `v2` also has basic preprocessing transforms like changing the dtype, scaling, and normalization. We encourage you to explore the [online documentation](https://pytorch.org/vision/stable/transforms.html) to learn more about what it has to offer and how to use it. We will show a simple one as an example. You can explore with other datasets and other transforms yourself!\n","\n","Note: Since this technique does not explicitly affect the neural network design or parameters, some may say that it is not technically regularization, and some call it implicit regularization."]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["# Loading the MNIST dataset from torchvision\n","\n","dataset = datasets.MNIST(\n","    root = 'MNIST',\n","    train = True,\n","    download = True,\n","    # transform the data to torch.Tensor and scale it to [0, 1]\n","    transform = v2.Compose([\n","        # augmentation:\n","        v2.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n","        # convert to tensor and scale to [0, 1]\n","        v2.ToImage(),\n","        v2.ToDtype(torch.float32, scale=True),\n","    ])\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ImageDataViz(dataset).show()"]},{"cell_type":"markdown","metadata":{},"source":["## L1/L2 regularization\n","\n","L1 and L2 regularization is another common method of regularization which simply penalizes the magnitude of the weights of the model. Let's look at the loss function of a model with these regularization and how the weight update changes.\n","\n","\n","### L1 Loss:\n","The total loss after adding the L1 regularization is:\n","$$\n","L = L_{pred} + \\lambda L_{reg} = \\sum_{samples} div(y_{pred}, y_{true}) + \\lambda \\sum_{i} |w_i|\n","$$\n","where $\\lambda$ is a hyperparameter in charge of the importance of the regularization term. The higher $\\lambda$ is set, the model is optimized towards having smaller weights. If you set it too high, the model is at the risk of underfitting due to the prediction loss having too little importance compared to the regularization loss.\n","\n","Let's look at the derivative of the loss now. \n","$$\n","\\frac{\\partial L}{\\partial w_i} = \\frac{\\partial L_{pred}}{\\partial w_i} + \\lambda\\frac{\\partial |w_i|}{\\partial w_i} \n","$$\n","The last term depends only on the sign of $w_i$:\n","$$\n","w_i > 0 \\rightarrow \\frac{\\partial |w_i|}{\\partial w_i} = 1\n","$$\n","$$\n","w_i < 0 \\rightarrow \\frac{\\partial |w_i|}{\\partial w_i} = -1\n","$$\n","Let's see what happens in the gradient descent update.\n","\n","$$\n","w_i > 0 \\rightarrow w_i(t+1) = w_i(t) - \\alpha (\\frac{\\partial L_{pred}}{\\partial w_i(t)} + \\lambda) = w_i(t) - \\alpha\\lambda - \\alpha\\frac{\\partial L_{pred}}{\\partial w_i(t)} \\\\\n","$$\n","$$\n","w_i < 0 \\rightarrow w_i(t+1) = w_i(t) - \\alpha (\\frac{\\partial L_{pred}}{\\partial w_i(t)} - \\lambda) = w_i(t) + \\alpha\\lambda - \\alpha\\frac{\\partial L_{pred}}{\\partial w_i(t)} \n","$$\n","As you can see, this regularization causes a constant term in the gradient update. Whether the weight is positive or negative, this term moves the weight closer to zero in each update. That is why this regularization causes sparse weights.\n","\n","L1 regularization is more common in shallow machine learning where sparse weights correspond to omitting some of the designed features and makes the model more understandable. In deep learning models, the number of parameters are actually a point of strength for the model, and sparsifying the weights has more downside than benefits.\n","\n","### L2 Loss:\n","The L2 regularization loss is more common in deep neural networks. Let's look at the total loss, and the gradient descent update to see the impact of this regularization. We will also see why it is called __weight decay__.\n","\n","$$\n","L = L_{pred} + \\lambda L_{reg} = \\sum_{samples} div(y_{pred}, y_{true}) + \\frac{\\lambda}{2} \\sum_{i} w_i^2\n","$$\n","$$\n","\\frac{\\partial L}{\\partial w_i} = \\frac{\\partial L_{pred}}{\\partial w_i} + \\lambda\\frac{\\partial w_i^2}{\\partial w_i} = \\frac{\\partial L_{pred}}{\\partial w_i} + \\lambda w_i\n","$$\n","$$\n","w_i(t+1) = w_i(t) - \\alpha (\\frac{\\partial L_{pred}}{\\partial w_i(t)} + \\lambda w_i(t)) = w_i(t) - \\alpha\\lambda w_i(t) - \\alpha\\frac{\\partial L_{pred}}{\\partial w_i(t)} \n","$$\n","The term corresponding to the regularization loss is $\\alpha\\lambda w_i$, which is interpreted as a shrinking (or decaying) term, Since this is not constant, it is free from some of the downsides of the L1 regularization. However, you should look for a good $\\lambda$ to avoid both underfitting and overfitting.\n","\n","As you saw above, this regularization can be directly implemented in the update step rather than the loss function. In PyTorch, you can add L2 regularization by passing `weight_decay` to your optimizer. You can see the next cell as an example:"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["model = nn.Linear(28*28, 10)\n","optimizer = optim.Adam(\n","    params = model.parameters(),\n","    lr = 0.001,\n","    weight_decay = 1e-5, # L2 regularization coefficient)\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Dropout\n","\n","Dropout is a regularization technique that randomly turns some neurons (or channels) of the feature map off, and scales the remaining ones up to roughly maintain the same range for the output. By doing so, the network avoids relying heavily on certain features that might only be dominant in the training data, and also helps with reducing the correlation of learned features. Dropout is only applied in training, and is turned off during inference.\n","\n","$$\n","\\text{Dropout}_p(x) =\n","\\begin{cases}\n","    0 & \\text{with probability  } p \\\\\n","    \\dfrac{x}{1-p}  & \\text{with probability  } 1-p\n","\\end{cases}\n","$$\n","\n","In PyTorch, you can use `nn.Dropout()` to turn off neurons randomly, or `nn.Dropout1D()`, `nn.Dropout2D()`, `nn.Dropout3D()` to randomly turn off random channels in 1D, 2D, or 3D data respectively. You can set the probability as one of the initialization arguments of your model. By doing so, you can assign the dropout probability to 0 if you do not want to use dropout.Let's take a closer look. Try changing the dropout probability and see the results. Run the cells multiple times to see if things are different each time. What do you notice? How does dropout behave differently during training and evaluation?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dropout = nn.Dropout(p=0.5)\n","\n","batch_size = 10\n","dim = 3\n","\n","T = torch.randn(batch_size, dim)\n","\n","print('T before dropout:')\n","print(T)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# run this cell multiple times to see the effect of dropout\n","dropout.train()\n","print('T after dropout in training mode:')\n","print(dropout(T))\n","# Compare the output of the dropout layer in training and evaluation mode\n","# what happens to the values that are not dropped out?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# run this cell multiple times to see the effect of dropout\n","dropout.eval()\n","print('T after dropout during evaluation:')\n","print(dropout(T))\n","# Compare the output of the dropout layer in training and evaluation mode"]},{"cell_type":"markdown","metadata":{},"source":["### Channel-wise dropout\n","\n","In spatial data with 1D, 2D or 3D structure, the feature dimension is also commonly refered to as the channel dimension. TO randomly turn off some feature maps (channels) in all locations, you can use `nn.Dropout1D()`, `nn.Dropout2D()`, `nn.Dropout3D()` for 1D, 2D, 3D data respectively. Here is an example for 2D (image) data:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dropout2d = nn.Dropout2d(p=0.5)\n","\n","# create a random tensor with 8 channels and 5x5 spatial dimensions\n","\n","batch_size, channels, height, width = 1, 8, 5, 5\n","\n","T = torch.randn(batch_size, channels, height, width)\n","\n","print('T before dropout2d:')\n","print(T)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dropout2d.train()\n","print('T after dropout2d in training mode:')\n","print(dropout2d(T))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["dropout2d.eval()\n","print('T after dropout2d during evaluation:')\n","print(dropout2d(T))"]},{"cell_type":"markdown","metadata":{},"source":["## Batch Normalization\n","\n","There are several types of normalizations in deep learning that try to contain the output values of the inner layers within a certain range or distribution. The one you learn about this week is Batch Normalization (called BatchNorm). As you can guess by the name, the normalization is done across the mini-batch. It first scales and shifts the input to a standard normal distribution $N(0,1)$, and then applies an affine (almost the same as linear) transform so the final output has a distribution of $N(\\mu,\\sigma^2)$ where $\\mu$ and $\\sigma$ are learnable parameters being updated by gradient steps. For each feature/channel dimension, the output of batchnorm is calculated as:\n","$$\n","BatchNorm(x) = \\mu + \\sigma\\cdot\\dfrac{x-E}{\\sqrt{V+\\epsilon}}\n","$$\n","where $E$ and $V$ are the mean and variance of the feature. $\\epsilon$ is a small positive number to avoid devision by zero. The same calculation is broadcasted for all samples and locations. However, we don't know the mean and variance of the features. Depending on whether we are doing training or evaluation, different values are used for $E$ and $V$:\n","- During __training__, the layer calculates mean and variance of x in the current mini-batch and uses it to shift and scale the data. It also uses them to calculate an exponential running average of the mean and variance of the feature over the whole dataset (let's call them $\\bar{E}$ and $\\bar{V}$). In the beginning, $\\bar{E}=0$ and $\\bar{V}=1$. They are stored in the module's __buffer__ and are updated with every batch. They are not learnable parameters to be updated with gradient steps. The new distribution's parameters ($\\mu$ and $\\sigma$) of the features (the trainable __parameters__ of batchnorm) are also being updated by the gradient-based optimizer. Here's all that happens in the forward pass during training:\n","$$\n","E = \\text{mean of $x$ over the current mini-batch}\n","$$\n","$$\n","V = \\text{variance of $x$ over the current mini-batch}\n","$$\n","$$\n","\\bar{E} = (1-m)\\bar{E} + mE\n","$$\n","$$\n","\\bar{V} = (1-m)\\bar{V} + mV\n","$$\n","$$\n","BatchNorm(x) = \\mu + \\sigma\\cdot\\dfrac{x-E}{\\sqrt{V+\\epsilon}}\n","$$\n","where $m$ is the momentum. In PyTorch, the default momentum is $0.1$.\n","- During __evaluation__, the layer uses $\\bar{E}$ and $\\bar{V}$ to shift and scale the input data to $N(0,1)$ and then applies the learned mean $\\mu$ and variance $\\sigma^2$. Since the layer has seen a lot of data after training, these are now better values. It also prevents the model's definition to differ based on the input mini-batch. Here's how it works during evaluation:\n","$$\n","BatchNorm(x) = \\mu + \\sigma\\cdot\\dfrac{x-\\bar{E}}{\\sqrt{\\bar{V}+\\epsilon}}\n","$$\n","\n","PyTorch has different BatchNorm layers you can use for different structures of data (1D, 2D, 3D). Let's take a look at `nn.BatchNorm2d()`:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# define a simple random tensor:\n","\n","batch_size, channels, height, width = 13, 3, 5, 4\n","\n","T = torch.randn(batch_size, channels, height, width)*20 + 100 # artificially skew the data\n","\n","stat_dims = [0, 2, 3]\n","\n","# Let's define a batch normalization layer\n","\n","print('T statistics over the batch dimension:')\n","print(f'mean:\\n{T.mean(stat_dims)}')\n","print(f'variance:\\n{T.var(stat_dims)}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Let's look at a batchnorm layer and what it has\n","\n","bn2d = nn.BatchNorm2d(num_features=channels)\n","bn2d"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Let's take a closer look into all the members (attributes) of the BatchNorm Layer\n","# Among the ones that start with an underline, look closely at _parameters and _buffers\n","vars(bn2d) # or bn2d.__dict__"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# train mode\n","# Try running this for more than once to see the effect of batch normalization. See how the running mean and variance change.\n","\n","bn2d_train = nn.BatchNorm2d(num_features=channels).train()\n","for i in range(100):\n","    T_bn = bn2d_train(T)\n","    if i % 10 == 0:\n","        print(f'Iteration {i+1}')\n","        print(bn2d_train._buffers)\n","        print(50*'-')\n","\n","\n","print(50*'=')\n","print('T_bn statistics over the batch dimension in training mode:')\n","print(f'mean:\\n{T_bn.mean(stat_dims)}')\n","print(f'standard deviation:\\n{T_bn.std(stat_dims)}')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# evaluation mode\n","# Try running this for more than once to see the effect of batch normalization. See how the running mean and variance change.\n","\n","bn2d_train = nn.BatchNorm2d(num_features=channels).eval()\n","for i in range(100):\n","    T_bn = bn2d_train(T)\n","    if i % 10 == 0:\n","        print(f'Iteration {i+1}')\n","        print(bn2d_train._buffers)\n","        print(50*'-')\n","\n","\n","print(50*'=')\n","print('T_bn statistics over the batch dimension in training mode:')\n","print(f'mean:\\n{T_bn.mean(stat_dims)}')\n","print(f'standard deviation:\\n{T_bn.std(stat_dims)}')"]}],"metadata":{"kernelspec":{"display_name":"DL_TA","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":2}