{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Deep Convolutional Generative Adversarial Networks (ConvGANs, DCGANs)\n","\n","The generator and discriminator of a GAN can have any architecture that suits the data they are trained on. For image data, the most common choice is convolutional layers. Let's take a look at a convolutional GAN."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from typing import Sequence, Union\n","from tqdm import tqdm\n","import numpy as np\n","import matplotlib.pyplot as plt\n","# plt.rcParams.update({'figure.autolayout': True})\n","from IPython.display import display, clear_output\n","\n","import torch\n","from torch import nn, optim\n","from torch.optim import lr_scheduler\n","from torch.nn import functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","from torchvision import datasets\n","from torchvision.transforms import v2\n","\n","if torch.cuda.is_available():\n","    Device = 'cuda'\n","elif torch.backends.mps.is_available():\n","    Device = 'mps'\n","else:\n","    Device = 'cpu'\n","print(f'Device is {Device}')"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["train_dataset = datasets.MNIST(\n","    root = 'MNIST',\n","    train = True,\n","    download = True,\n","    # transform the data to torch.Tensor and scale it to [0, 1]\n","    transform = v2.Compose([\n","        # convert to tensor and scale to [0, 1]\n","        v2.ToImage(),\n","        v2.ToDtype(torch.float32, scale=True),\n","    ])\n","    )"]},{"cell_type":"markdown","metadata":{},"source":["## Architecture of Generator and Discriminator"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Generator(nn.Module):\n","\n","    def __init__(\n","            self,\n","            latent_size: int = 32,\n","            hidden_sizes: Sequence[int] = [64, 128],\n","            initial_2D_shape: Sequence[int] = [16, 7, 7],\n","            hidden_channels: Sequence[int] = [8, 4, 4],\n","            batchnorm: bool = True,\n","            activation_name: str = 'ReLU',\n","            ):\n","        super().__init__()\n","\n","        activation = nn.__getattribute__(activation_name)\n","        self.latent_size = latent_size\n","        self.layers = nn.Sequential()\n","\n","        # Fully Connected Layers\n","        for i in range(len(hidden_sizes)):\n","            self.layers.append(\n","                nn.Linear(\n","                    in_features = latent_size if i == 0 else hidden_sizes[i-1],\n","                    out_features = hidden_sizes[i],\n","                    bias = False,\n","                    ))\n","            self.layers.append(activation())\n","\n","        # Transitioning from Fully Connected to Convolutional\n","        self.layers.append(\n","            nn.Linear(\n","                in_features = hidden_sizes[-1] if hidden_sizes else latent_size,\n","                out_features = np.prod(initial_2D_shape),\n","                bias = False,\n","                ))\n","        self.layers.append(nn.Unflatten(dim=1, unflattened_size=initial_2D_shape))\n","        if batchnorm:\n","            self.layers.append(nn.BatchNorm2d(initial_2D_shape[0]))\n","        self.layers.append(activation())\n","\n","        # Convolutional Layers\n","        for i in range(len(hidden_channels)):\n","            self.layers.append(\n","                nn.ConvTranspose2d(\n","                    in_channels = initial_2D_shape[0] if i==0 else hidden_channels[i-1],\n","                    out_channels = hidden_channels[i],\n","                    kernel_size = 4,\n","                    stride = 2,\n","                    padding = 1,\n","                    bias = False,\n","                    ))\n","            if batchnorm:\n","                self.layers.append(nn.BatchNorm2d(hidden_channels[i]))\n","            self.layers.append(activation())\n","\n","        self.layers.append(\n","            nn.Conv2d(\n","                in_channels = hidden_channels[-1] if hidden_channels else initial_2D_shape[0],\n","                out_channels = 1,\n","                kernel_size = 1,\n","                padding = 'same',\n","                bias = False,\n","                ))\n","        self.layers.append(nn.Sigmoid()) # coz output is in [0, 1]\n","\n","    def forward(\n","            self, \n","            z: torch.FloatTensor, # (batch_size, latent_size)\n","            ) -> torch.FloatTensor: # (batch_size, *output_shape)\n","        \"\"\"\n","        Input z is the latent vector, typically sampled from N(0, I)\n","        Outputs generated images\n","        \"\"\"\n","        return self.layers(z)\n","\n","    def generate(\n","            self,\n","            n_samples: int,\n","            device: str = Device,\n","            ) -> torch.FloatTensor: # (n_samples, *output_shape)\n","        self.to(device)\n","        z = torch.randn(n_samples, self.latent_size, device=device)\n","        return self(z)\n","\n","\n","class Discriminator(nn.Module):\n","\n","    def __init__(\n","            self,\n","            input_shape: Sequence[int] = [1, 28, 28],\n","            hidden_channels: Sequence[int] = [4, 8],\n","            hidden_sizes: Sequence[int] = [256, 128],\n","            batchnorm: bool = True,\n","            activation_name: str = 'ReLU',\n","            ):\n","        super().__init__()\n","\n","        activation = nn.__getattribute__(activation_name)\n","\n","        self.layers = nn.Sequential()\n","\n","        for i in range(len(hidden_channels)):\n","            self.layers.append(\n","                nn.Conv2d(\n","                    in_channels = input_shape[0] if i == 0 else hidden_channels[i-1],\n","                    out_channels = hidden_channels[i],\n","                    kernel_size = 4,\n","                    stride = 2,\n","                    padding = 1,\n","                    bias = False,\n","                    ))\n","            if batchnorm and i>0:\n","                self.layers.append(nn.BatchNorm2d(hidden_channels[i]))\n","            self.layers.append(activation())\n","\n","        self.layers.append(nn.Flatten())\n","\n","        flattened_size = hidden_channels[-1] * (input_shape[1] // 2**len(hidden_channels)) * (input_shape[2] // 2**len(hidden_channels))\n","\n","        for i in range(len(hidden_sizes)):\n","            \n","            self.layers.append(\n","                nn.Linear(\n","                    in_features = flattened_size if i == 0 else hidden_sizes[i-1],\n","                    out_features = hidden_sizes[i],\n","                    bias = False,\n","                    ))\n","            self.layers.append(activation())\n","\n","        self.layers.append(nn.Linear(hidden_sizes[-1] if hidden_sizes else flattened_size, 1))\n","        self.layers.append(nn.Sigmoid()) # Because it is a binary classification (REAL or FAKE)\n","\n","    def forward(\n","            self,\n","            x: torch.FloatTensor, # (batch_size, *input_shape)\n","            ) -> torch.FloatTensor: # (batch_size, 1)\n","        \n","        return self.layers(x)"]},{"cell_type":"markdown","metadata":{},"source":["## Tracking and Visualization"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["class GAN_Tracker:\n","    \"\"\"\n","    Logs and plots different loss terms of a GAN during training.\n","    \"\"\"\n","    def __init__(\n","            self, \n","            n_iters: int,\n","            plot_freq: Union[int, None] = None, # plot every plot_freq iterations\n","            ):\n","        \n","        self.real_scores = []\n","        self.fake_scores = []\n","        self.D_losses = []\n","        self.G_losses = []\n","\n","        self.plot = plot_freq is not None\n","        self.iter = 0\n","        self.n_iters = n_iters\n","\t\t\n","        if self.plot:\n","            self.plot_freq = plot_freq\n","            self.plot_results()\n","\n","\n","    def plot_results(self):\n","        self.fig, (self.ax1, self.ax2) = plt.subplots(1, 2, figsize=(13, 3), sharex=True)\n","\n","        # Score plot:\n","        self.real_score_curve, = self.ax1.plot(\n","            range(1, self.iter+1),\n","            self.real_scores,\n","\t\t\tlabel = r'$D(x)$',\n","            )\n","        self.fake_score_curve, = self.ax1.plot(\n","            range(1, self.iter+1),\n","            self.fake_scores,\n","            label = r'$D(G(z))$',\n","            )\n","\n","        self.ax1.set_xlim(0, self.n_iters+1)\n","        self.ax1.set_ylim(0, 1)\n","        self.ax1.set_xlabel('Iteration')\n","        self.ax1.set_ylabel('Discriminator Score')\n","        self.ax1.set_title('Discriminator Score')\n","        self.ax1.grid(linestyle='--')\n","        self.ax1.legend()\n","\n","        # Loss plot:\n","        self.D_loss_curve, = self.ax2.plot(\n","            range(1, self.iter+1),\n","            self.D_losses,\n","\t\t\tlabel = 'D',\n","            )\n","        self.G_loss_curve, = self.ax2.plot(\n","            range(1, self.iter+1),\n","            self.G_losses,\n","            label = 'G',\n","            )\n","        self.ax2.set_xlim(0, self.n_iters+1)\n","        self.ax2.set_xlabel('Iteration')\n","        self.ax2.set_ylabel('Loss')\n","        self.ax2.set_title('Learning Curve')\n","        self.ax2.grid(linestyle='--')\n","        self.ax2.legend()\n","\n","        self.samples_fig, self.samples_axes = plt.subplots(4, 15, figsize=(12, 4), sharex=True, sharey=True)\n","        self.sample_axes = self.samples_axes.flat\n","        self.samples = []\n","        for ax in self.sample_axes:\n","            ax.axis('off')\n","            self.samples.append(ax.imshow(np.zeros((28, 28)), cmap='gray', vmin=0, vmax=1))\n","\n","\n","    def update(\n","            self, \n","            real_score: float,\n","            fake_score: float,\n","            D_loss: float,\n","            G_loss: float,\n","            ):\n","        self.real_scores.append(real_score)\n","        self.fake_scores.append(fake_score)\n","        self.D_losses.append(D_loss)\n","        self.G_losses.append(G_loss)\n","        self.iter += 1\n","\t\t\n","        if self.plot and self.iter % self.plot_freq == 0:\n","\n","            # score plot:\n","            self.real_score_curve.set_data(range(1, self.iter+1), self.real_scores)\n","            self.fake_score_curve.set_data(range(1, self.iter+1), self.fake_scores)\n","            self.ax1.relim()\n","            self.ax1.autoscale_view()\n","\n","            # loss plot:\n","            self.D_loss_curve.set_data(range(1, self.iter+1), self.D_losses)\n","            self.G_loss_curve.set_data(range(1, self.iter+1), self.G_losses)\n","            self.ax2.relim()\n","            self.ax2.autoscale_view()\n","\n","            self.samples_fig.suptitle(f'Generated Samples at Iteration {self.iter}')\n","\n","            self.fig.canvas.draw()\n","            clear_output(wait=True)\n","            display(self.fig)\n","            display(self.samples_fig)\n","\n","    \n","    def get_samples(\n","            self, \n","            samples: torch.FloatTensor, # (n_samples, *output_shape)\n","            ):\n","        for sample, sample_img in zip(samples, self.samples):\n","            sample_img.set_data(sample.detach().squeeze().cpu().numpy())\n","            "]},{"cell_type":"markdown","metadata":{},"source":["## Losses "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def D_real_loss_fn(\n","        D_real: torch.FloatTensor, # (batch_size, 1)\n","        ) -> torch.FloatTensor: # ()\n","    \"\"\"\n","    D_real is D(x), the discriminator's output when fed with real images\n","    We want this to be close to 1, because the discriminator should recognize real images\n","    \"\"\"\n","    return F.binary_cross_entropy(D_real, torch.ones_like(D_real, requires_grad=False))\n","\n","\n","def D_fake_loss_fn(\n","        D_fake: torch.FloatTensor, # (batch_size, 1)\n","        ) -> torch.FloatTensor: # ()\n","    \"\"\"\n","    D_fake is D(G(z)), the discriminator's output when fed with generated images\n","    We want this to be close to 0, because the discriminator should not be fooled\n","    \"\"\"\n","    return F.binary_cross_entropy(D_fake, torch.zeros_like(D_fake, requires_grad=False))\n","\n","\n","def G_loss_fn(\n","        D_fake: torch.FloatTensor, # (batch_size, 1)\n","        ) -> torch.FloatTensor: # ()\n","    \"\"\"\n","    D_fake is D(G(z)), the discriminator's output when fed with generated images\n","    We want this to be close to 1, because the generator wants to fool the discriminator\n","    \"\"\"\n","    return F.binary_cross_entropy(D_fake, torch.ones_like(D_fake, requires_grad=False))"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_GAN(\n","        generator: Generator,\n","        discriminator: Discriminator,\n","        train_dataset: Dataset,\n","        device: str = Device,\n","        plot_freq: int = 100,\n","        \n","        # Generator \n","        optimizer_name_G: str = 'Adam',\n","        optimizer_config_G: dict = dict(),\n","        lr_scheduler_name_G: Union[str, None] = None,\n","        lr_scheduler_config_G: dict = dict(),\n","\n","        # Discriminator\n","        optimizer_name_D: str = 'Adam',\n","        optimizer_config_D: dict = dict(),\n","        lr_scheduler_name_D: Union[str, None] = None,\n","        lr_scheduler_config_D: dict = dict(),\n","\n","        n_iters: int = 2000,\n","        batch_size: int = 128,\n","        ):\n","    \n","    generator = generator.to(device)\n","    discriminator = discriminator.to(device)\n","\n","    optimizer_G: optim.Optimizer = optim.__getattribute__(optimizer_name_G)(generator.parameters(), **optimizer_config_G)\n","    if lr_scheduler_name_G is not None:\n","        lr_scheduler_G: lr_scheduler._LRScheduler = lr_scheduler.__getattribute__(lr_scheduler_name_G)(optimizer_G, **lr_scheduler_config_G)\n","\n","    optimizer_D: optim.Optimizer = optim.__getattribute__(optimizer_name_D)(discriminator.parameters(), **optimizer_config_D)\n","    if lr_scheduler_name_D is not None:\n","        lr_scheduler_D: lr_scheduler._LRScheduler = lr_scheduler.__getattribute__(lr_scheduler_name_D)(optimizer_D, **lr_scheduler_config_D)\n","\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n","\n","    tracker = GAN_Tracker(n_iters=n_iters, plot_freq=plot_freq)\n","\n","    iter_pbar = tqdm(range(n_iters), desc='Training', unit='iter')\n","    iter = 0\n","\n","    while iter < n_iters:\n","\n","        for x_real, _ in train_loader:\n","\n","            x_real = x_real.to(device)\n","            n_samples = len(x_real)\n","\n","            # ============ Train Discriminator ============\n","            generator.train(False).requires_grad_(False)\n","            discriminator.train(True).requires_grad_(True)\n","\n","            # Real data\n","            D_real = discriminator(x_real)\n","            D_real_loss = D_real_loss_fn(D_real)\n","\n","            # Fake data\n","            x_fake = generator.generate(n_samples, device=device).detach() # Detach to discard computationa graph and save memory\n","            assert not x_fake.requires_grad\n","            D_fake = discriminator(x_fake)\n","            D_fake_loss = D_fake_loss_fn(D_fake)\n","\n","            D_loss = (D_real_loss + D_fake_loss) / 2\n","            D_loss.backward()\n","            optimizer_D.step()\n","            if lr_scheduler_name_D is not None:\n","                lr_scheduler_D.step()\n","            optimizer_D.zero_grad()\n","\n","            D_real_avg = D_real.mean().item() # For logging\n","            D_fake_avg = D_fake.mean().item() # For logging\n","            D_loss_item = D_loss.item() # For logging\n","\n","            # ============ Train Generator ============\n","            generator.train(True).requires_grad_(True)\n","            discriminator.train(False).requires_grad_(False)\n","\n","            x_fake = generator.generate(n_samples)\n","            D_fake = discriminator(x_fake)\n","            G_loss = G_loss_fn(D_fake)\n","\n","            G_loss.backward()\n","            optimizer_G.step()\n","            if lr_scheduler_name_G is not None:\n","                lr_scheduler_G.step()\n","            optimizer_G.zero_grad()\n","\n","            G_loss_item = G_loss.item() # For logging\n","\n","            # ============ Logging =================\n","            iter += 1\n","            iter_pbar.update(1)\n","            if iter % plot_freq == 0:\n","                with torch.inference_mode():\n","                    tracker.get_samples(generator.generate(n_samples=60, device=device))\n","            tracker.update(D_real_avg, D_fake_avg, D_loss_item, G_loss_item)\n","\n","            if iter >= n_iters:\n","                break"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\"\"\"\n","Search for good hyperparameters (difficult)\n","\"\"\"\n","generator_config = dict(\n","    latent_size = 64,\n","    hidden_sizes = [],\n","    initial_2D_shape = [64, 7, 7],\n","    hidden_channels = [32, 16],\n","    batchnorm = True,\n","    activation_name = 'ReLU',\n","    )\n","\n","discriminator_config = dict(\n","    input_shape = [1, 28, 28],\n","    hidden_channels = [16, 23],\n","    hidden_sizes = [],\n","    batchnorm = True,\n","    activation_name = 'LeakyReLU',\n","    )\n","\n","train_config = dict(\n","    # Generator\n","    optimizer_name_G = 'Adam',\n","    optimizer_config_G = dict(),\n","    lr_scheduler_name_G = None,\n","    lr_scheduler_config_G = dict(),\n","\n","    # Discriminator\n","    optimizer_name_D = 'Adam',\n","    optimizer_config_D = dict(),\n","    lr_scheduler_name_D = None,\n","    lr_scheduler_config_D = dict(),\n","\n","    n_iters = 50000,\n","    batch_size = 64,\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["if __name__ == '__main__':\n","    generator = Generator(**generator_config)\n","    discriminator = Discriminator(**discriminator_config)\n","    train_GAN(\n","        generator = generator, \n","        discriminator = discriminator, \n","        train_dataset = train_dataset,\n","        device = Device,\n","        plot_freq = 500,\n","        **train_config,\n","        )"]}],"metadata":{"kernelspec":{"display_name":"DL_TA","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"}},"nbformat":4,"nbformat_minor":2}