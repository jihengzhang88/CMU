{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Neural Networks (GNN)\n",
    "\n",
    "This week, you learned about Graph Neural Networks. GNNs are simply neural networks for graphs, so if you are familiar with graphs, the rest is straightforward.\n",
    "\n",
    "\n",
    "A graph is a set of nodes (vertices) that are connected to each other via edges. You should have gotten familiar with some examples in the lectures about what kinds of data can be represented using graphs.\n",
    "\n",
    "In order to analyze a graph using a neural network, we need to represent each node as a usual input to a neural network, like a feature vector. One can also represent edges as feature vectors, but that may introduce more complexity to the model. In the simplest setting, the edges indicate which nodes can pass a messsage to the node. Some edges have directions and information only  flows in the specified direction.\n",
    "\n",
    "Most supervised learning tasks on graphs are about perdicting a target for either the whole graph or for each node. Here, we go over a node classification task and a graph regression task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[86 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/placeholder.py -> build/lib.linux-x86_64-cpython-311/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/testing.py -> build/lib.linux-x86_64-cpython-311/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/__init__.py -> build/lib.linux-x86_64-cpython-311/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/scatter.py -> build/lib.linux-x86_64-cpython-311/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/segment_coo.py -> build/lib.linux-x86_64-cpython-311/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/segment_csr.py -> build/lib.linux-x86_64-cpython-311/torch_scatter\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/utils.py -> build/lib.linux-x86_64-cpython-311/torch_scatter\n",
      "  \u001b[31m   \u001b[0m creating build/lib.linux-x86_64-cpython-311/torch_scatter/composite\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/composite/__init__.py -> build/lib.linux-x86_64-cpython-311/torch_scatter/composite\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/composite/std.py -> build/lib.linux-x86_64-cpython-311/torch_scatter/composite\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/composite/logsumexp.py -> build/lib.linux-x86_64-cpython-311/torch_scatter/composite\n",
      "  \u001b[31m   \u001b[0m copying torch_scatter/composite/softmax.py -> build/lib.linux-x86_64-cpython-311/torch_scatter/composite\n",
      "  \u001b[31m   \u001b[0m running egg_info\n",
      "  \u001b[31m   \u001b[0m writing torch_scatter.egg-info/PKG-INFO\n",
      "  \u001b[31m   \u001b[0m writing dependency_links to torch_scatter.egg-info/dependency_links.txt\n",
      "  \u001b[31m   \u001b[0m writing requirements to torch_scatter.egg-info/requires.txt\n",
      "  \u001b[31m   \u001b[0m writing top-level names to torch_scatter.egg-info/top_level.txt\n",
      "  \u001b[31m   \u001b[0m reading manifest file 'torch_scatter.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m reading manifest template 'MANIFEST.in'\n",
      "  \u001b[31m   \u001b[0m warning: no previously-included files matching '*' found under directory 'test'\n",
      "  \u001b[31m   \u001b[0m adding license file 'LICENSE'\n",
      "  \u001b[31m   \u001b[0m writing manifest file 'torch_scatter.egg-info/SOURCES.txt'\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"<string>\", line 2, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "  \u001b[31m   \u001b[0m   File \"/tmp/pip-install-joe73n9y/torch-scatter_6d8f61c2999c48838d7eee904625153f/setup.py\", line 120, in <module>\n",
      "  \u001b[31m   \u001b[0m     setup(\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/site-packages/setuptools/__init__.py\", line 103, in setup\n",
      "  \u001b[31m   \u001b[0m     return distutils.core.setup(**attrs)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 185, in setup\n",
      "  \u001b[31m   \u001b[0m     return run_commands(dist)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/site-packages/setuptools/_distutils/core.py\", line 201, in run_commands\n",
      "  \u001b[31m   \u001b[0m     dist.run_commands()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 969, in run_commands\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/site-packages/setuptools/dist.py\", line 989, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/site-packages/wheel/bdist_wheel.py\", line 364, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(\"build\")\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/site-packages/setuptools/dist.py\", line 989, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/site-packages/setuptools/_distutils/command/build.py\", line 131, in run\n",
      "  \u001b[31m   \u001b[0m     self.run_command(cmd_name)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/site-packages/setuptools/_distutils/cmd.py\", line 318, in run_command\n",
      "  \u001b[31m   \u001b[0m     self.distribution.run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/site-packages/setuptools/dist.py\", line 989, in run_command\n",
      "  \u001b[31m   \u001b[0m     super().run_command(command)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/site-packages/setuptools/_distutils/dist.py\", line 988, in run_command\n",
      "  \u001b[31m   \u001b[0m     cmd_obj.run()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/site-packages/setuptools/command/build_ext.py\", line 88, in run\n",
      "  \u001b[31m   \u001b[0m     _build_ext.run(self)\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/site-packages/setuptools/_distutils/command/build_ext.py\", line 345, in run\n",
      "  \u001b[31m   \u001b[0m     self.build_extensions()\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 511, in build_extensions\n",
      "  \u001b[31m   \u001b[0m     compiler_name, compiler_version = self._check_abi()\n",
      "  \u001b[31m   \u001b[0m                                       ^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 897, in _check_abi\n",
      "  \u001b[31m   \u001b[0m     _, version = get_compiler_abi_compatibility_and_version(compiler)\n",
      "  \u001b[31m   \u001b[0m                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 361, in get_compiler_abi_compatibility_and_version\n",
      "  \u001b[31m   \u001b[0m     if not check_compiler_ok_for_platform(compiler):\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/site-packages/torch/utils/cpp_extension.py\", line 313, in check_compiler_ok_for_platform\n",
      "  \u001b[31m   \u001b[0m     which = subprocess.check_output(['which', compiler], stderr=subprocess.STDOUT)\n",
      "  \u001b[31m   \u001b[0m             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/subprocess.py\", line 466, in check_output\n",
      "  \u001b[31m   \u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/opt/conda/lib/python3.11/subprocess.py\", line 571, in run\n",
      "  \u001b[31m   \u001b[0m     raise CalledProcessError(retcode, process.args,\n",
      "  \u001b[31m   \u001b[0m subprocess.CalledProcessError: Command '['which', 'g++']' returned non-zero exit status 1.\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for torch-scatter\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Could not build wheels for torch-scatter, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from typing import Sequence\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# torch_geometric for Graph Neural Networks:\n",
    "try:\n",
    "    import torch_geometric as gtorch\n",
    "except ImportError:\n",
    "    os.system('pip install torch_geometric -qq')\n",
    "    os.system('pip install torch-scatter -qq')\n",
    "    import torch_geometric as gtorch\n",
    "import torch_geometric.nn as gnn\n",
    "import torch_geometric.data as gdata\n",
    "import torch_geometric.datasets as gdatasets\n",
    "from torch_geometric.loader import DataLoader as gDataLoader\n",
    "import torch_geometric.transforms as gtransforms\n",
    "\n",
    "# rdkit for cheminformatics:\n",
    "try:\n",
    "    import rdkit\n",
    "except ImportError:\n",
    "    os.system('pip install rdkit-pypi -qq')\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    Device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    Device = 'mps'\n",
    "else:\n",
    "    Device = 'cpu'\n",
    "\n",
    "print(f'Using {Device} device')\n",
    "\n",
    "\n",
    "def print_tensor_info(\n",
    "        name: str, \n",
    "        tensor, # torch.Tensor\n",
    "        ):\n",
    "    print(f'{name}')\n",
    "    print(20*'-')\n",
    "    if not isinstance(tensor, torch.Tensor):\n",
    "        print(f'It is {type(tensor).__name__}!')\n",
    "        print(20*'='+'\\n')\n",
    "        return\n",
    "    # print name, shhape, dtype, device, require_grad\n",
    "    print(f'shape: {tensor.shape}')\n",
    "    print(f'dtype: {tensor.dtype}')\n",
    "    print(f'device: {tensor.device}')\n",
    "    print(f'requires_grad: {tensor.requires_grad}')\n",
    "    print(20*'='+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Data Structure\n",
    "\n",
    "We use [`torch_geometric`](https://pytorch-geometric.readthedocs.io/en/stable/), which we import as `gtorch`, to represent data as graphs and define graph neural networks that can analyze them. A graph is formally defined with a set of vertices ($V$) (also called nodes) and edges ($E$) connecting those vertices. The number of vertices and edges are denoted as $|V|$ and $|E|$ respectively. In `gtorch`, a graph is defined by these matrices:\n",
    "\n",
    "- $X \\in \\mathbb{R}^{|V|\\times d_v}$ containing vertex features. Each row represents a vertex (node) as a feature vector of size $d_v$.\n",
    "\n",
    "- $I \\in \\{0, 1, ..., |V|-1\\}^{2\\times |E|}$ containing the index of the nodes at the two ends of each edge. Each column corresponds to one edge, where the first element is the index of the source node and the second element is the index of the target node. The source node is a neighbor of the target node, since it can send messages to it.\n",
    "\n",
    "- Optional: $E \\in \\mathbb{R}^{|E|\\times d_e}$ containing edge attributes. Each row corresponds to the feature representation of an edge as a vector of size $d_e$.\n",
    "\n",
    "Let's take a look at a dataset where each sample is a graph. We are going to use the [Cora](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.Planetoid.html) dataset that consists of 2708 scientific publications classified as one of seven classes. Let's take a look at the dataset:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cora_dataset = gdatasets.Planetoid(\n",
    "    root = 'week5-data',\n",
    "    name = 'Cora',\n",
    "    transform = gtransforms.NormalizeFeatures(),\n",
    ")\n",
    "\n",
    "if isinstance(cora_dataset, Dataset):\n",
    "    n = len(cora_dataset)\n",
    "    print(f'Number of samples in the dataset: {n}')\n",
    "    sample = cora_dataset[0]\n",
    "    print_tensor_info('Sample', sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(cora_dataset, gdata.Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a new data type used to represent homogeneous graphs in `gtorch`. You can read more about it, as well as other data classes [here](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_tensor_info('x', sample.x)\n",
    "print_tensor_info('edge_index', sample.edge_index)\n",
    "print_tensor_info('y', sample.y)\n",
    "print_tensor_info('train_mask', sample.train_mask)\n",
    "print_tensor_info('val_mask', sample.val_mask)\n",
    "print_tensor_info('test_mask', sample.test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting information about the dataset that we need for defining the model:\n",
    "\n",
    "num_classes = cora_dataset.num_classes\n",
    "num_features = cora_dataset.num_features\n",
    "\n",
    "print(f'num_classes: {num_classes}')\n",
    "print(f'num_features: {num_features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Message Passing\n",
    "\n",
    "In graph neural networks, the exchange of information between connected nodes (vertices) is commonly known as message passing. Let's represent the feature vector of node $i$ as $x_i$. The message passing operation updates the nodes as shown below:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_i' = \\gamma_\\Theta \\left( \\mathbf{x}_i, \\bigoplus_{j \\in \\mathcal{N}(i)} \\phi_\\Theta \\left( \\mathbf{x}_i, \\mathbf{x}_j, \\mathbf{e}_{j,i} \\right) \\right)\n",
    "$$\n",
    "\n",
    "The update consists of three steps. First, $\\phi_\\Theta$ calculates the message from every neighbor $j$ of node $i$ based on both their values, as well as the optional weight of the edge that connects them. Then, the information from all neighbors is pooled using a permutation-invariant operation $\\bigoplus$ (min, max, mean, sum, etc). Finally, the updated value of the node for the next layer is calculated by $\\gamma_\\Theta$ using its current value and the aggregated message from all its neighbors. There is a base class [`MessagePassing`](https://pytorch-geometric.readthedocs.io/en/stable/generated/torch_geometric.nn.conv.MessagePassing.html#torch_geometric.nn.conv.MessagePassing) that you can use to define your custom message passing layer.\n",
    "\n",
    "## Graph Convolution Operation\n",
    "Any model that falls into the definition of message passing can be used as a layer of a graph neural network. A prominent layer used in graph neural networks is the graph convolution layer [GCNConv](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.conv.GCNConv.html). You can learn more about it in the online documentation or the paper. Like `torch`, `gtorch` also has its `nn` (which we imported as `gnn`) and you can look at the modules it offers and how to use them in the [online documentation](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html). Let's use `GCNConv` from `gnn.conv`. If you wonder why it is called colvolution, it is because of the same central property from our classic convolution: **parameter sharing**. The same operation with the same parameters is applied to every node and its neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_layer = gnn.conv.GCNConv(\n",
    "    in_channels = num_features,\n",
    "    out_channels = 142,\n",
    "    cached = True, # If you have only one graph, use cached=True for better performance.\n",
    ").to(Device)\n",
    "\n",
    "output = gcn_layer(sample.x.to(Device), sample.edge_index.to(Device))\n",
    "\n",
    "print_tensor_info('output', output)\n",
    "\n",
    "print(f'Is gcn_layer, an instance og nn.Module? {isinstance(gcn_layer, nn.Module)}')\n",
    "print(f'Is gcn_layer, an instance of gnn.conv.MessagePassing? {isinstance(gcn_layer, gnn.conv.MessagePassing)}')\n",
    "print(gcn_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_node(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_features: int,\n",
    "            num_classes: int,\n",
    "            hidden_dims: Sequence[int],\n",
    "            activation: str = 'ReLU',\n",
    "            dropout: float = 0.0,\n",
    "            ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        dims = [in_features] + hidden_dims\n",
    "        n_graph_layers = len(hidden_dims)\n",
    "        for i in range(n_graph_layers):\n",
    "            self.layers.append(\n",
    "                gnn.conv.GCNConv(\n",
    "                    in_channels = dims[i],\n",
    "                    out_channels = dims[i+1],\n",
    "                    cached = True, # Use this if the whole data is one big graph\n",
    "                    )\n",
    "                    )\n",
    "            \n",
    "        self.act = F.__getattribute__(activation)\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.out_layer = nn.Linear(dims[-1], num_classes)\n",
    "\n",
    "    def forward(\n",
    "            self, \n",
    "            x: torch.FloatTensor, # (N_nodes, in_features)\n",
    "            edge_index: torch.LongTensor, # (2, N_edges)\n",
    "            ) -> torch.FloatTensor: # (N_nodes, num_classes)\n",
    "        \n",
    "        x = x.to(torch.float)\n",
    "        edge_index = edge_index.to(torch.long)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, edge_index)\n",
    "            x = self.act(x)\n",
    "            x = F.dropout(x, p=self.dropout, training=self.training) # make sure to pass training=self.training when using F.dropout\n",
    "            \n",
    "        x = self.out_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN_node(\n",
    "    in_features = num_features,\n",
    "    num_classes = num_classes,\n",
    "    hidden_dims = [64, 128, 64],\n",
    "    activation = 'relu',\n",
    "    dropout = 0.1,\n",
    ").to(Device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.enable_grad()\n",
    "def train(\n",
    "        model: nn.Module,\n",
    "        dataset: Dataset,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        loss_fn: nn.Module,\n",
    "        epochs: int,\n",
    "        device: str = Device,\n",
    "        ):\n",
    "    \n",
    "    epoch_pbar = tqdm(range(epochs), leave=True)\n",
    "    model.to(device)\n",
    "    sample = dataset[0].to(Device)\n",
    "    for epoch in epoch_pbar:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(sample.x, sample.edge_index)\n",
    "        target = sample.y\n",
    "        train_loss = loss_fn(output[sample.train_mask], target[sample.train_mask])\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            train_acc = (output[sample.train_mask].argmax(dim=1) == target[sample.train_mask]).float().mean()\n",
    "            val_loss = loss_fn(output[sample.val_mask], target[sample.val_mask])\n",
    "            val_acc = (output[sample.val_mask].argmax(dim=1) == target[sample.val_mask]).float().mean()\n",
    "            test_loss = loss_fn(output[sample.test_mask], target[sample.test_mask])\n",
    "            test_acc = (output[sample.test_mask].argmax(dim=1) == target[sample.test_mask]).float().mean()\n",
    "\n",
    "        postfix_str = f\"train: loss {train_loss.item():.6f}, acc {train_acc:.3f} | \"\n",
    "        postfix_str += f\"val: loss {val_loss.item():.3f}, acc {val_acc:.3f} | \"\n",
    "        postfix_str += f\"test: loss {test_loss.item():.3f}, acc {test_acc:.3f}\"\n",
    "        epoch_pbar.set_postfix_str(postfix_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNN_node(\n",
    "    in_features = num_features,\n",
    "    num_classes = num_classes,\n",
    "    hidden_dims = [64, 128, 64],\n",
    "    activation = 'leaky_relu',\n",
    "    dropout = 0.0,\n",
    ")\n",
    "model\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss = nn.CrossEntropyLoss()\n",
    "\n",
    "train(model, cora_dataset, optimizer, loss, epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split of train/val/test here is actually a bit problematic. Can you tell why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Regression\n",
    "Now let's try a graph regression task, which is predicting a real value for the whole graph. For this, we will use [`MoleculeNet`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.datasets.MoleculeNet.html) dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moleculenet_dataset = gdatasets.MoleculeNet(\n",
    "    root = 'week5-data',\n",
    "    name = 'Lipo',\n",
    ")\n",
    "\n",
    "if isinstance(moleculenet_dataset, Dataset):\n",
    "    n = len(moleculenet_dataset)\n",
    "    print(f'Number of samples in the dataset: {n}')\n",
    "    sample = moleculenet_dataset[0]\n",
    "    print_tensor_info('Sample', sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moleculenet_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moleculenet_dataset[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we have many small graphs instead of one huge graph. How do we create mini-batches of graphs? Each has a different number of nodes and edges, so we cannot simply stack them along a new batch dimension like before!\n",
    "\n",
    "The answer is simple. A mini-batch of several graphs, is just a big graph that consists of those graphs. There is no edge connecting the nodes from different subgraphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"data:image/png;base64,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\" alt=\"image\" width=\"30%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "gdata_loader = gDataLoader(\n",
    "    dataset = moleculenet_dataset,\n",
    "    batch_size = batch_size,\n",
    "    shuffle = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample0, sample1, sample2 = moleculenet_dataset[:3]\n",
    "\n",
    "batch0 = next(iter(gdata_loader))\n",
    "\n",
    "for i in range(batch_size):\n",
    "    print(f'sample {i}')\n",
    "    print(moleculenet_dataset[i])\n",
    "    print(20*'-')\n",
    "\n",
    "print()\n",
    "print('The first mini-batch containing the first 3 graphs')\n",
    "print(batch0)\n",
    "print(50*'='+'\\n')\n",
    "print('There are new fields in the batched graph that help you revocer the subgraphs:\\n')\n",
    "\n",
    "print_tensor_info('batch0.batch', batch0.batch)\n",
    "print_tensor_info('batch0.ptr', batch0.ptr)\n",
    "print(f'batch.ptr: {batch0.ptr}')\n",
    "\n",
    "print(\"Take a look at the actual features of nodes:\")\n",
    "print_tensor_info('batch0.x', batch0.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Pooling\n",
    "The final component to complete our toolkit for graph-level prediction, is how to aggregate the feature vectors of all the nodes and get a single output from it. Such operation is called  a global pooling. You can find options for pooling layers of `gtorch` [here](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#pooling-layers). We'll use [`global_max_pooling`](https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.nn.pool.global_max_pool.html#torch_geometric.nn.pool.global_max_pool) for our example. You have to be careful to pass the batch field if you have batched graphs, to specify the subgraphs in the batched graph to be pooled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_function = gnn.pool.global_max_pool # It is a function, not a Module\n",
    "\n",
    "\n",
    "output = pool_function(\n",
    "    x = batch0.x,\n",
    "    batch = batch0.batch, # so the function knows which subgraph each node belongs to\n",
    "    )\n",
    "print_tensor_info('output after global max pooling', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a graph neural network for graph regression. It is basically the same, except we have some global pooling, and some more layers after it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN_graph(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "\n",
    "            node_features: int,\n",
    "            node_hidden_dims: Sequence[int],\n",
    "            graph_hidden_dims: Sequence[int],\n",
    "            out_features: int,\n",
    "            activation: str = 'relu',\n",
    "            pool: str = 'max', # max, mean, add\n",
    "            ):\n",
    "        super().__init__()\n",
    "            \n",
    "        self.act = F.__getattribute__(activation)\n",
    "\n",
    "        self.node_layers = nn.ModuleList()\n",
    "        n_node_layers = len(node_hidden_dims)\n",
    "        node_dims = [node_features] + node_hidden_dims\n",
    "        for i in range(n_node_layers):\n",
    "            self.node_layers.append(\n",
    "                gnn.conv.GCNConv(\n",
    "                    in_channels = node_dims[i],\n",
    "                    out_channels = node_dims[i+1],\n",
    "                    )\n",
    "                )\n",
    "            \n",
    "        self.pool = gnn.pool.__getattribute__(f'global_{pool}_pool')\n",
    "\n",
    "        self.graph_layers = nn.ModuleList()\n",
    "        n_graph_layers = len(graph_hidden_dims)\n",
    "        graph_dims = [node_dims[-1]] + graph_hidden_dims\n",
    "        for i in range(n_graph_layers):\n",
    "            self.graph_layers.append(\n",
    "                nn.Linear(\n",
    "                    in_features = graph_dims[i],\n",
    "                    out_features = graph_dims[i+1],\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        self.out_layer = nn.Linear(graph_dims[-1], out_features)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x: torch.FloatTensor, # Batched input\n",
    "            edge_index: torch.LongTensor,\n",
    "            batch: torch.LongTensor,\n",
    "            ) -> torch.FloatTensor:\n",
    "        \n",
    "        # Always make sure dtypes are correct\n",
    "        x = x.to(torch.float)\n",
    "        edge_index = edge_index.to(torch.long)\n",
    "        batch = batch.to(torch.long)\n",
    "\n",
    "        # Node level layers. x is of shape [N_nodes, node_feature] in these layers\n",
    "        for layer in self.node_layers:\n",
    "            x = layer(x, edge_index)\n",
    "            x = self.act(x)\n",
    "\n",
    "        # The pooling layer extracts one feature vector per graph\n",
    "        x = self.pool(x, batch)\n",
    "        # Now, x is of shape [N_graphs, node_hidden_dims[-1]]\n",
    "\n",
    "        # The remaining layers are just like classic feed forward layer\n",
    "        for layer in self.graph_layers:\n",
    "            x = layer(x)\n",
    "            x = self.act(x)\n",
    "        \n",
    "        x = self.out_layer(x)\n",
    "\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.enable_grad()\n",
    "def train_epoch(\n",
    "        model: nn.Module,\n",
    "        loader: DataLoader,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        loss_fn: nn.Module,\n",
    "        device = Device,\n",
    "        ):\n",
    "    \n",
    "    model.train().to(device)\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch = batch.to(device)\n",
    "        output = model(batch.x, batch.edge_index, batch.batch)\n",
    "        target = batch.y\n",
    "        loss = loss_fn(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def eval_epoch(\n",
    "        model: nn.Module,\n",
    "        loader: DataLoader,\n",
    "        loss_fn: nn.Module,\n",
    "        device = Device,\n",
    "        ):\n",
    "    n = len(loader.dataset)\n",
    "    model.eval().to(device)\n",
    "    Loss = 0.\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        output = model(batch.x, batch.edge_index, batch.batch)\n",
    "        target = batch.y\n",
    "        loss = loss_fn(output, target)\n",
    "        if loss_fn.reduction == 'sum':\n",
    "            Loss += loss.item()\n",
    "        elif loss_fn.reduction == 'mean':\n",
    "            Loss += loss.item()*len(batch)\n",
    "    return Loss/n\n",
    "\n",
    "\n",
    "def train(\n",
    "        model: nn.Module,\n",
    "        train_dataset: Dataset,\n",
    "        val_dataset: Dataset,\n",
    "        optimizer: torch.optim.Optimizer,\n",
    "        loss_fn: nn.Module,\n",
    "        batch_size: int,\n",
    "        epochs: int,\n",
    "        device = Device,\n",
    "        ):\n",
    "    \n",
    "    train_loader = gDataLoader(\n",
    "        dataset = train_dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = True,\n",
    "        )\n",
    "    \n",
    "    val_loader = gDataLoader(\n",
    "        dataset = val_dataset,\n",
    "        batch_size = batch_size,\n",
    "        shuffle = False,\n",
    "        )\n",
    "    \n",
    "    epoch_pbar = tqdm(range(epochs))\n",
    "    for epoch in epoch_pbar:\n",
    "        train_epoch(\n",
    "            model = model, \n",
    "            loader = train_loader, \n",
    "            optimizer = optimizer, \n",
    "            loss_fn = loss_fn, \n",
    "            device = device,\n",
    "            )\n",
    "        train_loss = eval_epoch(\n",
    "            model = model, \n",
    "            loader = train_loader, \n",
    "            loss_fn = loss_fn,\n",
    "            device = device,\n",
    "            )\n",
    "        val_loss = eval_epoch(\n",
    "            model = model, \n",
    "            loader = val_loader, \n",
    "            loss_fn = loss_fn,\n",
    "            device = device,\n",
    "            )\n",
    "        postfix_str = f\"train loss {train_loss:.4f} | val loss {val_loss:.4f}\"\n",
    "        epoch_pbar.set_postfix_str(postfix_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(moleculenet_dataset)\n",
    "\n",
    "train_dataset = moleculenet_dataset[:-round(N/5)]\n",
    "va_dataset = moleculenet_dataset[-round(N/5):]\n",
    "\n",
    "node_features = moleculenet_dataset.num_node_features\n",
    "graph_features = 1\n",
    "\n",
    "model = GNN_graph(\n",
    "    node_features = node_features,\n",
    "    node_hidden_dims = [64, 128],\n",
    "    graph_hidden_dims = [64, 32],\n",
    "    out_features = graph_features,\n",
    "    activation = 'leaky_relu',\n",
    "    pool = 'max',\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "train(\n",
    "    model = model,\n",
    "    train_dataset = train_dataset,\n",
    "    val_dataset = va_dataset,\n",
    "    optimizer = optimizer,\n",
    "    loss_fn = loss_fn,\n",
    "    batch_size = 32,\n",
    "    epochs = 100,\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
