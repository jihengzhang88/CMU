{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1745905060098,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "1gw8Bht2NYYb"
   },
   "outputs": [],
   "source": [
    "# Change to runtime to GPU-T4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "referenced_widgets": [
      "fda3c7a15eef4d60901797750420c7e7",
      "9a077f71782f4fd6adcd212edf706bfc",
      "f99d220ca20547179be152853a02ec68",
      "d8982a44da304f9481062f2f4019c3d4",
      "2d54c779b9ac4240a885daff808a6821",
      "3c87ccdf392c4799b146c5f15631c13f",
      "7ee4e027800e496eb6b684bc71946bf1",
      "96d56c3bfa80490986a2246c362aa78f",
      "91265efea71343bea7a3195666f075a0",
      "74ca64aa476c4727b5a9fd513a8fe01c",
      "5c5c9276b5f34ce7b87e1194e765321b"
     ]
    },
    "executionInfo": {
     "elapsed": 1609,
     "status": "ok",
     "timestamp": 1745905061710,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "HjVKFH1Z_66R",
    "outputId": "90d415a4-eb6c-4c84-eabe-204b1afae217"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda3c7a15eef4d60901797750420c7e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/MyDrive/gemma-transformers-1.1-2b-it-v1'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DONOT CHANGE THIS CODE\n",
    "# Mount your drive\n",
    "\n",
    "import os\n",
    "import gdown\n",
    "import subprocess\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "model_name=\"gemma-transformers-1.1-2b-it-v1\"\n",
    "\n",
    "GOOGLE_DRIVE_PATH = os.path.join(\"/content/drive/MyDrive/\",model_name)\n",
    "REPO_ID = \"MangalamSahai/24789HWs\"\n",
    "if not os.path.exists(os.path.join(\"/content/drive/MyDrive/\",model_name)):\n",
    "  os.mkdir(os.path.join(\"/content/drive/MyDrive/\",model_name))\n",
    "\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "snapshot_download(repo_id=REPO_ID, repo_type=\"dataset\", local_dir = GOOGLE_DRIVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12283,
     "status": "ok",
     "timestamp": 1745905074002,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "ehPrNLNHp1t3",
    "outputId": "d0eda679-b365-483c-bec9-18fc7254882f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Running in Google Colab, installing requirements.\n",
      "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.11/dist-packages (1.25.5)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.30.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n",
      "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.5)\n",
      "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "## DONOT CHANGE THE CODE\n",
    "\n",
    "import os\n",
    "\n",
    "if \"COLAB_GPU\" in os.environ:\n",
    "    print(\"[INFO] Running in Google Colab, installing requirements.\")\n",
    "    # !pip install -U torch # requires torch 2.1.1+ (for efficient sdpa implementation)\n",
    "    !pip install PyMuPDF # for reading PDFs with Python\n",
    "    !pip install tqdm # for progress bars\n",
    "    !pip install sentence-transformers # for embedding models\n",
    "    !pip install accelerate # for quantization model loading\n",
    "    !pip install bitsandbytes # for quantizing models (less storage space)\n",
    "    # !pip install flash-attn --no-build-isolation # for faster attention mechanism = faster LLM inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BxV7PGcNp1t9"
   },
   "source": [
    "##To understand how we obtained embedding file. Please refer to the recitation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 951,
     "status": "ok",
     "timestamp": 1745905074949,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "fyZf54GIp1t9",
    "outputId": "aa157a59-b3b5-488c-8d36-dfa3b52cf629"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"text_chunks_and_embedding_df_load\",\n  \"rows\": 1680,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 349,\n        \"min\": -39,\n        \"max\": 1166,\n        \"num_unique_values\": 1136,\n        \"samples\": [\n          795,\n          918,\n          265\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence_chunk\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1680,\n        \"samples\": [\n          \"The major determinants of Type 2 diabetes that can be changed are overnutrition and a sedentary lifestyle. Therefore, reversing or improving these factors by lifestyle interventions markedly improve the overall health of Type 2 diabetics and lower blood-glucose levels. In fact it has been shown that when people are overweight, losing as little as nine pounds (four kilograms) decreases blood- glucose levels in Type 2 diabetics. The Diabetes Prevention Trial demonstrated that by adhering to a diet containing between 1,200 and 1,800 kilocalories per day with a dietary fat intake goal of less than 25 percent and increasing physical activity to at least 150 minutes per week, people at high risk for Type 2 diabetes achieved a weight loss of 7 percent and significantly decreased their chances of developing Type 2 diabetes.15 The American Diabetes Association (ADA) has a website that provides information and tips for helping diabetics answer the question, \\u201cWhat Can I Eat\\u201d. In regard to carbohydrates the ADA recommends diabetics keep track of the carbohydrates they eat and set a limit. These dietary practices will help keep blood-glucose levels in the target range. Figure 18.5 Metabolic Syndrome: A Combination of Risk Factors Increasing the Chances for Chronic Disease 15.\\u00a0Knowler WC. (2002). Reduction in the Incidence of Type 2 Diabetes with Lifestyle Intervention or Metformin.\",\n          \"Scheme of a micelle formed by phospholipid s in an aqueous solution by Emmanuel Boutet /\\u00a0CC BY-SA 3.0 cholesterol so it acts as an emulsifier. It attracts and holds onto fat while it is simultaneously attracted to and held on to by water. Emulsification increases the surface area of lipids over a thousand- fold, making them more accessible to the digestive enzymes. Once the stomach contents have been emulsified, fat-breaking enzymes work on the triglycerides and diglycerides to sever fatty acids from their glycerol foundations. As pancreatic lipase enters the small intestine, it breaks down the fats into free fatty acids and monoglycerides. Yet again, another hurdle presents itself. How will the fats pass through the watery layer of mucus that coats the absorptive lining of the digestive tract?As before, the answer is bile. Bile salts envelop the fatty acids and monoglycerides to form micelles. Micelles have a fatty acid core with a water-soluble exterior.\",\n          \"Image by\\u00a0Gtirouflet / CC BY-SA 3.0 dense cortical bone is about 10 percent porous and it looks like many concentric circles, similar to the rings in a tree trunk, sandwiched together (Figure 2.27 \\u201cCortical (Compact) Bone\\u201d). Cortical bone tissue makes up approximately 80 percent of the adult skeleton. It surrounds all trabecular tissue and is the only bone tissue in the shafts of long bones. Figure 2.26 The Arrangement of Bone Tissues The two basic tissue types of bones are trabecular and cortical. This photo shows normal (left) and degraded (right) trabecular (spongy) bone. Figure 2.27 Cortical (Compact) Bone. The Skeletal System | 123\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 415,\n        \"min\": 121,\n        \"max\": 1831,\n        \"num_unique_values\": 992,\n        \"samples\": [\n          421,\n          777,\n          1617\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 66,\n        \"min\": 9,\n        \"max\": 297,\n        \"num_unique_values\": 257,\n        \"samples\": [\n          227,\n          276,\n          73\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 103.84074677997317,\n        \"min\": 30.25,\n        \"max\": 457.75,\n        \"num_unique_values\": 992,\n        \"samples\": [\n          105.25,\n          194.25,\n          404.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embedding\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1680,\n        \"samples\": [\n          \"[ 2.29729712e-02  2.80858967e-02  1.12715000e-02  2.38459539e-02\\n  6.08849078e-02 -2.20196079e-02 -3.04390900e-02  7.16837496e-02\\n  3.34621929e-02  2.56895032e-02  3.81384008e-02 -1.46272471e-02\\n  6.05134619e-03  3.96036990e-02 -2.14921068e-02 -2.19398756e-02\\n  3.65133472e-02 -3.25683095e-02 -2.36242507e-02  1.27335191e-02\\n -2.04192102e-02 -7.57569866e-03 -2.97014453e-02  1.34395417e-02\\n -3.89803052e-02  7.09829107e-03  8.38490427e-02  4.68919054e-03\\n -6.21600682e-03 -3.64423431e-02  6.83111176e-02  2.30760835e-02\\n  1.70159172e-02 -3.24641615e-02  2.05456968e-06 -3.15726995e-02\\n -4.76174010e-03  2.94138342e-02  9.00705997e-03  4.50910553e-02\\n  1.82500333e-02 -2.45281626e-02  2.80213878e-02  3.24000865e-02\\n  1.67630445e-02 -2.78206356e-02  2.20614839e-02  3.33284363e-02\\n  2.97171902e-02  2.26193946e-02  9.21218190e-03 -9.00363252e-02\\n  1.28435260e-02 -2.76685599e-02 -1.24296751e-02  2.76750606e-02\\n -7.83731788e-03  5.50725460e-02 -3.28327604e-02 -9.84455738e-03\\n  4.40649912e-02 -5.17247468e-02 -2.61238627e-02  2.34586280e-02\\n -3.23815495e-02  1.43910442e-02 -2.31870990e-02 -6.13956004e-02\\n  3.56797464e-02  4.42797542e-02  7.54781887e-02 -1.18830567e-02\\n  2.61459257e-02  2.20207274e-02  1.07243182e-02  9.09299869e-03\\n  2.32546125e-02 -7.01869577e-02 -4.57891934e-02  2.91911066e-02\\n -1.55159086e-02  1.12296343e-02  1.60889253e-02 -2.10474096e-02\\n  1.33295916e-03 -1.47659099e-02 -1.23751191e-02  2.28247084e-02\\n  1.66185517e-02  7.55396718e-03 -2.91870032e-02  2.15626378e-02\\n -4.81806546e-02  2.03475356e-02  2.55755950e-02 -1.04519501e-02\\n  2.39592195e-02  7.22793639e-02  2.07349043e-02 -5.32777049e-02\\n  4.41575311e-02  7.44044557e-02  8.33298713e-02 -2.73018633e-03\\n  2.66782939e-03  1.34550482e-02 -3.98521274e-02  1.67210482e-03\\n -3.05837709e-02  4.47130464e-02 -2.64749187e-03 -1.56471636e-02\\n -7.53284320e-02 -2.73020063e-02  4.55050021e-02 -1.80287682e-03\\n  5.39066736e-03 -4.53817174e-02 -4.69642878e-02 -3.81236225e-02\\n -1.64280459e-02  8.61568097e-03  3.98035981e-02 -1.25875622e-02\\n -1.80445574e-02  1.02986321e-02  4.12722863e-02 -1.31619982e-02\\n  4.11747545e-02  6.24924619e-03  4.69480641e-02 -1.32724438e-02\\n -3.39613925e-03  8.27880949e-03  6.28442178e-03  5.81035987e-02\\n -1.29940435e-02 -3.89739405e-03  2.20918935e-02  7.65071576e-03\\n -5.82268909e-02  5.13123441e-03 -2.03677192e-02 -6.17644452e-02\\n  5.85707976e-03 -2.43919939e-02  3.34543549e-02  3.82698886e-02\\n  3.51353991e-03 -1.31548736e-02 -1.13629512e-02 -1.27255134e-02\\n -2.48163124e-03  1.72944702e-02  8.27016830e-02 -3.42380162e-03\\n -5.70958778e-02  2.50502699e-03  3.03654727e-02  9.44623444e-03\\n  1.72372162e-02  7.99587555e-03 -2.43335757e-02  2.92636547e-02\\n  6.21003769e-02  7.02438236e-04  4.73045446e-02 -2.81685181e-02\\n  3.49310180e-03  5.04241772e-02  1.11960378e-02  3.41657214e-02\\n -4.17057648e-02  5.92101254e-02  2.73968335e-02 -3.56948450e-02\\n  3.93448658e-02  3.51529308e-02  8.42085201e-03 -1.98321957e-02\\n  4.33469713e-02 -1.18261045e-02  4.92545627e-02 -3.05400956e-02\\n  4.26846975e-03  4.03070636e-03 -6.49488345e-02 -7.95924570e-04\\n -3.46954390e-02 -9.99789964e-03  1.64243709e-02 -1.20536881e-02\\n  7.07974732e-02 -9.05370936e-02  2.73720417e-02  2.17130873e-02\\n -2.33670045e-02  9.12264287e-02  1.64063182e-02  3.62658389e-02\\n  1.47561124e-02 -6.63467124e-03 -5.39022982e-02 -6.76799789e-02\\n  4.55670338e-03 -2.33429801e-02  4.05053683e-02 -1.99937429e-02\\n  3.04897185e-02 -5.00026420e-02  4.43863347e-02 -2.04093903e-02\\n  6.49715960e-02 -4.67289835e-02  3.04762907e-02 -4.17238474e-02\\n  1.36049623e-02 -1.18888151e-02  1.12099862e-02  6.29553497e-02\\n -1.77913252e-02  4.15676348e-02 -1.00580053e-02  6.75371150e-04\\n  2.51111519e-02 -1.26866316e-02  1.68014988e-02 -3.28931436e-02\\n -4.90503013e-02 -5.12543647e-03  5.16201593e-02  2.13184338e-02\\n -6.63907155e-02  2.90889200e-02 -7.74796936e-04 -3.93134579e-02\\n -6.18337747e-03  1.16510903e-02 -3.34364139e-02 -2.28617378e-02\\n -9.15383454e-03 -3.65933105e-02 -3.25162262e-02 -1.53120402e-02\\n -9.30079743e-02 -3.57470699e-02 -7.29239685e-03  4.26715985e-02\\n -4.41388600e-02  6.41513467e-02 -2.29487196e-02 -5.17616123e-02\\n  1.37782358e-02  2.89340112e-02 -7.61773959e-02  1.12729799e-02\\n -7.52168596e-02  5.26861101e-03  1.53507972e-02  4.68976758e-02\\n -1.34887900e-02 -8.26931838e-03  5.36376378e-03  4.95160818e-02\\n -1.30892098e-02 -4.93161492e-02  8.89857020e-03  2.88364477e-02\\n -4.04627621e-02 -5.19260094e-02  6.67705247e-03 -2.46643461e-03\\n  2.24981792e-02  2.95400154e-02 -5.24829235e-03  1.17631555e-02\\n  3.35198268e-02 -4.02946165e-03  1.56868231e-02  2.04602368e-02\\n  8.72190893e-02  6.44629896e-02  4.14886996e-02  1.39544327e-02\\n  3.64198275e-02 -4.75281104e-02 -1.10576088e-02 -6.18624985e-02\\n  1.04863308e-02  1.40444888e-03  5.39735444e-02 -1.78427231e-02\\n  4.31339396e-03  6.45331889e-02 -4.83200699e-02  7.58985654e-02\\n  1.35629280e-02 -4.18066978e-02  1.54282255e-02 -2.14080475e-02\\n  2.15211418e-02  3.94406319e-02 -1.00536551e-02  6.46505458e-03\\n -1.72794964e-02 -1.12749003e-02 -2.73868050e-02 -3.50129902e-02\\n  8.66162498e-03 -6.33235043e-03  4.85603251e-02  3.74994241e-02\\n  2.12799814e-02 -2.77581662e-02 -3.15048848e-03 -5.67035610e-03\\n -4.97750714e-02  1.61907412e-02 -3.45674045e-02  2.42027901e-02\\n -6.08441420e-04 -1.02716545e-02 -4.54923660e-02 -1.02981711e-02\\n  1.40911583e-02  3.75180282e-02  1.68752670e-02  2.34776717e-02\\n -6.18587136e-02  3.49496827e-02  2.63891201e-02  3.26851308e-02\\n  1.95784457e-02  4.40052897e-03 -2.89963484e-02 -6.47588400e-03\\n  3.63212861e-02  7.43830502e-02 -2.42742654e-02  5.18779270e-02\\n -7.51687139e-02 -3.40245292e-02  1.70769282e-02  8.19654763e-02\\n -1.98336337e-02 -1.83938295e-02 -5.01794182e-02  2.75631640e-02\\n -4.24477682e-02  2.42094826e-02 -2.18182877e-02  1.33800218e-02\\n  3.36719789e-02  1.84938870e-03  1.08133173e-02 -6.36373088e-02\\n  4.44507152e-02  1.21700894e-02 -2.84682978e-02 -1.32247293e-02\\n  2.05366500e-02  4.44630794e-02 -6.56913072e-02 -1.55691383e-02\\n  1.03134699e-02 -3.58045660e-02 -1.38292583e-02  4.76093061e-04\\n  1.26236770e-02  1.15177697e-02 -8.35120492e-03  3.91691998e-02\\n  3.29118059e-03  1.52778924e-02 -4.11466584e-02 -2.41303574e-02\\n  2.96915621e-02  1.58117022e-02 -5.56087382e-02 -6.59467326e-03\\n  3.99093628e-02 -9.33313295e-02  1.72184650e-02  3.36729432e-03\\n -1.26988860e-02  2.48798970e-02 -8.33463483e-03 -2.17450745e-02\\n  5.75357601e-02 -3.72378267e-02  2.99197137e-02 -5.25633106e-03\\n  7.22941477e-03 -2.23791022e-02  9.92902089e-03 -4.89397943e-02\\n -3.30955125e-02 -3.47236134e-02 -3.55034024e-02  4.07063104e-02\\n  2.99441069e-02 -1.32662177e-01  3.83205079e-02 -2.97528952e-02\\n  1.57118067e-02  1.49636613e-02  3.05667035e-02  5.80923259e-03\\n  4.45674136e-02  8.68415609e-02  3.00537106e-02 -3.14172730e-02\\n -1.12733599e-02 -6.85861632e-02  8.29143077e-02  9.32687253e-04\\n  8.28287005e-03  4.60251383e-02  5.92524000e-02 -8.62023327e-03\\n -1.28702521e-02 -9.42938589e-03  2.76092310e-02  4.46677580e-02\\n -6.61543086e-02 -4.07488719e-02 -2.03002933e-02  2.83709373e-02\\n -2.50594653e-02  1.89823210e-02  6.60287142e-02  6.14814945e-02\\n -1.37754623e-03 -3.10590887e-03  2.10915934e-02  1.09830508e-02\\n -2.95005180e-02 -1.86231025e-02  8.57474133e-02  5.12545519e-02\\n  2.64492128e-02 -3.36265191e-02  3.48860510e-02 -2.85117216e-02\\n  7.39187151e-02  2.89257746e-02  3.45990323e-02 -1.44111356e-02\\n  6.03982285e-02  5.42171765e-03  1.08373733e-02  1.06092310e-02\\n -6.86236396e-02  2.03288117e-04  1.60011351e-02 -8.57141428e-03\\n -2.39313841e-02 -2.35607326e-02 -3.61981764e-02 -4.08706516e-02\\n  1.07472815e-01  3.81626049e-03  3.19191143e-02 -1.63456071e-02\\n -1.62466131e-02 -6.20278204e-03 -5.22550605e-02  5.99559210e-02\\n -5.44447228e-02 -1.31396269e-02  4.59873267e-02  4.86567691e-02\\n  2.39443686e-02  3.04191876e-02 -4.35936712e-02 -5.25394008e-02\\n -7.88886696e-02  2.50433222e-03 -3.06024663e-02 -3.53882685e-02\\n -4.16593067e-02  2.38333158e-02 -1.45164807e-03 -2.84110345e-02\\n -2.23766919e-02  3.99736091e-02 -1.79760810e-02 -2.58030556e-02\\n -2.83792131e-02  1.75174233e-02 -3.72972079e-02 -2.12674402e-03\\n -6.22427501e-02 -6.65185601e-02 -2.23804079e-02 -7.89400488e-02\\n -1.83512717e-02  5.61043695e-02  1.92721095e-02  5.66429757e-02\\n  6.06254116e-03 -2.40686052e-02  3.77018452e-02 -2.64845230e-02\\n -3.40272952e-03 -3.50038707e-02 -1.39413746e-02  1.73181354e-03\\n  2.22160127e-02  3.72691378e-02 -3.77214104e-02  4.42813803e-03\\n  1.78216752e-02 -2.63886321e-02  1.44201498e-02  5.36084129e-03\\n -1.14303408e-02  1.87312569e-02  1.34985230e-03 -4.87835286e-03\\n -3.91572108e-03 -1.70022659e-02  5.42912353e-03  6.04774803e-02\\n  1.41849415e-02  3.82715948e-02 -3.95067558e-02 -2.00121272e-02\\n -2.09616944e-02  2.74659786e-02  9.52143967e-03 -6.81963516e-03\\n -1.10616554e-02 -8.06704164e-03 -2.63257436e-02 -1.13474466e-02\\n  4.03615721e-02 -2.57180873e-02 -3.69606055e-02 -2.36007608e-02\\n  1.45106716e-02 -3.04713380e-02  2.86728740e-02 -2.93919444e-03\\n -2.37204973e-03 -8.02426413e-03 -3.76460627e-02 -2.02637110e-02\\n  2.97035687e-02 -2.50616148e-02  1.87683746e-03 -5.57896569e-02\\n -2.73329560e-02 -3.78011167e-02  1.61891244e-02 -5.38802897e-33\\n  2.96111181e-02 -9.83624719e-03  4.98379879e-02  5.77777326e-02\\n  5.10296822e-02  1.32933445e-02 -6.45377636e-02 -9.37822554e-03\\n -5.93022890e-02  7.89666101e-02 -2.94902213e-02 -2.31253002e-02\\n  2.72985399e-05 -4.59786952e-02 -6.16562590e-02 -2.65489593e-02\\n  7.99557474e-03  1.57606276e-03 -3.19293663e-02 -1.06759686e-02\\n -6.44516200e-02  3.45161185e-02  8.73177964e-03  1.91062242e-02\\n  2.99177933e-02  4.72093485e-02  1.41662303e-02 -5.85156435e-04\\n  1.50941033e-03  1.84304677e-02 -1.34501401e-02  7.77282864e-02\\n -3.04925285e-04  4.56190482e-02  1.24297431e-02  7.88352489e-02\\n -6.26035081e-03 -3.03326119e-02  2.22761407e-02  3.11824074e-03\\n  6.48971424e-02 -5.33699654e-02  1.14038214e-03  3.22002592e-03\\n -8.58834088e-02  7.39832669e-02 -2.24174149e-02 -1.98693629e-02\\n  2.02376978e-03  2.72148270e-02 -4.08967547e-02 -6.27453765e-03\\n  2.47814786e-02  4.25336882e-02 -2.33338978e-02  4.21236120e-02\\n -5.35760261e-03 -1.38929300e-02  2.34857071e-02  2.73593366e-02\\n -2.59762164e-02  3.09628043e-02 -4.41264287e-02  3.08858417e-02\\n -3.02760443e-03 -7.87444599e-03 -1.53053449e-02  1.24025848e-02\\n -1.44549301e-02 -7.57972375e-02 -7.31532797e-02 -6.25108033e-02\\n  2.19611656e-02  4.04325128e-02 -2.31300900e-03 -6.09265566e-02\\n -4.90598902e-02  1.80721302e-02 -4.54713255e-02 -4.93626110e-02\\n -9.59483627e-03 -1.43828010e-02  1.51193840e-02 -3.79985310e-02\\n -4.98143584e-02  7.65519217e-02 -3.22400853e-02 -3.91259827e-02\\n -7.93610960e-02 -3.08242645e-02  2.04933472e-02  9.77791287e-03\\n -4.97575093e-04  5.28453244e-03  4.71585430e-02  5.64918062e-03\\n  8.42629075e-02 -6.12572692e-02 -3.29317059e-03  7.30867079e-03\\n -1.97428428e-02 -1.43558551e-02  1.55156280e-03  4.72506173e-02\\n -2.23656278e-03  2.66999938e-02 -1.06265480e-02 -2.58234162e-02\\n  7.00479820e-02 -1.86981261e-02 -3.37645672e-02  2.94382703e-02\\n -3.21478657e-02 -3.84805053e-02 -5.57591282e-02  4.19322774e-02\\n  2.99479533e-02  8.49122629e-02  1.71478745e-02 -5.41984625e-02\\n -3.17496471e-02  3.15846279e-02  3.04570980e-02 -3.83366346e-02\\n -6.88592112e-03 -1.95200220e-02  3.14677991e-02 -1.13854895e-03\\n -7.36938119e-02 -1.74842868e-02  5.41744661e-03 -1.92077104e-02\\n  2.78682137e-07  4.08470184e-02 -1.53628632e-03 -1.78423189e-02\\n -1.03535233e-02 -3.08404192e-02 -6.02940517e-03  3.50857563e-02\\n -6.51571378e-02 -5.80149330e-02  4.58920673e-02  4.16008011e-02\\n -3.90204564e-02 -3.64808962e-02 -3.20155025e-02  6.35178387e-02\\n  2.17223652e-02  1.82709042e-02 -1.36412205e-02 -4.86772275e-03\\n -7.65325595e-03  4.14719339e-03  3.67409103e-02 -2.42217444e-02\\n -7.34255984e-02 -3.96182053e-02  4.99258470e-03  4.51597758e-03\\n -4.24852669e-02  6.33915886e-04 -2.84216926e-02  9.66380630e-03\\n  8.96294974e-03  4.65004295e-02 -7.67296329e-02 -2.04632115e-02\\n -7.31814653e-02 -6.51863143e-02  6.90150857e-02 -3.71697284e-02\\n  4.29911762e-02 -1.47519633e-02  1.61680002e-02 -1.68805681e-02\\n -4.27693613e-02 -3.91147472e-03 -5.42811714e-02 -4.86369655e-02\\n  4.49501313e-02  2.64735520e-02 -7.76932463e-02  4.56957985e-03\\n  2.51250882e-02 -5.15799671e-02  9.67912097e-03 -1.23087047e-02\\n  2.34425999e-02  5.72338793e-03  3.40899415e-02  2.72950362e-02\\n -3.42273489e-02 -3.70479841e-03 -5.08046616e-03  3.87148373e-02\\n -8.21772218e-03 -5.54903001e-02 -7.95374159e-04  1.96648911e-02\\n  1.80299746e-34  1.06232800e-02  7.13028107e-03 -8.23647343e-03\\n -4.72389422e-02  1.98795907e-02 -8.45584832e-03 -1.23929149e-02\\n  1.10524462e-03 -2.05322794e-04  2.58644149e-02 -6.45051524e-02]\",\n          \"[-2.89910735e-04 -4.76907492e-02  4.69394773e-02 -1.34329256e-02\\n  2.12686360e-02 -6.53087068e-03  1.84320193e-02 -5.08894678e-03\\n  4.43609543e-02 -2.93549653e-02  1.95518564e-02  5.42302988e-02\\n  1.03220148e-02 -2.42334884e-02  4.03822921e-02 -3.05171981e-02\\n -2.09650956e-02  7.83721544e-03 -1.09209083e-01 -1.38158966e-02\\n -1.22020207e-02  3.17832944e-03 -2.41586682e-03  3.28726098e-02\\n  6.40644804e-02 -3.99281606e-02 -1.83247719e-02 -3.14286165e-02\\n -4.16912213e-02 -3.85700837e-02 -1.26829343e-02 -1.41209865e-03\\n  1.31108062e-02 -9.94675606e-03  2.10254871e-06 -7.18540251e-02\\n -5.03428802e-02  2.97871716e-02 -1.69740599e-02 -2.29916032e-02\\n -1.15232810e-03 -9.93641540e-02 -1.35236466e-02 -7.95061979e-03\\n  3.42773981e-02 -5.95667362e-02 -1.08887609e-02 -1.95660256e-02\\n  7.71187916e-02  8.48048739e-03 -8.23518354e-03 -7.42352158e-02\\n  3.04884110e-02  5.15961759e-02  5.65598123e-02 -2.05483083e-02\\n -5.42607950e-03 -2.55838782e-02 -3.91943641e-02 -3.48252021e-02\\n  1.56397428e-02  5.52787483e-02  2.22797859e-02 -2.18725502e-02\\n  1.79669056e-02  6.62144572e-02  1.44304838e-02 -3.41766737e-02\\n  3.90865318e-02 -2.13876888e-02 -7.50386193e-02 -3.48831303e-02\\n -3.81169505e-02  1.10502150e-02 -4.12468053e-02  1.77212786e-02\\n  6.79142866e-03  1.09248254e-02  2.70897709e-02  1.80689208e-02\\n  6.10432960e-02  1.14617050e-02 -3.80260847e-03 -3.18495855e-02\\n  1.48276682e-03 -1.43926553e-02 -1.98453534e-02 -4.45456058e-02\\n  7.31130037e-03 -5.73883876e-02  5.91200357e-03 -3.44004631e-02\\n  4.99224756e-03  3.59584615e-02  6.44462276e-03  1.80018116e-02\\n  6.66555017e-02 -1.43214915e-04  5.16850278e-02 -2.70861341e-03\\n -8.12202320e-03 -3.59169021e-02 -2.43293568e-02  3.56825888e-02\\n  3.06037888e-02  2.07295101e-02 -2.72049010e-02  3.89792062e-02\\n -2.48995889e-03  2.48275232e-02  1.78018268e-02 -2.56232731e-02\\n  3.13616432e-02  7.06909299e-02 -3.29235606e-02  6.67960197e-03\\n  1.89117144e-03 -1.15451925e-02 -1.72486249e-02  3.75370458e-02\\n  8.91311280e-03  3.62646990e-02  3.51427384e-02  1.90210883e-02\\n -4.58955057e-02  1.19934686e-01  3.36185768e-02 -7.39446096e-03\\n -1.03651602e-02  8.12049881e-02  4.01631668e-02  1.02531221e-02\\n -4.94023971e-02 -1.05537288e-02 -2.22280361e-02  3.68791493e-03\\n -1.05225015e-02 -2.50414722e-02 -3.70037444e-02 -4.52544652e-02\\n  7.80267594e-03  3.66311371e-02  2.23718900e-02  5.00968890e-03\\n -2.80605908e-02 -1.19902072e-02  1.19607532e-02  3.20545770e-02\\n  2.95469817e-03  6.01988584e-02  2.25247932e-03  6.14434443e-02\\n -2.30831411e-02  1.14265140e-02  2.39354055e-02  1.05365543e-02\\n -1.95968766e-02 -6.21460844e-03 -3.88051420e-02  1.25449747e-02\\n  4.87881489e-02 -1.30138760e-02  3.37267518e-02 -2.83177737e-02\\n -3.69780660e-02 -1.87966954e-02  2.91273575e-02  3.03810872e-02\\n -2.06773151e-02  1.28035024e-02  3.55234668e-02  5.05655212e-03\\n  3.86969410e-02 -6.59883097e-02  6.12658076e-03  6.19648397e-02\\n -7.87231326e-03  2.65277247e-03 -5.22707403e-02  8.05527344e-03\\n  8.70954469e-02  5.17622009e-02  2.65399069e-02 -1.25501503e-03\\n  1.99715979e-02  2.72730663e-02  4.49447669e-02  3.09776962e-02\\n -3.58908772e-02  1.70547906e-02  1.96558908e-02 -1.12501793e-02\\n -5.03951497e-03 -5.86961955e-02 -9.83590959e-04  4.27694917e-02\\n -5.07055633e-02 -1.89975593e-02  4.01682733e-03  3.92593034e-02\\n  1.55548693e-03  7.46860541e-03  5.32642640e-02 -4.59342403e-03\\n -2.41951030e-02  5.40453196e-02 -7.32910186e-02 -6.24243878e-02\\n  1.83400027e-02 -8.10545012e-02 -4.10038345e-02 -5.79014830e-02\\n  5.77441463e-03 -1.38999149e-02  3.55744734e-02  6.65484229e-03\\n  3.30575407e-02  1.69161614e-02 -1.81860868e-02  3.19372416e-02\\n -1.55437961e-02  5.05150668e-02  5.82077727e-02 -2.34185979e-02\\n  5.89606073e-03 -2.15327907e-02  4.05314192e-03 -1.02185132e-02\\n -4.35367711e-02 -4.99300100e-03  1.46756461e-02 -1.71298329e-02\\n  1.82500705e-02  1.11141447e-02 -7.75681529e-03 -3.55587192e-02\\n  7.47965276e-03 -8.88304040e-03 -2.59020030e-02  6.83148857e-03\\n -4.77917828e-02 -2.46858783e-02 -3.94188752e-03  1.66137014e-02\\n  7.24267634e-03 -1.85808558e-02 -6.14708364e-02  1.26165347e-02\\n -3.51574942e-02  1.67146046e-02 -3.57484780e-02 -4.81097698e-02\\n -6.50256276e-02 -3.60029191e-02 -9.18580964e-03  4.35761511e-02\\n  6.52618706e-02 -1.07990149e-02  8.77566729e-03 -6.16241572e-03\\n -1.77497417e-02  2.54069455e-02  3.35899442e-02 -1.85800437e-02\\n -1.61337592e-02 -4.13477346e-02 -1.49374781e-02  2.46960893e-02\\n  2.41081249e-02  9.61841142e-04  3.87678668e-02  3.26361246e-02\\n  6.86734822e-03  5.00897691e-02  5.11830188e-02 -4.05327789e-02\\n -8.97689760e-02 -8.13838269e-04 -3.85215655e-02  1.96412746e-02\\n -4.87155095e-03  6.26533031e-02 -5.00203371e-02  1.36259310e-02\\n  1.98138170e-02  2.23706998e-02 -1.47624556e-02 -2.08031237e-02\\n  5.73374741e-02 -1.08853631e-01  7.27196187e-02  3.57444026e-02\\n -1.71732216e-03 -4.48276754e-03 -1.11867022e-02 -1.15359081e-02\\n -6.34977967e-03  6.09585308e-02  1.82274189e-02  8.02924335e-02\\n -6.22956501e-03  2.27490924e-02  2.69049909e-02  1.22414818e-02\\n -3.83753851e-02  5.28007969e-02 -2.88848113e-02  4.91494825e-03\\n  5.87584712e-02  4.09483258e-03  1.61685962e-02  1.02570588e-02\\n -5.05571030e-02  5.99424243e-02  2.43418068e-02 -2.55821459e-02\\n  3.37801017e-02  2.05735844e-02 -2.93751583e-02 -3.52503918e-02\\n  2.85072997e-02  2.23733932e-02 -1.63354613e-02  1.31281624e-02\\n -2.06889194e-02  1.05702206e-02  2.39639115e-02  2.64977408e-03\\n -4.38604094e-02 -2.48857718e-02 -5.12024667e-03 -4.29650396e-02\\n  3.62291187e-02  1.21394852e-02 -2.53850687e-02 -4.19798791e-02\\n -2.53918786e-02  3.80630642e-02 -1.32844523e-02 -6.39878362e-02\\n -6.17671805e-03 -2.35546101e-03  2.35471465e-02  5.53486682e-02\\n  3.34822424e-02 -6.05285168e-03  3.04024760e-02  4.34461422e-03\\n -3.28185409e-02 -8.15888401e-03 -9.93102323e-03 -4.69345376e-02\\n -2.77412869e-02 -4.12742235e-02 -5.05772792e-02 -2.53928006e-02\\n -7.41897238e-05  2.39503905e-02 -1.62669905e-02 -1.58133835e-03\\n  1.10625632e-01  1.11785065e-02  2.37951316e-02 -6.20768294e-02\\n  4.65054922e-02  4.82486039e-02  5.55227557e-03  3.66428234e-02\\n  5.40087791e-03 -1.73931941e-02 -5.17673744e-03  1.78117566e-02\\n -3.30749415e-02  3.81597057e-02  9.95533075e-03 -2.02581962e-03\\n  6.38610404e-03  4.95962463e-02 -8.83316472e-02 -6.28763228e-04\\n  8.45937710e-03  1.88548472e-02  1.00364130e-04 -1.25207026e-02\\n -7.24978046e-03 -2.52529141e-03  3.03694909e-03 -2.31710598e-02\\n  1.31589556e-02  4.16985825e-02 -9.28908773e-03 -5.84791750e-02\\n -6.46011680e-02 -9.03794989e-02  1.03680491e-02 -1.47697348e-02\\n  3.39405611e-02  2.08894573e-02 -2.78323218e-02  6.83821589e-02\\n  3.50590236e-02 -4.68555689e-02  4.72042076e-02 -3.77112441e-03\\n -5.75096253e-03  2.41767466e-02 -4.17235941e-02  2.28880756e-02\\n  4.02758233e-02 -4.91468981e-02  5.05239628e-02 -1.00680158e-01\\n -2.90744472e-02 -1.67413708e-02  1.17921848e-02 -1.26427915e-02\\n  2.48180218e-02  2.69938000e-02 -9.13031306e-03 -5.64103164e-02\\n  4.68287803e-02  3.84604670e-02  3.81848142e-02  2.33297888e-02\\n  2.69803610e-02 -3.96660902e-02 -2.58250758e-02 -6.80448487e-02\\n -7.80719274e-04 -1.27612036e-02  2.29762010e-02 -1.66850742e-02\\n -1.99029464e-02  1.26399044e-02  9.90941525e-02 -4.01808042e-03\\n  4.98764403e-02 -5.11977188e-02  1.49696544e-02  3.19837220e-02\\n  2.93051992e-02 -3.64751816e-02  3.42510156e-02 -6.13177707e-03\\n  6.81453645e-02 -2.24678498e-03 -1.98654179e-02  5.55717237e-02\\n -2.85562035e-02  3.58595587e-02 -1.89085081e-02  3.27889845e-02\\n -3.52927782e-02  4.49295454e-02  3.33935805e-02 -2.24282476e-03\\n  1.83846429e-02  4.99155652e-03  2.87550557e-02 -6.33753240e-02\\n  2.02281289e-02 -2.66907533e-04  3.94245014e-02 -1.10258274e-02\\n -3.18046138e-02 -1.13525372e-02 -2.47585289e-02 -4.52763459e-04\\n -1.15936622e-02 -9.17788537e-04 -4.71309796e-02  3.67310457e-02\\n -2.05483865e-02  7.07767438e-03 -3.64862531e-02 -6.40820637e-02\\n -5.00232242e-02 -2.14895923e-02 -2.11248789e-02  2.18097661e-02\\n -3.70296799e-02 -7.97156896e-03 -7.28307292e-04 -5.57386354e-02\\n  1.37111153e-02  3.24638449e-02 -4.11842763e-02 -8.94591399e-03\\n -2.66409963e-02 -4.38091956e-04  7.27507696e-02  3.20167057e-02\\n  7.19509600e-03 -7.69989118e-02 -8.38134252e-03 -1.57920923e-02\\n  1.48548437e-02  3.00976424e-03  1.15479948e-02  7.29491115e-02\\n -1.33349663e-02 -1.48559851e-03  7.86755513e-03  1.31217875e-02\\n -7.11150914e-02 -1.64781269e-02 -1.93814710e-02 -1.98638737e-02\\n -3.71746086e-02  6.20009191e-02  3.73834372e-02 -1.30262449e-02\\n -9.81295332e-02  4.18129601e-02  1.00712385e-02 -8.76897667e-03\\n -3.06629203e-03 -7.13661686e-02  4.43297476e-02  2.85195205e-02\\n  7.60584278e-03 -1.95184536e-02 -3.53755690e-02 -1.98184811e-02\\n  1.83212617e-03  3.23965028e-02  7.32532330e-03  1.30306305e-02\\n -6.46259710e-02  6.06432855e-02 -2.28145029e-02 -9.53818951e-03\\n -2.75356565e-02 -1.14284353e-02  1.32799949e-02 -2.13443078e-02\\n  6.87101409e-02  5.53869046e-02  2.36709360e-02 -5.58103174e-02\\n -1.48388359e-03  1.56297795e-02 -2.13469844e-02  5.28876260e-02\\n  3.98664139e-02  2.78922580e-02 -2.24801544e-02  2.37328303e-03\\n  5.89274503e-02 -5.76467775e-02 -2.99277268e-02  1.71148032e-02\\n -5.72593212e-02 -4.33231285e-03  4.52495329e-02 -5.64960070e-33\\n  7.47269094e-02  3.62142138e-02  3.95277478e-02  3.07968576e-02\\n  5.90636693e-02  3.75339501e-02 -2.31895726e-02 -4.05858383e-02\\n -1.92534924e-03  9.35129151e-02 -5.24862250e-03 -1.54407015e-02\\n  9.47384071e-03  2.04024091e-02 -5.42762764e-02 -8.57644305e-02\\n  3.89174446e-02 -2.27755848e-02  4.55556773e-02  2.87668332e-02\\n -4.85455543e-02 -6.96453229e-02 -8.42806175e-02 -2.75642015e-02\\n  7.34933466e-03 -2.87090186e-02  6.42396212e-02 -6.70448085e-03\\n -9.58653986e-02  1.88550483e-02 -3.90135646e-02 -3.27156186e-02\\n -4.77096625e-02 -2.81655490e-02 -1.86207108e-02  2.91774161e-02\\n  2.21861750e-02 -1.15162283e-02 -2.01660730e-02  1.75863020e-02\\n -2.69363285e-03 -2.08921097e-02  1.97621379e-02 -4.78858082e-03\\n -1.26683619e-02  1.35948882e-02  1.80629622e-02  3.39837112e-02\\n  2.93650609e-02 -2.20210832e-02 -1.12774428e-02  2.52639689e-02\\n -3.16103883e-02 -3.53645737e-05  7.28631169e-02  9.46151465e-03\\n  5.21957800e-02 -6.56996295e-02  9.55659822e-02  8.91741831e-03\\n -4.13986333e-02 -1.96979428e-03 -2.69607920e-02 -3.09552792e-02\\n -3.77707742e-02 -2.05183239e-03  2.14984917e-04  6.11692369e-02\\n  7.54572675e-02  3.39427218e-02 -5.28387493e-03 -8.06459859e-02\\n  1.36984903e-02 -9.93240699e-02  1.18184602e-02 -1.05019547e-02\\n -1.66479088e-02 -5.94034381e-02 -7.50455484e-02 -9.63898599e-02\\n -4.96468060e-02 -3.80980363e-03  2.93504149e-02 -1.30302366e-02\\n -1.96876731e-02  7.66343391e-03 -6.62882987e-04 -6.74138777e-03\\n -3.01071852e-02 -3.74916419e-02  8.14801380e-02  1.02742910e-02\\n -3.63987908e-02  4.70368713e-02  4.34757471e-02 -6.05085008e-02\\n  5.91272563e-02 -6.01011366e-02 -4.68130084e-03  5.12216762e-02\\n -1.28850369e-02  1.57379806e-02  1.33342706e-02  2.65763644e-02\\n -5.83724044e-02  3.47832330e-02 -1.22272791e-02  1.50706095e-03\\n -1.26263043e-02 -2.35722829e-02 -8.27669539e-03  3.78872566e-02\\n -1.11825429e-02 -2.38841549e-02 -1.60250515e-02  3.18535157e-02\\n  5.17347408e-03 -9.09908582e-03 -6.33173902e-03 -4.04079333e-02\\n -1.26681169e-02  2.52131280e-03 -4.30412777e-02  9.42817144e-03\\n -6.64179102e-02 -2.53620930e-02  3.53503413e-03  8.19246657e-03\\n  5.27810073e-04 -4.34151255e-02 -3.94068286e-03 -1.35703934e-02\\n  2.88146026e-07  4.27781008e-02  5.02584130e-03  7.52717210e-03\\n  1.53221963e-02 -3.81950592e-03 -4.28856388e-02  4.52936767e-03\\n  1.84004679e-02 -6.30369643e-03 -2.81434022e-02 -1.63980434e-03\\n  3.87146894e-04 -2.06686463e-02  3.05093341e-02  5.44758840e-03\\n  5.98083548e-02  5.66420220e-02 -2.37631034e-02  3.25634168e-03\\n  4.02556248e-02  2.62154248e-02  2.93611716e-02  4.04729322e-02\\n -3.56091261e-02 -3.55419237e-03  7.23572969e-02 -5.21490350e-02\\n  3.87458056e-02  6.35161027e-02 -3.82685214e-02 -1.84759367e-02\\n  1.70838181e-02  7.56823570e-02 -5.08912429e-02 -1.48347579e-03\\n -2.67082378e-02  6.18557027e-03 -8.99394450e-04 -1.19499853e-02\\n -7.07405107e-03  3.20798010e-02  1.72325242e-02 -1.53200990e-02\\n  7.72961415e-03 -5.85321561e-02 -3.12942429e-03  5.66900382e-03\\n -2.73432378e-02  3.62395383e-02 -1.12157129e-02 -1.35082491e-02\\n  3.58749926e-02  6.22950634e-03  2.90324874e-02 -9.97451413e-03\\n -1.87331941e-02  3.08613442e-02  2.58529987e-02 -1.21459886e-02\\n  2.73999870e-02  1.34400213e-02  8.27934500e-03 -4.50032167e-02\\n  1.50194066e-02  5.98297454e-02 -6.39654621e-02 -3.34206223e-02\\n  3.10314759e-34 -3.95918760e-04 -6.39823824e-02 -1.33804725e-02\\n -3.26808076e-03  4.86219535e-03 -3.79619971e-02  3.63107920e-02\\n  3.06082554e-02 -7.84143514e-04  5.17915264e-02 -1.50401648e-02]\",\n          \"[ 1.57728139e-02 -5.64577729e-02 -3.63888033e-03  1.73159987e-02\\n -2.66745500e-03  2.10536420e-02 -2.02101655e-02  4.30584364e-02\\n -3.53951268e-02  2.19664518e-02 -4.33511287e-02 -5.20750508e-02\\n -5.89872338e-03  2.55817883e-02  3.15636769e-02  7.17286533e-03\\n -3.72072756e-02 -1.11364517e-02 -1.11601995e-02 -3.12545337e-02\\n -3.86702456e-02 -2.43350528e-02 -4.18420956e-02 -4.27627712e-02\\n -3.75365429e-02 -1.38184447e-02  2.48206146e-02 -1.15433261e-02\\n -1.36964619e-02 -3.28309881e-03  1.13740982e-02 -1.04402518e-02\\n  5.59672639e-02 -3.27690318e-02  2.05511310e-06  8.69802199e-03\\n  5.24926791e-03 -6.02529326e-04  7.95765501e-03  8.60197842e-02\\n  1.62341874e-02  3.49696493e-04 -2.19876915e-02 -1.75113545e-03\\n  2.28333678e-02  2.16686577e-02 -1.60330608e-02  2.73146927e-02\\n -6.92030713e-02 -6.68439129e-03 -2.49029859e-03  2.01713145e-02\\n  1.47406431e-02  1.52928270e-02  6.65228488e-03  2.65217684e-02\\n -5.44198453e-02  1.34172156e-01 -1.52043123e-02  2.51244586e-02\\n -2.14611422e-02  7.70719722e-04  1.96670070e-02 -2.68957503e-02\\n  4.74147126e-02  4.20907736e-02  4.15420420e-02 -4.86595370e-02\\n  2.95606162e-02  1.16407927e-02  8.50693509e-03 -1.01046441e-02\\n  4.82071489e-02  6.38101064e-03  2.07358552e-03 -5.98762743e-02\\n -1.56382807e-02  4.81903777e-02  2.05990300e-02 -7.43976096e-04\\n  1.42410239e-02  3.69130692e-04 -3.08749732e-02  5.07627055e-03\\n -1.63760204e-02 -8.42410028e-02  3.60810347e-02  8.39669071e-03\\n  5.14656641e-02 -3.27205695e-02  7.42358039e-04 -3.66088492e-03\\n  1.47049148e-02  4.50345390e-02 -6.67315498e-02 -3.50497290e-02\\n -2.04252452e-03 -1.14344561e-03  1.14085399e-01 -3.33110057e-02\\n -5.43336533e-02 -6.90692663e-03 -1.05299480e-01  4.72107492e-02\\n -7.59746283e-02 -4.97120246e-02 -4.04516011e-02  2.93611512e-02\\n  1.07014980e-02 -3.83841433e-03  3.03368866e-02  4.20745462e-03\\n  1.17355296e-02  3.46791558e-02 -2.05858164e-02 -6.12229370e-02\\n -2.72969250e-02 -4.43289801e-03  7.09903007e-03 -2.08244193e-02\\n -6.91965455e-03  6.64384440e-02  1.79156717e-02  2.68767122e-02\\n  1.39488243e-02 -5.40009560e-03 -3.38539816e-02  1.90435927e-02\\n  9.69934836e-03 -2.47601252e-02  1.38934590e-02  3.50373574e-02\\n -1.62765086e-02  6.22384483e-03 -2.01239274e-03 -7.44358748e-02\\n  5.14437035e-02  5.49384486e-03  1.21744294e-02  3.73239219e-02\\n -1.50537109e-02 -3.23126055e-02 -1.56131070e-02 -3.34043358e-03\\n -1.84459947e-02  2.07658317e-02 -1.68162771e-02  6.05426207e-02\\n -9.68633406e-03  8.01254530e-03 -2.72264406e-02 -4.52809175e-03\\n -7.75353014e-02 -2.35011261e-02  5.10917678e-02 -3.80758867e-02\\n  1.37859508e-02 -1.74623784e-02  3.25980298e-02  5.16020320e-02\\n  2.20756009e-02 -2.48685982e-02 -5.07828332e-02 -7.71802366e-02\\n  6.98580779e-03 -1.88843627e-02 -4.38863672e-02  1.05715198e-02\\n  3.30445208e-02  3.87548818e-03  4.17918675e-02  7.42689371e-02\\n -1.62473880e-02 -2.62174979e-02 -2.07489152e-02  1.32539898e-01\\n -4.11955602e-02  8.61665234e-04  6.24621920e-02 -7.81116262e-02\\n  7.59768039e-02 -2.26889178e-02  4.39216346e-02 -2.91420631e-02\\n  3.70258503e-02 -6.29993854e-03 -1.81148071e-02  5.53550711e-03\\n  1.04619013e-02 -3.58778760e-02 -7.27115991e-03 -4.91430573e-02\\n -1.79034621e-02  2.63426341e-02  2.13690829e-02 -1.68986395e-02\\n  6.72692084e-04 -7.07057565e-02 -1.83181074e-02  2.31532566e-02\\n -3.04878112e-02 -3.46204429e-03 -5.81701845e-03 -5.40998206e-02\\n -4.43706177e-02 -4.23483402e-02 -4.37021367e-02  2.59232540e-02\\n  3.26459343e-03  2.61602327e-02 -2.89005805e-02  3.05306297e-02\\n  6.89620385e-03 -4.21663038e-02  1.18978396e-02 -1.43660037e-02\\n  1.15575418e-02 -2.52995882e-02 -4.00522910e-02  1.20739350e-02\\n -2.85350680e-02 -1.07138921e-02 -3.22781019e-02  8.96959193e-03\\n -4.03619893e-02  8.41991827e-02  1.07750827e-02 -3.76433507e-02\\n -7.40152821e-02 -4.36559580e-02  2.69117989e-02 -1.00799685e-03\\n  3.17117423e-02  5.25654964e-02 -4.80853803e-02 -5.99735826e-02\\n -8.01319331e-02  4.34510373e-02 -1.74203720e-02  4.51894887e-02\\n  1.31648295e-02 -9.69533473e-02  4.54950668e-02  4.50390577e-03\\n  4.80539836e-02 -2.98802126e-02  2.00214162e-02  2.06043832e-02\\n -1.51534937e-02 -1.61411949e-02 -3.15559916e-02 -4.95321713e-02\\n  4.67078015e-03  2.07061321e-02 -7.21533224e-02  4.37031500e-02\\n -9.53316763e-02  1.38542540e-02 -5.00775017e-02 -2.00575031e-02\\n  4.04674932e-02 -4.60864492e-02  1.04648452e-02 -2.10422873e-02\\n -1.63541567e-02  2.04278179e-03  1.52302468e-02 -7.33365677e-03\\n  6.12386083e-03  1.80253889e-02 -3.73194413e-03  4.54135388e-02\\n -1.63549334e-02  2.90952753e-02 -5.81544563e-02  1.81123242e-02\\n  4.74347919e-02  3.09087802e-02 -4.07738751e-03  3.95797528e-02\\n  6.03918172e-03  7.78303444e-02 -7.49446228e-02 -2.59119626e-02\\n  1.99973714e-02 -3.86637338e-02 -5.09800501e-02 -3.81934792e-02\\n  3.10432483e-02  6.50127158e-02  5.39996009e-03  6.04327917e-02\\n -3.42034153e-03 -2.26868819e-02  3.74620222e-02  3.57151777e-03\\n -1.46530364e-02  2.38615051e-02  1.38629163e-02  7.55867735e-02\\n  9.34519246e-03  2.70611402e-02  2.20353133e-03  3.78867891e-03\\n  1.29411439e-03  3.50290649e-02 -2.58361306e-02 -3.20287235e-02\\n  3.19325998e-02  5.17735779e-02  3.26043740e-02 -2.35525165e-02\\n -8.89074616e-03 -9.25663188e-02 -1.09475553e-02  6.18337803e-02\\n -9.19495523e-03 -1.52931651e-02 -2.32071225e-02  5.54587785e-03\\n  2.19747284e-03  4.17146422e-02  2.41861083e-02  5.54502122e-02\\n  1.29140131e-02  2.37725414e-02 -4.04298417e-02 -8.37784261e-03\\n  1.24115532e-03  3.72962430e-02 -2.39369404e-02  3.17287147e-02\\n -1.81639157e-02 -3.81305888e-02 -2.29516570e-02  1.86472125e-02\\n -2.51918212e-02 -1.11231739e-02  2.61019431e-02 -4.17887326e-03\\n  6.35215407e-03  3.05978372e-03 -2.26567276e-02  5.52211553e-02\\n  2.82943714e-02  5.34320474e-02  1.34907169e-02 -4.37364243e-02\\n  4.72911022e-04 -5.08616529e-02  2.24530231e-03  2.40628179e-02\\n -5.08143939e-02  3.10155284e-02  5.88588463e-03  6.03938997e-02\\n -6.95785042e-03  8.09021369e-02  1.35770431e-02  2.20059697e-02\\n  2.92207021e-03  3.32831107e-02 -2.36401837e-02  3.11954096e-02\\n -4.10429724e-02 -5.60786687e-02 -5.94614656e-04  2.23749820e-02\\n  2.54720799e-03 -1.74369328e-02  6.10900186e-02  5.14829457e-02\\n  2.62116641e-02  1.92390960e-02  7.50768988e-04  3.26913968e-02\\n  4.47159680e-03  4.62862989e-03 -2.28656530e-02 -1.22452769e-02\\n -1.07553555e-02  5.44747226e-02  3.25375088e-02 -8.92883632e-03\\n -2.65619420e-02  5.18130697e-02 -4.14081924e-02  1.72498990e-02\\n  2.43675858e-02 -6.26094043e-02 -1.49971270e-03  3.28977033e-02\\n -1.18482010e-02  1.05182640e-02 -1.13771232e-02  1.52540198e-02\\n -6.17332496e-02  5.94183467e-02 -1.74798481e-02  3.99899669e-02\\n -4.91301995e-03 -3.88532132e-02  4.38259244e-02  7.51819462e-04\\n -1.10452743e-02  3.96509208e-02 -5.72199374e-02  5.06886914e-02\\n -1.45355109e-02 -2.52195727e-02  4.76122275e-02 -5.30431755e-02\\n -4.65138350e-03 -1.32735260e-02  1.48337185e-02  1.04052663e-01\\n  3.78372557e-02 -1.66944489e-02  4.94301505e-02  2.64419690e-02\\n  3.87430713e-02 -2.47008689e-02  3.11886929e-02 -2.38935221e-02\\n -3.32879019e-03 -4.90695462e-02  3.12209334e-02  5.64975198e-03\\n -1.49829434e-02  4.12272103e-02 -1.28541626e-02 -4.14467268e-02\\n  2.29692366e-02  5.30691929e-02 -2.79648392e-03  9.76619944e-02\\n -3.33680725e-03 -3.79325971e-02  3.39732692e-02 -8.15867353e-03\\n  1.95176713e-02 -3.17266546e-02  2.76834685e-02  3.81082972e-03\\n  1.27146882e-03 -3.36397141e-02 -6.71808943e-02 -2.31624162e-03\\n  9.74480063e-03  7.14553520e-02  1.19599774e-02  7.30535947e-03\\n -2.38244166e-03 -4.34231386e-03  8.59754626e-03  7.95183238e-03\\n -6.88121393e-02  6.17698543e-02  2.05795504e-02  1.50338085e-02\\n -2.66217478e-02 -8.35669599e-03 -6.99697286e-02 -4.49123569e-02\\n -7.37115517e-02  3.19464058e-02 -3.07798814e-02 -7.66985044e-02\\n  1.89426709e-02  1.13014160e-02 -2.09322702e-02  4.37395684e-02\\n -3.37798335e-02 -3.42227519e-02 -1.77016798e-02 -2.69568264e-02\\n -3.63975391e-02  1.71616971e-02 -5.46425465e-04  6.61909534e-03\\n -5.62195061e-03  8.01925510e-02 -3.30427475e-02  5.89381903e-02\\n  1.32203279e-02  1.94528140e-02 -4.06433875e-03  6.62010442e-03\\n  2.55999155e-02 -2.27144100e-02  1.24016985e-01  1.62556879e-02\\n -1.74379665e-02 -1.78733207e-02  6.59452146e-03  1.21004969e-01\\n  4.74225320e-02 -9.45353508e-03  4.78293635e-02 -1.17366121e-03\\n -2.45884508e-02  4.98797453e-04 -2.94580702e-02  6.61749989e-02\\n  5.29057719e-03  4.71242182e-02 -5.08823025e-04 -4.14450187e-03\\n -3.93875204e-02 -2.48596538e-02 -1.20226685e-02  2.86598373e-02\\n  4.12970260e-02  4.82219532e-02  2.75197811e-02 -4.92078774e-02\\n -2.67136749e-02  9.12839454e-03 -5.45903407e-02 -7.02854292e-03\\n -1.63870286e-02  3.04527041e-02  2.88387425e-02  8.44170060e-03\\n  2.42968020e-03  1.03751123e-02 -4.05461993e-03  2.85729840e-02\\n -2.12687515e-02  8.71469732e-03  4.17382643e-02 -5.81028350e-02\\n -6.57838881e-02 -3.61579955e-02  8.71246029e-03  8.81481692e-02\\n  8.76042992e-03 -1.10896332e-02 -3.13027054e-02 -8.36808607e-03\\n  2.24093217e-02 -3.53497788e-02  4.64792177e-02  1.57019366e-02\\n -2.84721907e-02 -4.64542629e-03 -9.90463980e-03 -2.12735198e-02\\n  3.03749405e-02 -4.30260785e-02 -1.26876626e-02 -2.74391491e-02\\n  7.86173344e-02 -4.44015302e-02 -4.63078730e-02 -5.84436873e-33\\n  2.52901334e-02 -1.22533622e-03  2.77748099e-03 -1.93980560e-02\\n  6.36001350e-03  7.04465210e-02  5.31472862e-02  8.39251745e-03\\n -5.63005917e-02  2.05048844e-02  2.50587221e-02  1.99946109e-02\\n -9.76994867e-04  1.19377207e-02  3.38732712e-02  3.10741961e-02\\n -1.95467491e-02 -8.23809952e-02  3.13371085e-02 -6.07699994e-03\\n -4.58179228e-02 -2.34846137e-02  4.69333977e-02 -2.66870335e-02\\n  2.99764778e-02  4.57940623e-03  3.68848629e-02 -8.09239410e-03\\n -3.91403548e-02 -2.12850124e-02 -1.58816334e-02  5.18109091e-03\\n  4.49667830e-04  6.88075125e-02  1.90401971e-02  3.69306235e-03\\n -5.63388783e-03 -6.54179081e-02  2.01118906e-04 -4.79890518e-02\\n -8.07988346e-02  3.20624709e-02 -1.01095662e-02 -4.02874388e-02\\n  2.35207118e-02  2.24210862e-02 -5.00765536e-03  6.61188085e-03\\n  6.33936897e-02 -3.64622623e-02 -3.75311524e-02  6.13954151e-03\\n -1.46715362e-02 -1.08262962e-02 -3.08830980e-02  6.81238919e-02\\n -1.99181549e-02 -9.69508290e-03  2.02977508e-02  3.64061221e-02\\n  1.30685084e-02  4.41168854e-03 -9.40269157e-02  2.14329083e-02\\n -1.58167053e-02  2.95217764e-02  1.00607947e-02  8.51427391e-03\\n -5.09682670e-02 -5.59637323e-02 -5.46281338e-02  3.26886475e-02\\n -2.30248272e-03 -1.13981050e-02  4.67576943e-02 -2.70811394e-02\\n -5.65672144e-02 -7.20600132e-03  8.01948085e-02  3.82957384e-02\\n -5.66239432e-02  2.40409803e-02 -3.09104827e-04  6.30280981e-03\\n  2.36698873e-02 -1.47807449e-02  1.33694778e-03 -1.02467937e-02\\n -6.37239143e-02 -1.35340011e-02  7.17986654e-03  5.95078664e-03\\n  1.69184320e-02 -1.00788902e-02  3.31459269e-02 -1.39945187e-02\\n  2.43025329e-02 -1.34768318e-02  5.46660088e-02  3.35097574e-02\\n -4.18631593e-03  1.20559670e-02  9.96872690e-03 -8.02405830e-03\\n -2.07022242e-02 -1.35632381e-02  1.76398959e-02  4.72904788e-03\\n  2.84237582e-02  5.47406683e-03  5.50538115e-02 -3.87830101e-02\\n  2.52093542e-02 -2.19766260e-03 -1.40215801e-02 -2.50385376e-03\\n -8.00563022e-04 -4.19641584e-02 -1.19341845e-02  7.16386829e-03\\n -4.00717109e-02 -3.03082317e-02 -2.99562775e-02 -2.40452476e-02\\n  3.49001437e-02 -1.57152545e-02  1.98987331e-02  2.06106920e-02\\n -3.93997021e-02 -2.40752660e-02  9.95541923e-03  3.20616625e-02\\n  2.85939109e-07  1.87074784e-02 -1.40055991e-03 -3.99141610e-02\\n -5.14978766e-02  6.18599914e-02 -3.14667150e-02  1.45795029e-02\\n -9.48637165e-03 -4.80021276e-02 -1.23640768e-01 -7.49548431e-03\\n  4.88134511e-02  2.27913447e-02  4.93784482e-03 -2.99876612e-02\\n  3.39511037e-02 -1.36664929e-02 -5.97698502e-02  1.43400822e-02\\n  7.44066387e-03 -6.55861422e-02 -4.55697849e-02 -4.00813520e-02\\n -3.28516476e-02  4.69854334e-03  3.70964557e-02 -1.64579116e-02\\n  2.70081754e-03  3.53282616e-02 -3.39808613e-02  1.71928611e-02\\n -1.29510593e-02  3.41144986e-02  2.54638400e-02  9.78602213e-04\\n -5.00721298e-03 -6.41398877e-03 -1.76254585e-02 -2.70556696e-02\\n  5.34198470e-02 -2.46294774e-03 -5.98585606e-03  1.63509268e-02\\n -2.67965533e-02 -3.76960542e-03  7.36843678e-04  8.06827098e-03\\n -9.76282060e-02  7.08713159e-02  8.81878007e-03  3.66175249e-02\\n -5.96829504e-02 -4.19249712e-03 -4.77341749e-02  3.49609321e-03\\n -6.49406388e-02  1.75297447e-02  1.77128389e-02  3.39440480e-02\\n  9.04354751e-02 -1.21040661e-02 -1.56275537e-02 -1.67851686e-05\\n -4.02413867e-02  6.47976026e-02  4.05368023e-03  2.55564079e-02\\n  2.74016963e-34  3.57053764e-02 -1.24818478e-02 -8.30527209e-03\\n -3.16899531e-02 -1.62917208e-02 -8.44950415e-03  1.34990709e-02\\n -2.03760136e-02  9.23624169e-03  1.50190108e-03 -2.35626735e-02]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "text_chunks_and_embedding_df_load"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-38c076d5-637d-4876-8875-57c7fe9f0891\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>sentence_chunk</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-39</td>\n",
       "      <td>Human Nutrition: 2020 Edition UNIVERSITY OF HA...</td>\n",
       "      <td>308</td>\n",
       "      <td>42</td>\n",
       "      <td>77.00</td>\n",
       "      <td>[ 6.74242675e-02  9.02281404e-02 -5.09548886e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-38</td>\n",
       "      <td>Human Nutrition: 2020 Edition by University of...</td>\n",
       "      <td>210</td>\n",
       "      <td>30</td>\n",
       "      <td>52.50</td>\n",
       "      <td>[ 5.52156419e-02  5.92139773e-02 -1.66167244e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-37</td>\n",
       "      <td>Contents Preface University of Hawaii at Mno...</td>\n",
       "      <td>766</td>\n",
       "      <td>114</td>\n",
       "      <td>191.50</td>\n",
       "      <td>[ 2.79801842e-02  3.39813754e-02 -2.06426680e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-36</td>\n",
       "      <td>Lifestyles and Nutrition University of Hawaii...</td>\n",
       "      <td>941</td>\n",
       "      <td>142</td>\n",
       "      <td>235.25</td>\n",
       "      <td>[ 6.82566911e-02  3.81275006e-02 -8.46854132e-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-35</td>\n",
       "      <td>The Cardiovascular System University of Hawai...</td>\n",
       "      <td>998</td>\n",
       "      <td>152</td>\n",
       "      <td>249.50</td>\n",
       "      <td>[ 3.30264494e-02 -8.49763490e-03  9.57159605e-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-38c076d5-637d-4876-8875-57c7fe9f0891')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-38c076d5-637d-4876-8875-57c7fe9f0891 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-38c076d5-637d-4876-8875-57c7fe9f0891');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-cdf4efde-8978-491e-84a6-5d00ea82fc02\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cdf4efde-8978-491e-84a6-5d00ea82fc02')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-cdf4efde-8978-491e-84a6-5d00ea82fc02 button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   page_number                                     sentence_chunk  \\\n",
       "0          -39  Human Nutrition: 2020 Edition UNIVERSITY OF HA...   \n",
       "1          -38  Human Nutrition: 2020 Edition by University of...   \n",
       "2          -37  Contents Preface University of Hawaii at Mno...   \n",
       "3          -36  Lifestyles and Nutrition University of Hawaii...   \n",
       "4          -35  The Cardiovascular System University of Hawai...   \n",
       "\n",
       "   chunk_char_count  chunk_word_count  chunk_token_count  \\\n",
       "0               308                42              77.00   \n",
       "1               210                30              52.50   \n",
       "2               766               114             191.50   \n",
       "3               941               142             235.25   \n",
       "4               998               152             249.50   \n",
       "\n",
       "                                           embedding  \n",
       "0  [ 6.74242675e-02  9.02281404e-02 -5.09548886e-...  \n",
       "1  [ 5.52156419e-02  5.92139773e-02 -1.66167244e-...  \n",
       "2  [ 2.79801842e-02  3.39813754e-02 -2.06426680e-...  \n",
       "3  [ 6.82566911e-02  3.81275006e-02 -8.46854132e-...  \n",
       "4  [ 3.30264494e-02 -8.49763490e-03  9.57159605e-...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## DONOT CHANGE THE CODE\n",
    "import pandas as pd\n",
    "# Import saved file and view\n",
    "embeddings_df_save_path = os.path.join(\"/content/drive/MyDrive/\",model_name,\"text_chunks_and_embeddings_df.csv\")\n",
    "text_chunks_and_embedding_df_load = pd.read_csv(embeddings_df_save_path)\n",
    "text_chunks_and_embedding_df_load.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJV6aK6Bp1t9"
   },
   "source": [
    "## 1. RAG - Search and Answer(70)\n",
    "\n",
    "We discussed RAG briefly in the beginning but let's quickly recap.\n",
    "\n",
    "RAG stands for Retrieval Augmented Generation.\n",
    "\n",
    "Which is another way of saying \"given a query, search for relevant resources and answer based on those resources\".\n",
    "\n",
    "Let's breakdown each step:\n",
    "* **Retrieval** - Get relevant resources given a query. For example, if the query is \"what are the macronutrients?\" the ideal results will contain information about protein, carbohydrates and fats (and possibly alcohol) rather than information about which tractors are the best for farming (though that is also cool information).\n",
    "* **Augmentation** - LLMs are capable of generating text given a prompt. However, this generated text is designed to *look* right. And it often has some correct information, however, they are prone to hallucination (generating a result that *looks* like legit text but is factually wrong). In augmentation, we pass relevant information into the prompt and get an LLM to use that relevant information as the basis of its generation.\n",
    "* **Generation** - This is where the LLM will generate a response that has been flavoured/augmented with the retrieved resources. In turn, this not only gives us a potentially more correct answer, it also gives us resources to investigate more (since we know which resources went into the prompt).\n",
    "\n",
    "The whole idea of RAG is to get an LLM to be more factually correct based on your own input as well as have a reference to where the generated output may have come from.\n",
    "\n",
    "This is an incredibly helpful tool.\n",
    "\n",
    "Let's say you had 1000s of customer support documents.\n",
    "\n",
    "You could use RAG to generate direct answers to questions with links to relevant documentation.\n",
    "\n",
    "Or you were an insurance company with large chains of claims emails.\n",
    "\n",
    "You could use RAG to answer questions about the emails with sources.\n",
    "\n",
    "One helpful analogy is to think of LLMs as calculators for words.\n",
    "\n",
    "With good inputs, the LLM can sort them into helpful outputs.\n",
    "\n",
    "How?\n",
    "\n",
    "It starts with better search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X9qMf9nKp1t9"
   },
   "source": [
    "### Similarity search\n",
    "\n",
    "Similarity search or semantic search or vector search is the idea of searching on *vibe*.\n",
    "\n",
    "If this sounds like woo, woo. It's not.\n",
    "\n",
    "Perhaps searching via *meaning* is a better analogy.\n",
    "\n",
    "With keyword search, you are trying to match the string \"apple\" with the string \"apple\".\n",
    "\n",
    "Whereas with similarity/semantic search, you may want to search \"macronutrients functions\".\n",
    "\n",
    "And get back results that don't necessarily contain the words \"macronutrients functions\" but get back pieces of text that match that meaning.\n",
    "\n",
    "> **Example:** Using similarity search on our textbook data with the query \"macronutrients function\" returns a paragraph that starts with:\n",
    ">\n",
    ">*There are three classes of macronutrients: carbohydrates, lipids, and proteins. These can be metabolically processed into cellular energy. The energy from macronutrients comes from their chemical bonds. This chemical energy is converted into cellular energy that is then utilized to perform work, allowing our bodies to conduct their basic functions.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1238,
     "status": "ok",
     "timestamp": 1745905076188,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "F1eG0nMBp1t9",
    "outputId": "0529b790-3e11-4ea9-b203-5f95d88e07be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1680, 768])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Import texts and embedding df\n",
    "text_chunks_and_embedding_df = pd.read_csv(os.path.join(\"/content/drive/MyDrive/\",model_name,\"text_chunks_and_embeddings_df.csv\"))\n",
    "\n",
    "# Convert embedding column back to np.array (it got converted to string when it got saved to CSV)\n",
    "text_chunks_and_embedding_df[\"embedding\"] = text_chunks_and_embedding_df[\"embedding\"].apply(lambda x: np.fromstring(x.strip(\"[]\"), sep=\" \"))\n",
    "\n",
    "# Convert texts and embedding df to list of dicts\n",
    "pages_and_chunks = text_chunks_and_embedding_df.to_dict(orient=\"records\")\n",
    "\n",
    "# Convert embeddings to torch tensor and send to device (note: NumPy arrays are float64, torch tensors are float32 by default)\n",
    "embeddings = torch.tensor(np.array(text_chunks_and_embedding_df[\"embedding\"].tolist()), dtype=torch.float32).to(device)\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 104,
     "status": "ok",
     "timestamp": 1745905076291,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "4qWPNWFvp1t9",
    "outputId": "a3630fd1-834f-4e87-c6fe-6f75e065dd82"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"text_chunks_and_embedding_df\",\n  \"rows\": 1680,\n  \"fields\": [\n    {\n      \"column\": \"page_number\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 349,\n        \"min\": -39,\n        \"max\": 1166,\n        \"num_unique_values\": 1136,\n        \"samples\": [\n          795,\n          918,\n          265\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentence_chunk\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1680,\n        \"samples\": [\n          \"The major determinants of Type 2 diabetes that can be changed are overnutrition and a sedentary lifestyle. Therefore, reversing or improving these factors by lifestyle interventions markedly improve the overall health of Type 2 diabetics and lower blood-glucose levels. In fact it has been shown that when people are overweight, losing as little as nine pounds (four kilograms) decreases blood- glucose levels in Type 2 diabetics. The Diabetes Prevention Trial demonstrated that by adhering to a diet containing between 1,200 and 1,800 kilocalories per day with a dietary fat intake goal of less than 25 percent and increasing physical activity to at least 150 minutes per week, people at high risk for Type 2 diabetes achieved a weight loss of 7 percent and significantly decreased their chances of developing Type 2 diabetes.15 The American Diabetes Association (ADA) has a website that provides information and tips for helping diabetics answer the question, \\u201cWhat Can I Eat\\u201d. In regard to carbohydrates the ADA recommends diabetics keep track of the carbohydrates they eat and set a limit. These dietary practices will help keep blood-glucose levels in the target range. Figure 18.5 Metabolic Syndrome: A Combination of Risk Factors Increasing the Chances for Chronic Disease 15.\\u00a0Knowler WC. (2002). Reduction in the Incidence of Type 2 Diabetes with Lifestyle Intervention or Metformin.\",\n          \"Scheme of a micelle formed by phospholipid s in an aqueous solution by Emmanuel Boutet /\\u00a0CC BY-SA 3.0 cholesterol so it acts as an emulsifier. It attracts and holds onto fat while it is simultaneously attracted to and held on to by water. Emulsification increases the surface area of lipids over a thousand- fold, making them more accessible to the digestive enzymes. Once the stomach contents have been emulsified, fat-breaking enzymes work on the triglycerides and diglycerides to sever fatty acids from their glycerol foundations. As pancreatic lipase enters the small intestine, it breaks down the fats into free fatty acids and monoglycerides. Yet again, another hurdle presents itself. How will the fats pass through the watery layer of mucus that coats the absorptive lining of the digestive tract?As before, the answer is bile. Bile salts envelop the fatty acids and monoglycerides to form micelles. Micelles have a fatty acid core with a water-soluble exterior.\",\n          \"Image by\\u00a0Gtirouflet / CC BY-SA 3.0 dense cortical bone is about 10 percent porous and it looks like many concentric circles, similar to the rings in a tree trunk, sandwiched together (Figure 2.27 \\u201cCortical (Compact) Bone\\u201d). Cortical bone tissue makes up approximately 80 percent of the adult skeleton. It surrounds all trabecular tissue and is the only bone tissue in the shafts of long bones. Figure 2.26 The Arrangement of Bone Tissues The two basic tissue types of bones are trabecular and cortical. This photo shows normal (left) and degraded (right) trabecular (spongy) bone. Figure 2.27 Cortical (Compact) Bone. The Skeletal System | 123\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_char_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 415,\n        \"min\": 121,\n        \"max\": 1831,\n        \"num_unique_values\": 992,\n        \"samples\": [\n          421,\n          777,\n          1617\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_word_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 66,\n        \"min\": 9,\n        \"max\": 297,\n        \"num_unique_values\": 257,\n        \"samples\": [\n          227,\n          276,\n          73\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_token_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 103.84074677997317,\n        \"min\": 30.25,\n        \"max\": 457.75,\n        \"num_unique_values\": 992,\n        \"samples\": [\n          105.25,\n          194.25,\n          404.25\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"embedding\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe",
       "variable_name": "text_chunks_and_embedding_df"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-003cdf83-afe3-4988-953c-97ad74a45cfb\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_number</th>\n",
       "      <th>sentence_chunk</th>\n",
       "      <th>chunk_char_count</th>\n",
       "      <th>chunk_word_count</th>\n",
       "      <th>chunk_token_count</th>\n",
       "      <th>embedding</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-39</td>\n",
       "      <td>Human Nutrition: 2020 Edition UNIVERSITY OF HA...</td>\n",
       "      <td>308</td>\n",
       "      <td>42</td>\n",
       "      <td>77.00</td>\n",
       "      <td>[0.0674242675, 0.0902281404, -0.00509548886, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-38</td>\n",
       "      <td>Human Nutrition: 2020 Edition by University of...</td>\n",
       "      <td>210</td>\n",
       "      <td>30</td>\n",
       "      <td>52.50</td>\n",
       "      <td>[0.0552156419, 0.0592139773, -0.0166167244, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-37</td>\n",
       "      <td>Contents Preface University of Hawaii at Mno...</td>\n",
       "      <td>766</td>\n",
       "      <td>114</td>\n",
       "      <td>191.50</td>\n",
       "      <td>[0.0279801842, 0.0339813754, -0.020642668, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-36</td>\n",
       "      <td>Lifestyles and Nutrition University of Hawaii...</td>\n",
       "      <td>941</td>\n",
       "      <td>142</td>\n",
       "      <td>235.25</td>\n",
       "      <td>[0.0682566911, 0.0381275006, -0.00846854132, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-35</td>\n",
       "      <td>The Cardiovascular System University of Hawai...</td>\n",
       "      <td>998</td>\n",
       "      <td>152</td>\n",
       "      <td>249.50</td>\n",
       "      <td>[0.0330264494, -0.0084976349, 0.00957159605, -...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-003cdf83-afe3-4988-953c-97ad74a45cfb')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-003cdf83-afe3-4988-953c-97ad74a45cfb button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-003cdf83-afe3-4988-953c-97ad74a45cfb');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    <div id=\"df-3c50dc82-978d-4d66-82c0-2c944f0fcc0b\">\n",
       "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3c50dc82-978d-4d66-82c0-2c944f0fcc0b')\"\n",
       "                title=\"Suggest charts\"\n",
       "                style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "      </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "      <script>\n",
       "        async function quickchart(key) {\n",
       "          const quickchartButtonEl =\n",
       "            document.querySelector('#' + key + ' button');\n",
       "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "          try {\n",
       "            const charts = await google.colab.kernel.invokeFunction(\n",
       "                'suggestCharts', [key], {});\n",
       "          } catch (error) {\n",
       "            console.error('Error during call to suggestCharts:', error);\n",
       "          }\n",
       "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "        }\n",
       "        (() => {\n",
       "          let quickchartButtonEl =\n",
       "            document.querySelector('#df-3c50dc82-978d-4d66-82c0-2c944f0fcc0b button');\n",
       "          quickchartButtonEl.style.display =\n",
       "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "        })();\n",
       "      </script>\n",
       "    </div>\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "   page_number                                     sentence_chunk  \\\n",
       "0          -39  Human Nutrition: 2020 Edition UNIVERSITY OF HA...   \n",
       "1          -38  Human Nutrition: 2020 Edition by University of...   \n",
       "2          -37  Contents Preface University of Hawaii at Mno...   \n",
       "3          -36  Lifestyles and Nutrition University of Hawaii...   \n",
       "4          -35  The Cardiovascular System University of Hawai...   \n",
       "\n",
       "   chunk_char_count  chunk_word_count  chunk_token_count  \\\n",
       "0               308                42              77.00   \n",
       "1               210                30              52.50   \n",
       "2               766               114             191.50   \n",
       "3               941               142             235.25   \n",
       "4               998               152             249.50   \n",
       "\n",
       "                                           embedding  \n",
       "0  [0.0674242675, 0.0902281404, -0.00509548886, -...  \n",
       "1  [0.0552156419, 0.0592139773, -0.0166167244, -0...  \n",
       "2  [0.0279801842, 0.0339813754, -0.020642668, 0.0...  \n",
       "3  [0.0682566911, 0.0381275006, -0.00846854132, -...  \n",
       "4  [0.0330264494, -0.0084976349, 0.00957159605, -...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_chunks_and_embedding_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyH8Hd7up1t9"
   },
   "source": [
    "Nice!\n",
    "\n",
    "Now let's prepare another instance of our embedding model. Not because we have to but because we'd like to make it so you can start the notebook from the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 7330,
     "status": "ok",
     "timestamp": 1745905083646,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "KUv4BVvhp1t9"
   },
   "outputs": [],
   "source": [
    "device=\"cuda\"\n",
    "from sentence_transformers import util, SentenceTransformer\n",
    "\n",
    "embedding_model = SentenceTransformer(model_name_or_path=\"all-mpnet-base-v2\",\n",
    "                                      device=device) # choose the device to load the model to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ozJqrrX7p1t9"
   },
   "source": [
    "Embedding model ready!\n",
    "\n",
    "Time to perform a semantic search.\n",
    "\n",
    "Let's say you were studying the macronutrients.\n",
    "\n",
    "And wanted to search your textbook for \"macronutrients functions\".\n",
    "\n",
    "Well, we can do so with the following steps:\n",
    "1. Define a query string (e.g. `\"macronutrients functions\"`) - note: this could be anything, specific or not.\n",
    "2. Turn the query string in an embedding with same model we used to embed our text chunks.\n",
    "3. Perform a [dot product](https://pytorch.org/docs/stable/generated/torch.dot.html) or [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) function between the text embeddings and the query embedding (we'll get to what these are shortly) to get similarity scores.\n",
    "4. Sort the results from step 3 in descending order (a higher score means more similarity in the eyes of the model) and use these values to inspect the texts.\n",
    "\n",
    "Easy!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 287,
     "status": "ok",
     "timestamp": 1745905083933,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "qgFJ8TuCp1t9",
    "outputId": "d092c6c0-8f4f-4af4-bdf8-01d2d558bba5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: macronutrients functions\n",
      "Time take to get scores on 1680 embeddings: 0.00040 seconds.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([0.6926, 0.6738, 0.6646, 0.6536, 0.6473], device='cuda:0'),\n",
       "indices=tensor([42, 47, 41, 51, 46], device='cuda:0'))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Define the query\n",
    "# Note: This could be anything. But since we're working with a nutrition textbook, we'll stick with nutrition-based queries.\n",
    "query = \"macronutrients functions\"\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# 2. Embed the query to the same numerical space as the text examples\n",
    "# Note: It's important to embed your query with the same model you embedded your examples with.\n",
    "query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "# 3. Get similarity scores with the dot product (we'll time this for fun)\n",
    "from time import perf_counter as timer\n",
    "\n",
    "start_time = timer()\n",
    "dot_scores = util.dot_score(a=query_embedding, b=embeddings)[0]\n",
    "end_time = timer()\n",
    "\n",
    "print(f\"Time take to get scores on {len(embeddings)} embeddings: {end_time-start_time:.5f} seconds.\")\n",
    "\n",
    "# 4. Get the top-k results (we'll keep this to 5)\n",
    "top_results_dot_product = torch.topk(dot_scores, k=5)\n",
    "top_results_dot_product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gw9u6NTsp1t9"
   },
   "source": [
    "We can get pretty far by just storing our embeddings in `torch.tensor` for now.\n",
    "\n",
    "However, for *much* larger datasets, we'd likely look at a dedicated vector database/indexing libraries such as [Faiss](https://github.com/facebookresearch/faiss).\n",
    "\n",
    "Let's check the results of our original similarity search.\n",
    "\n",
    "[`torch.topk`](https://pytorch.org/docs/stable/generated/torch.topk.html) returns a tuple of values (scores) and indicies for those scores.\n",
    "\n",
    "The indicies relate to which indicies in the `embeddings` tensor have what scores in relation to the query embedding (higher is better).\n",
    "\n",
    "We can use those indicies to map back to our text chunks.\n",
    "\n",
    "First, we'll define a small helper function to print out wrapped text (so it doesn't print a whole text chunk as a single line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1745905083946,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "lyfjL-tJp1t9"
   },
   "outputs": [],
   "source": [
    "# Define helper function to print wrapped text\n",
    "import textwrap\n",
    "\n",
    "def print_wrapped(text, wrap_length=80):\n",
    "    wrapped_text = textwrap.fill(text, wrap_length)\n",
    "    print(wrapped_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSFfTp-7p1t-"
   },
   "source": [
    "Now we can loop through the `top_results_dot_product` tuple and match up the scores and indicies and then use those indicies to index on our `pages_and_chunks` variable to get the relevant text chunk.\n",
    "\n",
    "Sounds like a lot but we can do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1745905083968,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "jQSqCePyp1t-",
    "outputId": "6997f308-8ae3-4aca-824b-dd6c2333c2af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: 'macronutrients functions'\n",
      "\n",
      "Results:\n",
      "Score: 0.6926\n",
      "Text:\n",
      "Macronutrients Nutrients that are needed in large amounts are called\n",
      "macronutrients. There are three classes of macronutrients: carbohydrates,\n",
      "lipids, and proteins. These can be metabolically processed into cellular energy.\n",
      "The energy from macronutrients comes from their chemical bonds. This chemical\n",
      "energy is converted into cellular energy that is then utilized to perform work,\n",
      "allowing our bodies to conduct their basic functions. A unit of measurement of\n",
      "food energy is the calorie. On nutrition food labels the amount given for\n",
      "calories is actually equivalent to each calorie multiplied by one thousand. A\n",
      "kilocalorie (one thousand calories, denoted with a small c) is synonymous with\n",
      "the Calorie (with a capital C) on nutrition food labels. Water is also a\n",
      "macronutrient in the sense that you require a large amount of it, but unlike the\n",
      "other macronutrients, it does not yield calories. Carbohydrates Carbohydrates\n",
      "are molecules composed of carbon, hydrogen, and oxygen.\n",
      "Page number: 5\n",
      "\n",
      "\n",
      "Score: 0.6738\n",
      "Text:\n",
      "Water There is one other nutrient that we must have in large quantities: water.\n",
      "Water does not contain carbon, but is composed of two hydrogens and one oxygen\n",
      "per molecule of water. More than 60 percent of your total body weight is water.\n",
      "Without it, nothing could be transported in or out of the body, chemical\n",
      "reactions would not occur, organs would not be cushioned, and body temperature\n",
      "would fluctuate widely. On average, an adult consumes just over two liters of\n",
      "water per day from food and drink combined. Since water is so critical for\n",
      "lifes basic processes, the amount of water input and output is supremely\n",
      "important, a topic we will explore in detail in Chapter 4. Micronutrients\n",
      "Micronutrients are nutrients required by the body in lesser amounts, but are\n",
      "still essential for carrying out bodily functions. Micronutrients include all\n",
      "the essential minerals and vitamins. There are sixteen essential minerals and\n",
      "thirteen vitamins (See Table 1.1 Minerals and Their Major Functions and Table\n",
      "1.2 Vitamins and Their Major Functions for a complete list and their major\n",
      "functions). In contrast to carbohydrates, lipids, and proteins, micronutrients\n",
      "are not sources of energy (calories), but they assist in the process as\n",
      "cofactors or components of enzymes (i.e., coenzymes).\n",
      "Page number: 8\n",
      "\n",
      "\n",
      "Score: 0.6646\n",
      "Text:\n",
      "Learning Objectives By the end of this chapter, you will be able to:  Describe\n",
      "basic concepts in nutrition  Describe factors that affect your nutritional\n",
      "needs  Describe the importance of research and scientific methods to\n",
      "understanding nutrition What are Nutrients? The foods we eat contain nutrients.\n",
      "Nutrients are substances required by the body to perform its basic functions.\n",
      "Nutrients must be obtained from our diet, since the human body does not\n",
      "synthesize or produce them. Nutrients have one or more of three basic functions:\n",
      "they provide energy, contribute to body structure, and/or regulate chemical\n",
      "processes in the body. These basic functions allow us to detect and respond to\n",
      "environmental surroundings, move, excrete wastes, respire (breathe), grow, and\n",
      "reproduce. There are six classes of nutrients required for the body to function\n",
      "and maintain overall health. These are carbohydrates, lipids, proteins, water,\n",
      "vitamins, and minerals. Foods also contain non-nutrients that may be harmful\n",
      "(such as natural toxins common in plant foods and additives like some dyes and\n",
      "preservatives) or beneficial (such as antioxidants). 4 | Introduction\n",
      "Page number: 4\n",
      "\n",
      "\n",
      "Score: 0.6536\n",
      "Text:\n",
      "Vitamins Major Functions Water-soluble Thiamin (B1) Coenzyme, energy metabolism\n",
      "assistance Riboflavin (B2 ) Coenzyme, energy metabolism assistance Niacin (B3)\n",
      "Coenzyme, energy metabolism assistance Pantothenic acid (B5) Coenzyme, energy\n",
      "metabolism assistance Pyridoxine (B6) Coenzyme, amino acid synthesis assistance\n",
      "Biotin (B7) Coenzyme, amino acid and fatty acid metabolism Folate (B9) Coenzyme,\n",
      "essential for growth Cobalamin (B12) Coenzyme, red blood cell synthesis C\n",
      "(ascorbic acid) Collagen synthesis, antioxidant Fat-soluble A Vision,\n",
      "reproduction, immune system function D Bone and teeth health maintenance, immune\n",
      "system function E Antioxidant, cell membrane protection K Bone and teeth health\n",
      "maintenance, blood clotting Vitamin deficiencies can cause severe health\n",
      "problems and even death. For example, a deficiency in niacin causes a disease\n",
      "called pellagra, which was common in the early twentieth century in some parts\n",
      "of America. The common signs and symptoms of pellagra are known as the\n",
      "4Dsdiarrhea, dermatitis, dementia, and death. Until scientists found out\n",
      "that better diets relieved the signs and symptoms of pellagra, many people with\n",
      "the disease ended up hospitalized in insane asylums awaiting death. Other\n",
      "vitamins were also found to prevent certain disorders and diseases such as\n",
      "scurvy (vitamin C), night blindness vitamin A, and rickets (vitamin D). Table\n",
      "1.3 Functions of Nutrients Introduction | 11\n",
      "Page number: 11\n",
      "\n",
      "\n",
      "Score: 0.6473\n",
      "Text:\n",
      "Figure 1.1 The Macronutrie nts: Carbohydrat es, Lipids, Protein, and Water\n",
      "Proteins Proteins are macromolecules composed of chains of subunits called amino\n",
      "acids. Amino acids are simple subunits composed of carbon, oxygen, hydrogen, and\n",
      "nitrogen. Food sources of proteins include meats, dairy products, seafood, and a\n",
      "variety of different plant- based foods, most notably soy. The word protein\n",
      "comes from a Greek word meaning of primary importance, which is an apt\n",
      "description of these macronutrients; they are also known colloquially as the\n",
      "workhorses of life. Proteins provide four kilocalories of energy per gram;\n",
      "however providing energy is not proteins most important function. Proteins\n",
      "provide structure to bones, muscles and skin, and play a role in conducting most\n",
      "of the chemical reactions that take place in the body. Scientists estimate that\n",
      "greater than one-hundred thousand different proteins exist within the human\n",
      "body. The genetic codes in DNA are basically protein recipes that determine the\n",
      "order in which 20 different amino acids are bound together to make thousands of\n",
      "specific proteins. Figure 1.1 The Macronutrients: Carbohydrates, Lipids,\n",
      "Protein, and Water Introduction | 7\n",
      "Page number: 7\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Query: '{query}'\\n\")\n",
    "print(\"Results:\")\n",
    "# Loop through zipped together scores and indicies from torch.topk\n",
    "for score, idx in zip(top_results_dot_product[0], top_results_dot_product[1]):\n",
    "    print(f\"Score: {score:.4f}\")\n",
    "    # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "    print(\"Text:\")\n",
    "    print_wrapped(pages_and_chunks[idx][\"sentence_chunk\"])\n",
    "    # Print the page number too so we can reference the textbook further (and check the results)\n",
    "    print(f\"Page number: {pages_and_chunks[idx]['page_number']}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHkRMk9qp1t-"
   },
   "source": [
    "### Similarity measures: dot product and cosine similarity\n",
    "\n",
    "Let's talk similarity measures between vectors.\n",
    "\n",
    "Specifically, embedding vectors which are representations of data with magnitude and direction in high dimensional space (our embedding vectors have 768 dimensions).\n",
    "\n",
    "Two of the most common you'll across are the dot product and cosine similarity.\n",
    "\n",
    "They are quite similar.\n",
    "\n",
    "The main difference is that cosine similarity has a normalization step.\n",
    "\n",
    "| Similarity measure | Description | Code |\n",
    "| ----- | ----- | ----- |\n",
    "| [Dot Product](https://en.wikipedia.org/wiki/Dot_product) | - Measure of magnitude and direction between two vectors<br>- Vectors that are aligned in direction and magnitude have a higher positive value<br>- Vectors that are opposite in direction and magnitude have a higher negative value | [`torch.dot`](https://pytorch.org/docs/stable/generated/torch.dot.html), [`np.dot`](https://numpy.org/doc/stable/reference/generated/numpy.dot.html), [`sentence_transformers.util.dot_score`](https://www.sbert.net/docs/package_reference/util.html#sentence_transformers.util.dot_score) |\n",
    "| [Cosine Similarity](https://en.wikipedia.org/wiki/Cosine_similarity) | - Vectors get normalized by magnitude/[Euclidean norm](https://en.wikipedia.org/wiki/Norm_(mathematics))/L2 norm so they have unit length and are compared more so on direction<br>- Vectors that are aligned in direction have a value close to 1<br>- Vectors that are opposite in direction have a value close to -1 | [`torch.nn.functional.cosine_similarity`](https://pytorch.org/docs/stable/generated/torch.nn.functional.cosine_similarity.html), [`1 - scipy.spatial.distance.cosine`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html) (subtract the distance from 1 for similarity measure), [`sentence_transformers.util.cos_sim`](https://www.sbert.net/docs/package_reference/util.html#sentence_transformers.util.cos_sim) |\n",
    "\n",
    "For text similarity, you generally want to use cosine similarity as you are after the semantic measurements (direction) rather than magnitude.\n",
    "\n",
    "In our case, our embedding model `all-mpnet-base-v2` outputs normalized outputs (see the [Hugging Face model card](https://huggingface.co/sentence-transformers/all-mpnet-base-v2#usage-huggingface-transformers) for more on this) so dot product and cosine similarity return the same results. However, dot product is faster due to not need to perform a normalize step.\n",
    "\n",
    "To make things bit more concrete, let's make simple dot product and cosine similarity functions and view their results on different vectors.\n",
    "\n",
    "> **Note:** Similarity measures between vectors and embeddings can be used on any kind of embeddings, not just text embeddings. For example, you could measure image embedding similarity or audio embedding similarity. Or with text and image models like [CLIP](https://github.com/mlfoundations/open_clip), you can measure the similarity between text and image embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 30,
     "status": "ok",
     "timestamp": 1745905084013,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "sPFnkrQ9p1t-"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def dot_product(vector1, vector2):\n",
    "\n",
    "    return torch.matmul(vector2, vector1)\n",
    "\n",
    "def cosine_similarity(vector1, vector2):\n",
    "\n",
    "    vector1 = vector1.float()\n",
    "    vector2 = vector2.float()\n",
    "\n",
    "    # Compute Frobenius norms of the original matrices\n",
    "    norm_v1 = torch.norm(vector1, p='fro')  # Frobenius norm of Vector1\n",
    "    norm_v2 = torch.norm(vector2, p='fro')  # Frobenius norm of Vector2\n",
    "\n",
    "    # Normalize the dot product by the product of the Frobenius norms\n",
    "    cos_sim_matrix = dot_product(vector1, vector2) / (norm_v1 * norm_v2)\n",
    "\n",
    "    return cos_sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 90,
     "status": "ok",
     "timestamp": 1745905084105,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "1w_ecaKzolK_",
    "outputId": "6def270b-5125-4161-997c-6e76cac881fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dot product between vector1 and vector2: tensor([[ 68, 170, 113],\n",
      "        [ 46, 113,  75],\n",
      "        [ 54, 149,  92]])\n",
      "Dot product between vector1 and vector3: tensor([[ 98, 291, 182],\n",
      "        [ 84, 234, 161],\n",
      "        [ 90, 255, 179]])\n",
      "Dot product between vector1 and vector4: tensor([[136, 376, 254],\n",
      "        [114, 379, 237],\n",
      "        [130, 347, 229]])\n",
      "Cosine similarity between vector1 and vector2: tensor([[0.1898, 0.4746, 0.3155],\n",
      "        [0.1284, 0.3155, 0.2094],\n",
      "        [0.1508, 0.4160, 0.2568]])\n",
      "Cosine similarity between vector1 and vector3: tensor([[0.1584, 0.4704, 0.2942],\n",
      "        [0.1358, 0.3782, 0.2602],\n",
      "        [0.1455, 0.4122, 0.2893]])\n",
      "Cosine similarity between vector1 and vector4: tensor([[0.1546, 0.4275, 0.2888],\n",
      "        [0.1296, 0.4309, 0.2694],\n",
      "        [0.1478, 0.3945, 0.2603]])\n"
     ]
    }
   ],
   "source": [
    "# Tests for above functions\n",
    "\n",
    "# Example tensors\n",
    "torch.manual_seed(42)\n",
    "vector1 = torch.randint(low=0, high=10, size=(3,3))\n",
    "vector2 = torch.randint(low=5, high=15, size=(3,3))\n",
    "vector3 = torch.randint(low=10, high=20, size=(3,3))\n",
    "vector4 = torch.randint(low=15, high=30, size=(3,3))\n",
    "\n",
    "# print(vector1)\n",
    "# print(vector2)\n",
    "# print(torch.norm(vector1, p='fro'))\n",
    "\n",
    "# Calculate dot product\n",
    "print(\"Dot product between vector1 and vector2:\", dot_product(vector1, vector2))\n",
    "assert torch.equal(dot_product(vector1, vector2),torch.tensor([[ 68, 170, 113],\n",
    "                                                      [ 46, 113,  75],\n",
    "                                                      [ 54, 149,  92]]))\n",
    "print(\"Dot product between vector1 and vector3:\", dot_product(vector1, vector3))\n",
    "assert torch.equal(dot_product(vector1, vector3),torch.tensor([[ 98, 291, 182],\n",
    "                                                         [ 84, 234, 161],\n",
    "                                                         [ 90, 255, 179]]))\n",
    "\n",
    "print(\"Dot product between vector1 and vector4:\", dot_product(vector1, vector4))\n",
    "assert torch.equal(dot_product(vector1, vector4),torch.tensor([[136, 376, 254],\n",
    "                                                               [114, 379, 237],\n",
    "                                                               [130, 347, 229]]))\n",
    "\n",
    "def test_close(a, b, eps=1e-4):\n",
    "    # Now depending on what a, b are you can add code here\n",
    "    # return abs(a-b)<eps # If scaler\n",
    "    return (abs(a-b)<eps).all() # If array\n",
    "\n",
    "# Calculate cosine similarity\n",
    "print(\"Cosine similarity between vector1 and vector2:\", cosine_similarity(vector1, vector2))\n",
    "assert test_close(cosine_similarity(vector1, vector2),torch.tensor([[0.1898, 0.4746, 0.3155],\n",
    "                                                                    [0.1284, 0.3155, 0.2094],\n",
    "                                                                    [0.1508, 0.4160, 0.2568]]))\n",
    "print(\"Cosine similarity between vector1 and vector3:\", cosine_similarity(vector1, vector3))\n",
    "assert test_close(cosine_similarity(vector1, vector3),torch.tensor([[0.1584, 0.4704, 0.2942],\n",
    "                                                                    [0.1358, 0.3782, 0.2602],\n",
    "                                                                    [0.1455, 0.4122, 0.2893]]))\n",
    "print(\"Cosine similarity between vector1 and vector4:\", cosine_similarity(vector1, vector4))\n",
    "assert test_close(cosine_similarity(vector1, vector4),torch.tensor([[0.1546, 0.4275, 0.2888],\n",
    "                                                                     [0.1296, 0.4309, 0.2694],\n",
    "                                                                     [0.1478, 0.3945, 0.2603]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFC78m7_p1t-"
   },
   "source": [
    "Notice for both dot product and cosine similarity the comparisons of `vector1` and `vector2` are the opposite of `vector1` and `vector4`.\n",
    "\n",
    "Comparing `vector1` and `vector2` both equations return positive values (14 for dot product and 1.0 for cosine similarity).\n",
    "\n",
    "But comparing `vector1` and `vector4` the result is in the negative direction.\n",
    "\n",
    "This makes sense because `vector4` is the negative version of `vector1`.\n",
    "\n",
    "Whereas comparing `vector1` and `vector3` shows a different outcome.\n",
    "\n",
    "For the dot product, the value is positive and larger then the comparison of two exactly the same vectors (32 vs 14).\n",
    "\n",
    "However, for the cosine similarity, thanks to the normalization step, comparing `vector1` and `vector3` results in a postive value close to 1 but not exactly 1.\n",
    "\n",
    "It is because of this that when comparing text embeddings, cosine similarity is generally favoured as it measures the difference in direction of a pair of vectors rather than difference in magnitude.\n",
    "\n",
    "And it is this difference in direction that is more generally considered to capture the semantic meaning/vibe of the text.\n",
    "\n",
    "The good news is that as mentioned before, the outputs of our embedding model `all-mpnet-base-v2` are already normalized.\n",
    "\n",
    "So we can continue using the dot product (cosine similarity is dot product + normalization).\n",
    "\n",
    "With similarity measures explained, let's functionize our semantic search steps from above so we can repeat them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zI9DwA5mp1t-"
   },
   "source": [
    "### Functionizing our semantic search pipeline\n",
    "\n",
    "Let's put all of the steps from above for semantic search into a function or two so we can repeat the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1745905084105,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "49Y_eqcRp1t-"
   },
   "outputs": [],
   "source": [
    "def retrieve_relevant_resources(query: str,\n",
    "                                embeddings: torch.tensor,\n",
    "                                model: SentenceTransformer=embedding_model,\n",
    "                                n_resources_to_return: int=5,\n",
    "                                product=\"cosine\"\n",
    "                                ):\n",
    "    \"\"\"\n",
    "    Embeds a query with model and returns top k scores and indices from embeddings.\n",
    "    return scores, indices\n",
    "    \"\"\"\n",
    "\n",
    "    # Embed the query\n",
    "    # (You can refer to previous code on how to embed query. (remember to convert it to Tensor)\n",
    "    query_embedding = model.encode(query, convert_to_tensor=True)\n",
    "\n",
    "    if product==\"cosine\":\n",
    "       dot_scores = cosine_similarity(query_embedding, embeddings)\n",
    "    if product==\"dot_product\":\n",
    "       dot_scores = dot_product(query_embedding, embeddings)\n",
    "    if product==\"utils\":\n",
    "       dot_scores = util.dot_score(query_embedding, embeddings)[0]\n",
    "\n",
    "    #Obtain Scores and Indices for top k documents\n",
    "    scores, indices = torch.topk(dot_scores, k=n_resources_to_return)\n",
    "\n",
    "    return scores, indices\n",
    "\n",
    "\n",
    "def print_top_results_and_scores(query: str,\n",
    "                                 embeddings: torch.tensor,\n",
    "                                 pages_and_chunks: list[dict]=pages_and_chunks,\n",
    "                                 n_resources_to_return: int=5):\n",
    "    \"\"\"\n",
    "    Takes a query, retrieves most relevant resources and prints them out in descending order.\n",
    "\n",
    "    Note: Requires pages_and_chunks to be formatted in a specific way (see above for reference).\n",
    "    \"\"\"\n",
    "\n",
    "    #Obtain Scores and Indices using retrieve_relevant_resources\n",
    "    scores, indices = retrieve_relevant_resources(\n",
    "        query=query,\n",
    "        embeddings=embeddings,\n",
    "        n_resources_to_return=n_resources_to_return,\n",
    "    )\n",
    "\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    print(\"Results:\")\n",
    "    # Loop through zipped together scores and indicies\n",
    "    for score, index in zip(scores, indices):\n",
    "        print(f\"Score: {score:.4f}\")\n",
    "        # Print relevant sentence chunk (since the scores are in descending order, the most relevant chunk will be first)\n",
    "        print_wrapped(pages_and_chunks[index][\"sentence_chunk\"])\n",
    "        # Print the page number too so we can reference the textbook further and check the results\n",
    "        print(f\"Page number: {pages_and_chunks[index]['page_number']}\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qMvdFdlZp1t-"
   },
   "source": [
    "Excellent! Now let's test our functions out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 136,
     "status": "ok",
     "timestamp": 1745905084240,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "o7ZfzsMwp1t-",
    "outputId": "22e2ac3c-3d2c-413a-c02a-1f88e9546cc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: symptoms of pellagra\n",
      "\n",
      "Results:\n",
      "Score: 0.0122\n",
      "Niacin deficiency is commonly known as pellagra and the symptoms include\n",
      "fatigue, decreased appetite, and indigestion. These symptoms are then commonly\n",
      "followed by the four Ds: diarrhea, dermatitis, dementia, and sometimes death.\n",
      "Figure 9.12 Conversion of Tryptophan to Niacin Water-Soluble Vitamins | 565\n",
      "Page number: 565\n",
      "\n",
      "\n",
      "Score: 0.0091\n",
      "car. Does it drive faster with a half-tank of gas or a full one?It does not\n",
      "matter; the car drives just as fast as long as it has gas. Similarly, depletion\n",
      "of B vitamins will cause problems in energy metabolism, but having more than is\n",
      "required to run metabolism does not speed it up. Buyers of B-vitamin supplements\n",
      "beware; B vitamins are not stored in the body and all excess will be flushed\n",
      "down the toilet along with the extra money spent. B vitamins are naturally\n",
      "present in numerous foods, and many other foods are enriched with them. In the\n",
      "United States, B-vitamin deficiencies are rare; however in the nineteenth\n",
      "century some vitamin-B deficiencies plagued many people in North America. Niacin\n",
      "deficiency, also known as pellagra, was prominent in poorer Americans whose main\n",
      "dietary staple was refined cornmeal. Its symptoms were severe and included\n",
      "diarrhea, dermatitis, dementia, and even death. Some of the health consequences\n",
      "of pellagra are the result of niacin being in insufficient supply to support the\n",
      "bodys metabolic functions.\n",
      "Page number: 591\n",
      "\n",
      "\n",
      "Score: 0.0072\n",
      "The carbon dioxide gas bubbles infiltrate the stretchy gluten, giving bread its\n",
      "porosity and tenderness. For those who are sensitive to gluten, it is good to\n",
      "know that corn, millet, buckwheat, and oats do not contain the proteins that\n",
      "make gluten. However, some people who have celiac disease also may have a\n",
      "response to products containing oats. This is most likely the result of cross-\n",
      "contamination of grains during harvest, storage, packaging, and processing.\n",
      "Celiac disease is most common in people of European descent and is rare in\n",
      "people of African American, Japanese, and Chinese descent. It is much more\n",
      "prevalent in women and in people with Type 1 diabetes, autoimmune thyroid\n",
      "disease, and Down and Turner syndromes. Symptoms can range from mild to severe\n",
      "and can include pale, fatty, loose stools, gastrointestinal upset, abdominal\n",
      "pain, weight loss and, in children, a failure to grow and thrive. The symptoms\n",
      "can appear in infancy or much later in life, even Nutrition, Health and Disease\n",
      "| 1079\n",
      "Page number: 1079\n",
      "\n",
      "\n",
      "Score: 0.0068\n",
      "Image by BruceBlaus/ CC BY 4.0 When the vertebral bone tissue is weakened, it\n",
      "can cause the spine to curve. The increase in spine curvature not only causes\n",
      "pain, but also decreases a persons height. Curvature of the upper spine\n",
      "produces what is called Dowagers hump, also known as kyphosis. Severe upper-\n",
      "spine deformity can compress the chest cavity and cause difficulty breathing. It\n",
      "may also cause abdominal pain and loss of appetite because of the increased\n",
      "pressure on the abdomen. 1090 | Nutrition, Health and Disease\n",
      "Page number: 1090\n",
      "\n",
      "\n",
      "Score: 0.0066\n",
      "esophagus and cause irritation. It is estimated that GERD affects 25 to 35\n",
      "percent of the US population. An analysis of several studies published in the\n",
      "August 2005 issue of Annals of Internal Medicine concludes that GERD is much\n",
      "more prevalent in people who are obese.1 The most common GERD symptom is\n",
      "heartburn, but people with GERD may also experience regurgitation (flow of the\n",
      "stomachs acidic contents into the mouth), frequent coughing, and trouble\n",
      "swallowing. There are other causative factors of GERD that may be separate from\n",
      "or intertwined with obesity. The sphincter that separates the stomachs internal\n",
      "contents from the esophagus often does not function properly and acidic gastric\n",
      "contents seep upward. Sometimes the peristaltic contractions of the esophagus\n",
      "are also sluggish and compromise the clearance of acidic contents. In addition\n",
      "to having an unbalanced, high-fat diet, some people with GERD are sensitive to\n",
      "particular foodschocolate, garlic, spicy foods, fried foods, and tomato-based\n",
      "foodswhich worsen symptoms. Drinks containing alcohol or caffeine may also\n",
      "worsen GERD symptoms. GERD is diagnosed most often by a history of the frequency\n",
      "of recurring symptoms. A more proper diagnosis can be made when a doctor inserts\n",
      "a small device into the lower esophagus that measures the acidity of the\n",
      "contents during ones daily activities.\n",
      "Page number: 1077\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"symptoms of pellagra\"\n",
    "\n",
    "# Get just the scores and indices of top related results\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings,\n",
    "                                              product=\"cosine\")\n",
    "scores, indices\n",
    "\n",
    "# Print out the texts of the top scores\n",
    "print_top_results_and_scores(query=query,\n",
    "                             embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 90,
     "status": "ok",
     "timestamp": 1745905084330,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "4xTlGbApp1t-",
    "outputId": "52830499-847a-4322-83eb-b5c5c40d5b9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: symptoms of pellagra\n",
      "\n",
      "Results:\n",
      "Score: 0.0122\n",
      "Niacin deficiency is commonly known as pellagra and the symptoms include\n",
      "fatigue, decreased appetite, and indigestion. These symptoms are then commonly\n",
      "followed by the four Ds: diarrhea, dermatitis, dementia, and sometimes death.\n",
      "Figure 9.12 Conversion of Tryptophan to Niacin Water-Soluble Vitamins | 565\n",
      "Page number: 565\n",
      "\n",
      "\n",
      "Score: 0.0091\n",
      "car. Does it drive faster with a half-tank of gas or a full one?It does not\n",
      "matter; the car drives just as fast as long as it has gas. Similarly, depletion\n",
      "of B vitamins will cause problems in energy metabolism, but having more than is\n",
      "required to run metabolism does not speed it up. Buyers of B-vitamin supplements\n",
      "beware; B vitamins are not stored in the body and all excess will be flushed\n",
      "down the toilet along with the extra money spent. B vitamins are naturally\n",
      "present in numerous foods, and many other foods are enriched with them. In the\n",
      "United States, B-vitamin deficiencies are rare; however in the nineteenth\n",
      "century some vitamin-B deficiencies plagued many people in North America. Niacin\n",
      "deficiency, also known as pellagra, was prominent in poorer Americans whose main\n",
      "dietary staple was refined cornmeal. Its symptoms were severe and included\n",
      "diarrhea, dermatitis, dementia, and even death. Some of the health consequences\n",
      "of pellagra are the result of niacin being in insufficient supply to support the\n",
      "bodys metabolic functions.\n",
      "Page number: 591\n",
      "\n",
      "\n",
      "Score: 0.0072\n",
      "The carbon dioxide gas bubbles infiltrate the stretchy gluten, giving bread its\n",
      "porosity and tenderness. For those who are sensitive to gluten, it is good to\n",
      "know that corn, millet, buckwheat, and oats do not contain the proteins that\n",
      "make gluten. However, some people who have celiac disease also may have a\n",
      "response to products containing oats. This is most likely the result of cross-\n",
      "contamination of grains during harvest, storage, packaging, and processing.\n",
      "Celiac disease is most common in people of European descent and is rare in\n",
      "people of African American, Japanese, and Chinese descent. It is much more\n",
      "prevalent in women and in people with Type 1 diabetes, autoimmune thyroid\n",
      "disease, and Down and Turner syndromes. Symptoms can range from mild to severe\n",
      "and can include pale, fatty, loose stools, gastrointestinal upset, abdominal\n",
      "pain, weight loss and, in children, a failure to grow and thrive. The symptoms\n",
      "can appear in infancy or much later in life, even Nutrition, Health and Disease\n",
      "| 1079\n",
      "Page number: 1079\n",
      "\n",
      "\n",
      "Score: 0.0068\n",
      "Image by BruceBlaus/ CC BY 4.0 When the vertebral bone tissue is weakened, it\n",
      "can cause the spine to curve. The increase in spine curvature not only causes\n",
      "pain, but also decreases a persons height. Curvature of the upper spine\n",
      "produces what is called Dowagers hump, also known as kyphosis. Severe upper-\n",
      "spine deformity can compress the chest cavity and cause difficulty breathing. It\n",
      "may also cause abdominal pain and loss of appetite because of the increased\n",
      "pressure on the abdomen. 1090 | Nutrition, Health and Disease\n",
      "Page number: 1090\n",
      "\n",
      "\n",
      "Score: 0.0066\n",
      "esophagus and cause irritation. It is estimated that GERD affects 25 to 35\n",
      "percent of the US population. An analysis of several studies published in the\n",
      "August 2005 issue of Annals of Internal Medicine concludes that GERD is much\n",
      "more prevalent in people who are obese.1 The most common GERD symptom is\n",
      "heartburn, but people with GERD may also experience regurgitation (flow of the\n",
      "stomachs acidic contents into the mouth), frequent coughing, and trouble\n",
      "swallowing. There are other causative factors of GERD that may be separate from\n",
      "or intertwined with obesity. The sphincter that separates the stomachs internal\n",
      "contents from the esophagus often does not function properly and acidic gastric\n",
      "contents seep upward. Sometimes the peristaltic contractions of the esophagus\n",
      "are also sluggish and compromise the clearance of acidic contents. In addition\n",
      "to having an unbalanced, high-fat diet, some people with GERD are sensitive to\n",
      "particular foodschocolate, garlic, spicy foods, fried foods, and tomato-based\n",
      "foodswhich worsen symptoms. Drinks containing alcohol or caffeine may also\n",
      "worsen GERD symptoms. GERD is diagnosed most often by a history of the frequency\n",
      "of recurring symptoms. A more proper diagnosis can be made when a doctor inserts\n",
      "a small device into the lower esophagus that measures the acidity of the\n",
      "contents during ones daily activities.\n",
      "Page number: 1077\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"symptoms of pellagra\"\n",
    "\n",
    "# Get just the scores and indices of top related results\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings,\n",
    "                                              product=\"dot_product\")\n",
    "scores, indices\n",
    "\n",
    "# Print out the texts of the top scores\n",
    "print_top_results_and_scores(query=query,\n",
    "                             embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69,
     "status": "ok",
     "timestamp": 1745905084401,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "jB8r8S-b_ygc",
    "outputId": "3c080351-bdd1-441e-d74f-870095006694"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: symptoms of pellagra\n",
      "\n",
      "Results:\n",
      "Score: 0.0122\n",
      "Niacin deficiency is commonly known as pellagra and the symptoms include\n",
      "fatigue, decreased appetite, and indigestion. These symptoms are then commonly\n",
      "followed by the four Ds: diarrhea, dermatitis, dementia, and sometimes death.\n",
      "Figure 9.12 Conversion of Tryptophan to Niacin Water-Soluble Vitamins | 565\n",
      "Page number: 565\n",
      "\n",
      "\n",
      "Score: 0.0091\n",
      "car. Does it drive faster with a half-tank of gas or a full one?It does not\n",
      "matter; the car drives just as fast as long as it has gas. Similarly, depletion\n",
      "of B vitamins will cause problems in energy metabolism, but having more than is\n",
      "required to run metabolism does not speed it up. Buyers of B-vitamin supplements\n",
      "beware; B vitamins are not stored in the body and all excess will be flushed\n",
      "down the toilet along with the extra money spent. B vitamins are naturally\n",
      "present in numerous foods, and many other foods are enriched with them. In the\n",
      "United States, B-vitamin deficiencies are rare; however in the nineteenth\n",
      "century some vitamin-B deficiencies plagued many people in North America. Niacin\n",
      "deficiency, also known as pellagra, was prominent in poorer Americans whose main\n",
      "dietary staple was refined cornmeal. Its symptoms were severe and included\n",
      "diarrhea, dermatitis, dementia, and even death. Some of the health consequences\n",
      "of pellagra are the result of niacin being in insufficient supply to support the\n",
      "bodys metabolic functions.\n",
      "Page number: 591\n",
      "\n",
      "\n",
      "Score: 0.0072\n",
      "The carbon dioxide gas bubbles infiltrate the stretchy gluten, giving bread its\n",
      "porosity and tenderness. For those who are sensitive to gluten, it is good to\n",
      "know that corn, millet, buckwheat, and oats do not contain the proteins that\n",
      "make gluten. However, some people who have celiac disease also may have a\n",
      "response to products containing oats. This is most likely the result of cross-\n",
      "contamination of grains during harvest, storage, packaging, and processing.\n",
      "Celiac disease is most common in people of European descent and is rare in\n",
      "people of African American, Japanese, and Chinese descent. It is much more\n",
      "prevalent in women and in people with Type 1 diabetes, autoimmune thyroid\n",
      "disease, and Down and Turner syndromes. Symptoms can range from mild to severe\n",
      "and can include pale, fatty, loose stools, gastrointestinal upset, abdominal\n",
      "pain, weight loss and, in children, a failure to grow and thrive. The symptoms\n",
      "can appear in infancy or much later in life, even Nutrition, Health and Disease\n",
      "| 1079\n",
      "Page number: 1079\n",
      "\n",
      "\n",
      "Score: 0.0068\n",
      "Image by BruceBlaus/ CC BY 4.0 When the vertebral bone tissue is weakened, it\n",
      "can cause the spine to curve. The increase in spine curvature not only causes\n",
      "pain, but also decreases a persons height. Curvature of the upper spine\n",
      "produces what is called Dowagers hump, also known as kyphosis. Severe upper-\n",
      "spine deformity can compress the chest cavity and cause difficulty breathing. It\n",
      "may also cause abdominal pain and loss of appetite because of the increased\n",
      "pressure on the abdomen. 1090 | Nutrition, Health and Disease\n",
      "Page number: 1090\n",
      "\n",
      "\n",
      "Score: 0.0066\n",
      "esophagus and cause irritation. It is estimated that GERD affects 25 to 35\n",
      "percent of the US population. An analysis of several studies published in the\n",
      "August 2005 issue of Annals of Internal Medicine concludes that GERD is much\n",
      "more prevalent in people who are obese.1 The most common GERD symptom is\n",
      "heartburn, but people with GERD may also experience regurgitation (flow of the\n",
      "stomachs acidic contents into the mouth), frequent coughing, and trouble\n",
      "swallowing. There are other causative factors of GERD that may be separate from\n",
      "or intertwined with obesity. The sphincter that separates the stomachs internal\n",
      "contents from the esophagus often does not function properly and acidic gastric\n",
      "contents seep upward. Sometimes the peristaltic contractions of the esophagus\n",
      "are also sluggish and compromise the clearance of acidic contents. In addition\n",
      "to having an unbalanced, high-fat diet, some people with GERD are sensitive to\n",
      "particular foodschocolate, garlic, spicy foods, fried foods, and tomato-based\n",
      "foodswhich worsen symptoms. Drinks containing alcohol or caffeine may also\n",
      "worsen GERD symptoms. GERD is diagnosed most often by a history of the frequency\n",
      "of recurring symptoms. A more proper diagnosis can be made when a doctor inserts\n",
      "a small device into the lower esophagus that measures the acidity of the\n",
      "contents during ones daily activities.\n",
      "Page number: 1077\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"symptoms of pellagra\"\n",
    "\n",
    "# Get just the scores and indices of top related results\n",
    "scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                              embeddings=embeddings,\n",
    "                                              product=\"utils\")\n",
    "scores, indices\n",
    "\n",
    "# Print out the texts of the top scores\n",
    "print_top_results_and_scores(query=query,\n",
    "                             embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEkVj3O06-Hu"
   },
   "source": [
    "##Try 3 different queries for dot and cosine similarity and comment on differences in top k results if any. Also try product=\"utils\", does the score and final query change?\n",
    "\n",
    "The score and final queries for 3 different methods are the same, which is expected, because the dot product and cosine semilarity are doing the same things. The out put are same verctor, just scaled differently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJUmwErOp1t_"
   },
   "source": [
    "### Loading an LLM locally\n",
    "\n",
    "We will do it for `gemma-2b-it`\n",
    "\n",
    "\n",
    "The Hugging Face [`transformers`](https://huggingface.co/docs/transformers/) library has all the tools we need.\n",
    "\n",
    "To load our LLM, we're going to need a few things:\n",
    "1. A model ID - This is the reference Hugging Face model ID which will determine which tokenizer and model gets used. For example `gemma-2b-it`.2. A tokenzier - This is what will turn our raw text into tokens ready for the model. We can create it using the [`transformers.AutoTokenzier.from_pretrained`](https://huggingface.co/docs/transformers/v4.38.2/en/model_doc/auto#transformers.AutoTokenizer) method and passing it our model ID.\n",
    "3. An LLM model - Again, using our model ID we can load a specific LLM model. To do so we can use the [`transformers.AutoModelForCausalLM.from_pretrained`](https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM.from_pretrained) method and passing it our model ID as well as other various parameters.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let's do it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745905084406,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "iA-6HRpSjoR_",
    "outputId": "864a7b11-2faf-4822-e5ee-edf937177ced"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_id set to: /content/drive/MyDrive/gemma-transformers-1.1-2b-it-v1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "# Note: the following is Gemma focused, however, there are more and more LLMs of the 2B and 7B size appearing for local use.\n",
    "\n",
    "model_id = os.path.join(\"/content/drive/MyDrive/\",model_name)\n",
    "print(f\"model_id set to: {model_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 535,
     "referenced_widgets": [
      "70cdb00e808348dcaddaa7ec6f7fd9ac",
      "f12a051b04e5425da01a7babc4929454",
      "7313fdcf34764175bb092a488ff76fc7",
      "b32a9d3278074e22b652c588fc13ae1e",
      "5f13868b073d41fbb6f9afb07c2f9441",
      "ee39d84b949543509713a50094805ac1",
      "b594d2e620d44d959bb8e3cf5245184e",
      "6f21c25b5d304b179353abe2b42837a5",
      "8c5e430043d140c4b64f73b5ff67da4c",
      "549e8192fd7b4184a6a2c1e1eec21dc0",
      "7a7d5f82ecb04c6990e24facb2b9df18"
     ]
    },
    "executionInfo": {
     "elapsed": 36419,
     "status": "ok",
     "timestamp": 1745905120826,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "nJFS5kKFp1t_",
    "outputId": "eca5c36f-2beb-4d7d-a5cb-fcb018bbcab1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using attention implementation: sdpa\n",
      "[INFO] Using model_id: /content/drive/MyDrive/gemma-transformers-1.1-2b-it-v1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70cdb00e808348dcaddaa7ec6f7fd9ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GemmaForCausalLM(\n",
       "  (model): GemmaModel(\n",
       "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-17): 18 x GemmaDecoderLayer(\n",
       "        (self_attn): GemmaAttention(\n",
       "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
       "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
       "        )\n",
       "        (mlp): GemmaMLP(\n",
       "          (gate_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (up_proj): Linear(in_features=2048, out_features=16384, bias=False)\n",
       "          (down_proj): Linear(in_features=16384, out_features=2048, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): GemmaRMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): GemmaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.utils import is_flash_attn_2_available\n",
    "\n",
    "# 1. Create quantization config for smaller model loading (optional)\n",
    "# Requires !pip install bitsandbytes accelerate, see: https://github.com/TimDettmers/bitsandbytes, https://huggingface.co/docs/accelerate/\n",
    "# For models that require 4-bit quantization (use this if you have low GPU memory available)\n",
    "\n",
    "attn_implementation = \"sdpa\"\n",
    "print(f\"[INFO] Using attention implementation: {attn_implementation}\")\n",
    "\n",
    "# 2. Pick a model we'd like to use (this will depend on how much GPU memory you have available)\n",
    "#model_id = \"google/gemma-7b-it\"\n",
    "model_id = model_id # (we already set this above)\n",
    "print(f\"[INFO] Using model_id: {model_id}\")\n",
    "\n",
    "# 3. Instantiate tokenizer (tokenizer turns text into numbers ready for the model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_id)\n",
    "\n",
    "# 4. Instantiate the model\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id,\n",
    "                                                 torch_dtype=torch.float16, # datatype to use, we want float16\n",
    "                                                 low_cpu_mem_usage=False, # use full memory\n",
    "                                                 attn_implementation=attn_implementation) # which attention version to use\n",
    "\n",
    "llm_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AOfjjXbCp1t_"
   },
   "source": [
    "Parameters Of The LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41,
     "status": "ok",
     "timestamp": 1745905120868,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "DvioL7Iwp1t_",
    "outputId": "818eb519-0dfc-4d5d-c822-64d3626f3c0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model_mem_bytes': 5012345344, 'model_mem_mb': 4780.15, 'model_mem_gb': 4.67}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_num_params(model: torch.nn.Module):\n",
    "    return sum([param.numel() for param in model.parameters()])\n",
    "\n",
    "get_model_num_params(llm_model)\n",
    "\n",
    "def get_model_mem_size(model: torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Get how much memory a PyTorch model takes up.\n",
    "\n",
    "    See: https://discuss.pytorch.org/t/gpu-memory-that-model-uses/56822\n",
    "    \"\"\"\n",
    "    # Get model parameters and buffer sizes\n",
    "    mem_params = sum([param.nelement() * param.element_size() for param in model.parameters()])\n",
    "    mem_buffers = sum([buf.nelement() * buf.element_size() for buf in model.buffers()])\n",
    "\n",
    "    # Calculate various model sizes\n",
    "    model_mem_bytes = mem_params + mem_buffers # in bytes\n",
    "    model_mem_mb = model_mem_bytes / (1024**2) # in megabytes\n",
    "    model_mem_gb = model_mem_bytes / (1024**3) # in gigabytes\n",
    "\n",
    "    return {\"model_mem_bytes\": model_mem_bytes,\n",
    "            \"model_mem_mb\": round(model_mem_mb, 2),\n",
    "            \"model_mem_gb\": round(model_mem_gb, 2)}\n",
    "\n",
    "get_model_mem_size(llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PzSBE1MZp1t_"
   },
   "source": [
    "### Generating text with our LLM\n",
    "\n",
    "We can generate text with our LLM `model` instance by calling the [`generate()` method](https://huggingface.co/docs/transformers/v4.38.2/en/main_classes/text_generation#transformers.GenerationConfig) (this method has plenty of options to pass into it alongside the text) on it and passing it a tokenized input.\n",
    "\n",
    "The tokenized input comes from passing a string of text to our `tokenizer`.\n",
    "\n",
    "It's important to note that you should use a tokenizer that has been paired with a model.\n",
    "\n",
    "Otherwise if you try to use a different tokenizer and then pass those inputs to a model, you will likely get errors/strange results.\n",
    "\n",
    "For some LLMs, there's a specific template you should pass to them for ideal outputs.\n",
    "\n",
    "For example, the `gemma-2b-it` model has been trained in a dialogue fashion (instruction tuning).\n",
    "\n",
    "In this case, our `tokenizer` has a [`apply_chat_template()` method](https://huggingface.co/docs/transformers/main/en/internal/tokenization_utils#transformers.PreTrainedTokenizerBase.apply_chat_template) which can prepare our input text in the right format for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 56,
     "status": "ok",
     "timestamp": 1745905120925,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "p9g0eidzp1t_",
    "outputId": "20b1ccdb-ad34-46ee-a5c5-d4f28639c4b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text:\n",
      "What are the macronutrients, and what roles do they play in the human body?\n",
      "\n",
      "Prompt (formatted):\n",
      "<bos><start_of_turn>user\n",
      "What are the macronutrients, and what roles do they play in the human body?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_text = \"What are the macronutrients, and what roles do they play in the human body?\"\n",
    "print(f\"Input text:\\n{input_text}\")\n",
    "\n",
    "# Create prompt template for instruction-tuned model\n",
    "dialogue_template = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": input_text}\n",
    "]\n",
    "\n",
    "# Apply the chat template\n",
    "prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                       tokenize=False, # keep as raw text (not tokenized)\n",
    "                                       add_generation_prompt=True)\n",
    "print(f\"\\nPrompt (formatted):\\n{prompt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPjI7UHbp1t_"
   },
   "source": [
    "Notice the scaffolding around our input text, this is the kind of turn-by-turn instruction tuning our model has gone through.\n",
    "\n",
    "Our next step is to tokenize this formatted text and pass it to our model's `generate()` method.\n",
    "\n",
    "We'll make sure our tokenized text is on the same device as our model (GPU) using `to(\"cuda\")`.\n",
    "\n",
    "We can conver the output tokens to text using [`tokenizer.decode()`](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.decode)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vxr2mBU2p1uA"
   },
   "source": [
    "> **Note:** `\"<bos>\"` and `\"<eos>\"` are special tokens to denote \"beginning of sentence\" and \"end of sentence\" respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1745905120928,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "Erk7RqqIp1uA"
   },
   "outputs": [],
   "source": [
    "## Feel free to play with the questions\n",
    "\n",
    "# Nutrition-style questions generated with GPT4\n",
    "gpt4_questions = [\n",
    "    \"What are the macronutrients, and what roles do they play in the human body?\",\n",
    "    \"How do vitamins and minerals differ in their roles and importance for health?\",\n",
    "    \"Describe the process of digestion and absorption of nutrients in the human body.\",\n",
    "    \"What role does fibre play in digestion? Name five fibre containing foods.\",\n",
    "    \"Explain the concept of energy balance and its importance in weight management.\"\n",
    "]\n",
    "\n",
    "# Manually created question list\n",
    "manual_questions = [\n",
    "    \"How often should infants be breastfed?\",\n",
    "    \"What are symptoms of pellagra?\",\n",
    "    \"How does saliva help with digestion?\",\n",
    "    \"What is the RDI for protein per day?\",\n",
    "    \"water soluble vitamins\"\n",
    "]\n",
    "\n",
    "query_list = gpt4_questions + manual_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHILOHcpp1uA"
   },
   "source": [
    "### Augmenting our prompt with context items\n",
    "\n",
    "What we'd like to do with augmentation is take the results from our search for relevant resources and put them into the prompt that we pass to our LLM.\n",
    "\n",
    "In essence, we start with a base prompt and update it with context text.\n",
    "\n",
    "Let's write a function called `prompt_formatter` that takes in a query and our list of context items (in our case it'll be select indices from our list of dictionaries inside `pages_and_chunks`) and then formats the query with text from the context items.\n",
    "\n",
    "We'll apply the dialogue and chat template to our prompt before returning it as well.\n",
    "\n",
    "> **Note:** The process of augmenting or changing a prompt to an LLM is known as prompt engineering. And the best way to do it is an active area of research. For a comprehensive guide on different prompt engineering techniques, I'd recommend the Prompt Engineering Guide ([promptingguide.ai](https://www.promptingguide.ai/)), [Brex's Prompt Engineering Guide](https://github.com/brexhq/prompt-engineering) and the paper [Prompt Design and Engineering: Introduction and Advanced Models](https://arxiv.org/abs/2401.14423)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1745905120933,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "ZBzmx3CIp1uA"
   },
   "outputs": [],
   "source": [
    "def prompt_formatter(query: str,\n",
    "                     context_items: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Augments query with text-based context from context_items.\n",
    "    \"\"\"\n",
    "    # Join context items into one dotted paragraph\n",
    "    context = \"- \" + \"\\n- \".join([item[\"sentence_chunk\"] for item in context_items])\n",
    "\n",
    "    # Create a base prompt with examples to help the model\n",
    "    # Note: this is very customizable, I've chosen to use 3 examples of the answer style we'd like.\n",
    "    # We could also write this in a txt file and import it in if we wanted.\n",
    "    base_prompt = \"\"\"Based on the following context items, please answer the query.\n",
    "Give yourself room to think by extracting relevant passages from the context before answering the query.\n",
    "Don't return the thinking, only return the answer.\n",
    "Make sure your answers are as explanatory as possible.\n",
    "Use the following examples as reference for the ideal answer style.\n",
    "\\nExample 1:\n",
    "Query: What are the fat-soluble vitamins?\n",
    "Answer: The fat-soluble vitamins include Vitamin A, Vitamin D, Vitamin E, and Vitamin K. These vitamins are absorbed along with fats in the diet and can be stored in the body's fatty tissue and liver for later use. Vitamin A is important for vision, immune function, and skin health. Vitamin D plays a critical role in calcium absorption and bone health. Vitamin E acts as an antioxidant, protecting cells from damage. Vitamin K is essential for blood clotting and bone metabolism.\n",
    "\\nExample 2:\n",
    "Query: What are the causes of type 2 diabetes?\n",
    "Answer: Type 2 diabetes is often associated with overnutrition, particularly the overconsumption of calories leading to obesity. Factors include a diet high in refined sugars and saturated fats, which can lead to insulin resistance, a condition where the body's cells do not respond effectively to insulin. Over time, the pancreas cannot produce enough insulin to manage blood sugar levels, resulting in type 2 diabetes. Additionally, excessive caloric intake without sufficient physical activity exacerbates the risk by promoting weight gain and fat accumulation, particularly around the abdomen, further contributing to insulin resistance.\n",
    "\\nExample 3:\n",
    "Query: What is the importance of hydration for physical performance?\n",
    "Answer: Hydration is crucial for physical performance because water plays key roles in maintaining blood volume, regulating body temperature, and ensuring the transport of nutrients and oxygen to cells. Adequate hydration is essential for optimal muscle function, endurance, and recovery. Dehydration can lead to decreased performance, fatigue, and increased risk of heat-related illnesses, such as heat stroke. Drinking sufficient water before, during, and after exercise helps ensure peak physical performance and recovery.\n",
    "\\nNow use the following context items to answer the user query:\n",
    "{context}\n",
    "\\nRelevant passages: <extract relevant passages from the context here>\n",
    "User query: {query}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    # Update base prompt with context items and query\n",
    "    base_prompt = base_prompt.format(context=context, query=query)\n",
    "\n",
    "    # Create prompt template for instruction-tuned model\n",
    "    dialogue_template = [\n",
    "        {\"role\": \"user\",\n",
    "        \"content\": base_prompt}\n",
    "    ]\n",
    "\n",
    "    # Apply the chat template\n",
    "    prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                          tokenize=False,\n",
    "                                          add_generation_prompt=True)\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6uJHH28p1uA"
   },
   "source": [
    "Looking good! Let's try our function out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 45,
     "status": "ok",
     "timestamp": 1745905120979,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "Z-ly04eop1uA",
    "outputId": "0bdae00c-76c3-4031-fdee-469c21d5f8f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How do vitamins and minerals differ in their roles and importance for health?\n",
      "<bos><start_of_turn>user\n",
      "What are the macronutrients, and what roles do they play in the human body?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "query = random.choice(query_list)\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Create prompt template for instruction-tuned model\n",
    "dialogue_template = [\n",
    "    {\"role\": \"user\",\n",
    "     \"content\": input_text}\n",
    "]\n",
    "\n",
    "# Apply the chat template\n",
    "prompt = tokenizer.apply_chat_template(conversation=dialogue_template,\n",
    "                                       tokenize=False, # keep as raw text (not tokenized)\n",
    "                                       add_generation_prompt=True)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9995,
     "status": "ok",
     "timestamp": 1745905130974,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "HHeJZwhSp1uA",
    "outputId": "77156a8a-ca58-478c-a0ea-d7fb9762795b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: How do vitamins and minerals differ in their roles and importance for health?\n",
      "RAG answer:\n",
      "<bos>**Macronutrients** are the three main types of nutrients that the body uses for energy, growth, and repair:\n",
      "\n",
      "**1. Protein:**\n",
      "\n",
      "- Essential for building and repairing tissues, enzymes, hormones, and immune cells.\n",
      "- Plays a role in protein synthesis and muscle growth.\n",
      "- Provides energy for bodily functions.\n",
      "\n",
      "**2. Carbohydrates:**\n",
      "\n",
      "- The body's primary source of energy.\n",
      "- Used by cells for energy, storing energy as glycogen, and producing glucose for glucose metabolism.\n",
      "- Provides structure to cells and tissues.\n",
      "\n",
      "**3. Fats:**\n",
      "\n",
      "- Essential for energy storage, insulation, hormone production, and cell membrane formation.\n",
      "- Provide a concentrated source of energy and are important for brain function and nerve transmission.\n",
      "- Contribute to the formation of hormones, bile acids, and cholesterol.\n",
      "\n",
      "**Roles of Macronutrients in the Human Body:**\n",
      "\n",
      "**1. Energy Production:**\n",
      "- Provide the body with the energy it needs to function.\n",
      "- Fuel cellular activities and bodily processes.\n",
      "\n",
      "**2. Tissue Repair and Growth:**\n",
      "- Build and repair muscles, bones, cartilage, and soft tissues.\n",
      "- Support immune system function and prevent chronic diseases.\n",
      "\n",
      "**3. Metabolism and Regulation:**\n",
      "- Control\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate an output of tokens\n",
    "outputs = llm_model.generate(**input_ids,\n",
    "                             temperature=0.7, # lower temperature = more deterministic outputs, higher temperature = more creative outputs\n",
    "                             do_sample=True, # whether or not to use sampling, see https://huyenchip.com/2024/01/16/sampling.html for more\n",
    "                             max_new_tokens=256) # how many new tokens to generate from prompt\n",
    "\n",
    "# Turn the output tokens into text\n",
    "output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"RAG answer:\\n{output_text.replace(prompt, '')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKU7tJcrp1uA"
   },
   "source": [
    "Our RAG pipeline is complete!\n",
    "\n",
    "We just Retrieved, Augmented and Generated!\n",
    "\n",
    "And all on our own local GPU!\n",
    "\n",
    "How about we functionize the generation step to make it easier to use?\n",
    "\n",
    "We can put a little formatting on the text being returned to make it look nice too.\n",
    "\n",
    "And we'll make an option to return the context items if needed as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1745905130975,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "2sXb3FiFp1uA"
   },
   "outputs": [],
   "source": [
    "def ask(query,\n",
    "        temperature=0.7,\n",
    "        max_new_tokens=512,\n",
    "        format_answer_text=True,\n",
    "        return_answer_only=True):\n",
    "    \"\"\"\n",
    "    Takes a query, finds relevant resources/context and generates an answer to the query based on the relevant resources.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get just the scores and indices of top related results\n",
    "    scores, indices = retrieve_relevant_resources(query=query,\n",
    "                                                  embeddings=embeddings)\n",
    "\n",
    "    # Create a list of context items\n",
    "    context_items = [pages_and_chunks[i] for i in indices]\n",
    "\n",
    "    # Add score to context item\n",
    "    for i, item in enumerate(context_items):\n",
    "        item[\"score\"] = scores[i].cpu() # return score back to CPU\n",
    "\n",
    "    # Format the prompt with context items\n",
    "    prompt = prompt_formatter(query=query,\n",
    "                              context_items=context_items)\n",
    "\n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "    # Generate an output of tokens\n",
    "    outputs = llm_model.generate(**input_ids,\n",
    "                                 temperature=temperature,\n",
    "                                 do_sample=True,\n",
    "                                 max_new_tokens=max_new_tokens)\n",
    "\n",
    "    # Turn the output tokens into text\n",
    "    output_text = tokenizer.decode(outputs[0])\n",
    "\n",
    "    if format_answer_text:\n",
    "        # Replace special tokens and unnecessary help message\n",
    "        output_text = output_text.replace(prompt, \"\").replace(\"<bos>\", \"\").replace(\"<eos>\", \"\").replace(\"Sure, here is the answer to the user query:\\n\\n\", \"\")\n",
    "\n",
    "    # Only return the answer without the context items\n",
    "    if return_answer_only:\n",
    "        return output_text\n",
    "\n",
    "    return output_text, context_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3075,
     "status": "ok",
     "timestamp": 1745905152851,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "a9CaBYNLp1uA",
    "outputId": "8c2b9041-1067-42d0-acb8-aa20c31d7c1a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: water soluble vitamins\n",
      "Answer:\n",
      "\n",
      "**Water-soluble vitamins are required as functional parts of enzymes involved in\n",
      "energy release and storage.**  These vitamins are absorbed in the small\n",
      "intestine but are transported to the liver through blood vessels. They play a\n",
      "crucial role in metabolism and blood function by assisting in converting a\n",
      "substrate to an end-product and facilitating various biochemical reactions.\n",
      "Context items:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'page_number': 592,\n",
       "  'sentence_chunk': 'recommended that users complete these activities using a desktop or laptop computer and in Google Chrome. \\xa0 An interactive or media element has been excluded from this version of the text. You can view it online here: http://pressbooks.oer.hawaii.edu/ humannutrition2/?p=347 \\xa0 592 | Water-Soluble Vitamins',\n",
       "  'chunk_char_count': 305,\n",
       "  'chunk_word_count': 44,\n",
       "  'chunk_token_count': 76.25,\n",
       "  'embedding': array([ 1.31825311e-02, -5.76655753e-02, -5.24329953e-03, -2.93033267e-03,\n",
       "          4.46129739e-02,  2.92155873e-02,  2.06981711e-02,  2.26659495e-02,\n",
       "          8.79949108e-02,  2.06947071e-03, -3.64047615e-03,  4.82459664e-02,\n",
       "         -6.89612643e-04, -2.15124600e-02, -2.46340106e-03, -2.21591089e-02,\n",
       "         -1.44984135e-02,  2.34625731e-02, -2.20572762e-02,  1.16169238e-02,\n",
       "         -3.23735672e-04,  8.18866771e-03,  6.48807874e-03, -6.07292401e-03,\n",
       "          2.90775369e-03,  2.08477192e-02,  2.59447377e-02,  1.03110177e-02,\n",
       "         -7.81230163e-03, -9.17905495e-02,  2.81078159e-04,  5.67551814e-02,\n",
       "          3.62043791e-02, -7.64738098e-02,  2.13102840e-06, -1.45653067e-02,\n",
       "         -3.62870563e-03,  7.78660318e-03, -7.40434453e-02,  6.83321878e-02,\n",
       "          1.36635061e-02, -4.57317494e-02,  8.40815192e-04,  1.25524597e-02,\n",
       "          3.78874242e-02,  7.89504945e-02,  4.33884095e-03,  1.82802845e-02,\n",
       "         -2.61090994e-02, -3.34323235e-02, -9.90247063e-05,  9.48757231e-02,\n",
       "          4.92338948e-02,  3.45960595e-02,  6.55795112e-02, -3.93609144e-02,\n",
       "          6.32835925e-03,  1.01005800e-01,  4.26845402e-02,  4.03087139e-02,\n",
       "          3.19996662e-02,  6.77034035e-02, -2.16911286e-02,  1.88961383e-02,\n",
       "         -1.39179714e-02,  7.11333007e-02, -6.49291798e-02, -1.25494733e-01,\n",
       "          2.33228318e-02,  5.94829805e-02,  7.72745758e-02,  2.20212750e-02,\n",
       "          2.05327850e-02,  8.05954915e-03, -2.17657946e-02,  3.17269824e-02,\n",
       "          9.30006572e-05, -9.16713197e-03, -1.97608280e-03, -8.47726874e-03,\n",
       "         -3.71088013e-02, -3.60255986e-02,  1.36295054e-02,  3.65317171e-03,\n",
       "          8.20682049e-02,  2.44929008e-02, -2.65286420e-03,  1.37783950e-02,\n",
       "         -3.89290936e-02, -2.40202043e-02, -5.30586727e-02, -1.18657830e-03,\n",
       "          2.50301342e-02,  2.15555318e-02,  2.01427266e-02, -5.41892163e-02,\n",
       "         -3.06075960e-02, -3.86505504e-03,  6.60087615e-02, -5.10303117e-02,\n",
       "          7.55808726e-02, -6.74210489e-03, -5.08709103e-02,  1.72712989e-02,\n",
       "          1.81474369e-02, -6.49658591e-03, -7.51515059e-03,  3.07808742e-02,\n",
       "         -3.62133756e-02,  6.21045679e-02, -3.11712902e-02, -2.99869608e-02,\n",
       "         -4.90780547e-02,  2.22712010e-02, -4.65637557e-02, -3.84037662e-03,\n",
       "         -2.47179363e-02, -1.21503379e-02,  1.27636353e-02, -4.25385088e-02,\n",
       "         -3.08682583e-03,  1.24267722e-02,  3.95563319e-02,  2.24759374e-02,\n",
       "         -9.00304317e-02,  5.39007634e-02, -2.37496803e-03, -8.65518115e-03,\n",
       "          4.30377107e-03, -8.08766037e-02,  5.03186695e-02, -3.94206345e-02,\n",
       "         -2.06908733e-02,  4.01174463e-03,  1.57708414e-02, -7.90850818e-03,\n",
       "          5.09794727e-02, -3.53244855e-03, -1.35987652e-02, -3.04518230e-02,\n",
       "         -6.18200786e-02, -3.61916167e-03, -6.08896613e-02,  2.78873667e-02,\n",
       "          8.20279717e-02, -2.86811590e-02,  3.44315879e-02, -1.19735673e-02,\n",
       "          1.90773234e-02,  6.06017783e-02, -1.11197354e-02,  1.69930782e-03,\n",
       "          3.74032464e-03,  1.91085488e-02,  8.34385529e-02, -1.07403919e-02,\n",
       "          3.24087776e-02, -5.05087748e-02,  2.75345966e-02,  3.33784744e-02,\n",
       "          1.66991353e-02, -5.17200446e-03, -5.40766167e-03, -6.82464316e-02,\n",
       "         -3.70663055e-03,  9.33533348e-03,  4.14617695e-02,  1.96766648e-02,\n",
       "         -5.01905046e-02,  7.08037987e-04, -3.88514772e-02, -1.72349140e-02,\n",
       "          6.33149315e-03,  9.35151801e-03,  5.62576056e-02,  1.56800039e-02,\n",
       "          2.49910331e-03,  2.46262588e-02, -1.61326751e-02,  2.44142804e-02,\n",
       "          7.76518136e-02, -2.80578006e-02, -2.25022286e-02,  1.98916979e-02,\n",
       "         -2.85672434e-02,  1.00469161e-02,  7.59438437e-04,  1.75848063e-02,\n",
       "         -3.20344120e-02, -1.15901455e-02,  1.30613931e-02,  7.64501747e-03,\n",
       "          4.08373885e-02, -3.13548744e-02,  1.35102766e-02, -1.58023331e-02,\n",
       "         -6.75789360e-03, -1.31487781e-02,  3.28795686e-02, -3.04070078e-02,\n",
       "          5.40161245e-02,  6.81901425e-02,  5.07574677e-02,  5.48396595e-02,\n",
       "         -4.12027836e-02,  6.22607954e-03,  5.65817468e-02,  3.19638802e-03,\n",
       "          5.79630621e-02, -5.94276143e-03, -5.72302192e-02, -5.90240397e-03,\n",
       "          3.72078340e-03, -1.18324300e-02, -4.78167180e-03, -7.53098950e-02,\n",
       "          6.04630950e-05, -2.46779388e-03, -5.19799851e-02,  1.17605790e-01,\n",
       "          8.62280931e-03, -5.07721724e-03, -3.79097201e-02, -1.16786268e-02,\n",
       "          5.99624403e-02, -5.98944053e-02, -1.16180405e-02,  1.66779477e-02,\n",
       "         -3.68233770e-02,  7.94327538e-03,  4.57511023e-02,  3.26322205e-02,\n",
       "         -1.61036069e-03,  4.36167717e-02, -4.97722626e-02, -5.54902256e-02,\n",
       "          5.77012487e-02,  8.01316462e-03, -2.03095633e-03,  1.67653412e-02,\n",
       "         -1.92399509e-02, -2.16150451e-02, -3.31547894e-02,  1.32964049e-02,\n",
       "         -1.57406821e-03,  8.90703406e-03, -5.40615395e-02, -5.35883056e-03,\n",
       "         -2.17008442e-02,  4.49118428e-02,  1.23383747e-02,  5.80612943e-03,\n",
       "         -1.93090197e-02,  2.07060110e-02, -3.96212637e-02,  6.76707625e-02,\n",
       "         -2.83684172e-02, -6.06343783e-02, -7.10675195e-02,  5.10916710e-02,\n",
       "          1.13452766e-02, -7.00831506e-03,  4.32015024e-02, -7.31614942e-04,\n",
       "         -2.45542657e-02, -3.05892359e-02,  2.68280059e-02,  1.37381032e-02,\n",
       "         -4.07694727e-02,  1.37418564e-02,  2.49030199e-02, -1.50228404e-02,\n",
       "          2.06406005e-02,  1.54386861e-02,  1.98751595e-03,  1.51215326e-02,\n",
       "         -7.39923492e-02, -2.46385597e-02,  3.07902880e-02,  1.76563393e-02,\n",
       "         -6.37589302e-03,  5.41125499e-02, -1.38231842e-02,  6.90944446e-03,\n",
       "          1.13382572e-02, -1.67513210e-02, -1.95090715e-02,  4.40077595e-02,\n",
       "          7.98296779e-02, -5.10163754e-02,  2.71496791e-02,  8.47490621e-04,\n",
       "          4.54593748e-02,  2.84671020e-02,  2.82406732e-02,  2.92365141e-02,\n",
       "         -1.26276202e-02,  5.04271500e-02,  1.70967653e-02,  3.52587290e-02,\n",
       "          1.38155362e-02,  1.75517201e-02, -1.31882681e-02,  3.20820399e-02,\n",
       "         -2.18170583e-02,  4.54237573e-02,  7.04176584e-03, -5.07574789e-02,\n",
       "          2.36182585e-02, -3.37668438e-03, -1.19600473e-02,  5.66277355e-02,\n",
       "         -9.05274972e-03,  4.31360342e-02, -2.28643287e-02,  5.54379495e-03,\n",
       "         -2.22032145e-02,  1.29064638e-02, -1.07277129e-02, -3.73287462e-02,\n",
       "          1.87586050e-03, -3.63190379e-03, -9.20113642e-03, -2.83392351e-02,\n",
       "         -2.04364359e-02, -4.99765538e-02, -5.08244243e-03,  5.61831072e-02,\n",
       "         -1.17883995e-01, -5.99624179e-02, -4.70364504e-02, -8.25818107e-02,\n",
       "          5.49679715e-03, -8.75942584e-04,  1.83568988e-02,  4.31828462e-02,\n",
       "         -2.54906192e-02,  2.81391870e-02,  5.41370502e-03, -5.14171161e-02,\n",
       "         -7.22762346e-02,  2.54411274e-03,  3.13468017e-02,  3.01693892e-03,\n",
       "          7.37745501e-03,  6.50384789e-03,  4.13007326e-02, -6.10969681e-03,\n",
       "         -2.96949074e-02, -1.95868481e-02,  2.82100160e-02, -1.92944631e-02,\n",
       "         -1.98943242e-02,  3.48039307e-02, -2.57252459e-03,  2.00035200e-02,\n",
       "          1.36449104e-02, -4.25681174e-02, -4.88350727e-02, -1.60990134e-02,\n",
       "          7.22161159e-02,  5.47271818e-02, -2.94606127e-02, -4.98123467e-02,\n",
       "          9.24388617e-02,  2.41260957e-02, -6.00594357e-02,  3.46684530e-02,\n",
       "         -3.35518308e-02, -2.88173128e-02,  1.50628260e-03,  3.13347839e-02,\n",
       "         -1.09133718e-03,  2.44868617e-03,  2.49190014e-02,  1.58412512e-02,\n",
       "          3.66379507e-02,  3.85951810e-02, -8.04117844e-02, -2.65365764e-02,\n",
       "         -5.46862418e-03,  2.40477808e-02,  5.04099913e-02,  5.87089034e-03,\n",
       "          4.68894690e-02,  2.24828832e-02, -3.54933999e-02, -2.19388604e-02,\n",
       "          1.03824876e-01, -4.08333167e-02,  2.75745383e-03,  4.32718210e-02,\n",
       "         -3.66439335e-02, -2.80299466e-02, -1.59790907e-02,  4.64010350e-02,\n",
       "          6.38483316e-02, -1.20316744e-02, -4.03521629e-03,  2.83785071e-02,\n",
       "          3.98824885e-02, -6.76425174e-02,  3.46868485e-02,  2.32606139e-02,\n",
       "          6.05381355e-02, -8.51025954e-02,  9.10709146e-03,  7.99056981e-03,\n",
       "          6.52756840e-02, -6.04093680e-03, -5.22137480e-03, -5.99443130e-02,\n",
       "          1.09678535e-02, -2.83949133e-02,  8.34550783e-02, -2.39506899e-03,\n",
       "          2.30252072e-02, -2.75896788e-02,  4.29449379e-02, -5.98939508e-02,\n",
       "         -3.68925333e-02,  1.77078173e-02,  8.79034027e-03,  2.41470300e-02,\n",
       "         -2.57836282e-02, -1.04041481e-02,  7.10184872e-03,  1.85042582e-02,\n",
       "         -6.24347553e-02, -5.06545557e-03, -1.58097576e-02, -1.47663907e-03,\n",
       "          2.42914110e-02, -3.66600114e-03, -1.53142679e-02, -2.51311827e-02,\n",
       "          5.15674092e-02, -6.59613609e-02, -4.76972619e-03, -3.80529538e-02,\n",
       "         -2.75442507e-02,  5.27476408e-02,  3.27023566e-02, -2.09965538e-02,\n",
       "          3.09996158e-02,  8.46443325e-03,  9.12347902e-03,  4.21994403e-02,\n",
       "          3.73117737e-02,  3.60542797e-02, -3.61962314e-03,  4.86881807e-02,\n",
       "         -2.99608306e-04,  2.54619401e-02, -2.29649600e-02, -7.72714708e-03,\n",
       "          1.82843879e-02, -4.49418500e-02,  2.28702426e-02, -5.69169000e-02,\n",
       "         -2.59126946e-02, -7.71569228e-03, -1.49888759e-02, -1.87233575e-02,\n",
       "          2.12657098e-02,  2.76675224e-02, -1.70243271e-02, -3.32485177e-02,\n",
       "         -4.56153639e-02, -3.77352536e-02,  1.56731550e-02,  4.96087335e-02,\n",
       "         -1.62826497e-02, -3.26377451e-02, -3.78083251e-02, -4.22777608e-02,\n",
       "         -6.36984780e-02,  1.52016571e-02,  1.04766283e-02,  4.70366962e-02,\n",
       "         -2.78760754e-02, -5.11831231e-03,  8.91443808e-03, -2.87688430e-03,\n",
       "          1.22574512e-02, -5.68405865e-03, -3.15495469e-02, -2.18460280e-02,\n",
       "         -6.09624200e-03,  7.30570930e-04,  2.64152903e-02,  2.84519568e-02,\n",
       "         -1.55773442e-02,  2.31042448e-02,  2.68700096e-04,  4.68972698e-02,\n",
       "         -3.26741636e-02,  4.29712757e-02,  5.27602583e-02,  8.88524111e-03,\n",
       "          5.09010367e-02,  4.89001814e-03,  1.18550919e-02, -1.02283740e-02,\n",
       "         -5.63324578e-02,  3.85630876e-02, -4.95365262e-02,  7.42768217e-03,\n",
       "         -5.11846319e-02, -1.53978383e-02, -5.04307263e-02, -9.96529218e-03,\n",
       "         -1.63753759e-02,  3.60989161e-02, -6.02491423e-02,  3.01095601e-02,\n",
       "         -2.21771020e-02, -2.97946502e-02, -4.51994389e-02,  2.27444358e-02,\n",
       "          5.98639809e-03, -5.13992570e-02, -2.03796569e-02,  2.88748313e-02,\n",
       "         -1.79153103e-02,  4.84578051e-02,  3.71598676e-02,  2.51651346e-03,\n",
       "         -3.54514271e-02,  9.05017368e-03,  2.01294292e-02, -2.86821648e-02,\n",
       "          2.97062891e-03, -5.80706075e-02, -1.47820869e-02,  2.24247109e-02,\n",
       "         -5.74211106e-02,  6.42646849e-02, -4.02511768e-02, -1.43092060e-02,\n",
       "          2.99077053e-02, -3.77663486e-02,  1.51097039e-02, -8.21319502e-03,\n",
       "          2.16368437e-02,  3.73416804e-02, -9.59471241e-02, -7.18667498e-03,\n",
       "          3.06785088e-02, -1.47550374e-01, -3.53499129e-03,  2.92911194e-02,\n",
       "         -4.47793566e-02, -1.43008111e-02,  4.06535231e-02, -5.69588139e-33,\n",
       "          6.01690412e-02, -2.60138996e-02,  8.56014714e-03,  2.50593517e-02,\n",
       "          3.15702595e-02,  5.15528321e-02, -6.08102744e-03, -1.37955099e-02,\n",
       "         -2.95244325e-02,  3.46105881e-02,  8.41849204e-03,  3.07388194e-02,\n",
       "          1.73256490e-02, -2.79483385e-02, -8.88192374e-03, -1.69246476e-02,\n",
       "          1.39536019e-02, -5.70811927e-02,  2.39296407e-02,  2.30274675e-03,\n",
       "         -6.03055470e-02, -4.60699350e-02, -1.51095698e-02, -1.85946636e-02,\n",
       "          3.10970582e-02, -5.62550090e-02,  2.96221972e-02, -6.97094426e-02,\n",
       "         -4.03656019e-03,  6.59753233e-02,  4.44887159e-03, -6.09860430e-03,\n",
       "          9.23832506e-03,  5.33271618e-02, -4.85260747e-02,  2.19284985e-02,\n",
       "         -4.73634526e-02,  4.67354096e-02, -1.73647702e-02,  2.49222163e-02,\n",
       "         -1.05898805e-01, -1.64320190e-02, -1.46051254e-02, -2.00888067e-02,\n",
       "          3.06125488e-02,  7.92235415e-03, -3.23492475e-02, -1.49436202e-02,\n",
       "         -4.43378985e-02,  2.84800548e-02, -2.25263145e-02, -1.34851448e-02,\n",
       "         -9.20157414e-03,  5.85647859e-03,  2.17738468e-02,  8.69818851e-02,\n",
       "          2.67057531e-02, -1.48536190e-02,  1.79091580e-02,  2.55036149e-02,\n",
       "         -3.40873674e-02,  2.33558174e-02,  1.70364082e-02, -8.10574461e-03,\n",
       "          1.43753085e-03,  5.14448434e-03, -1.02375392e-02, -1.13474177e-02,\n",
       "         -5.86340316e-02, -3.69287692e-02, -1.47959532e-03, -2.30974313e-02,\n",
       "         -3.20485868e-02,  1.17048132e-03, -4.77194116e-02, -3.93026844e-02,\n",
       "         -3.44676152e-02,  1.87224206e-02, -2.46961955e-02,  4.74284496e-03,\n",
       "         -2.72937622e-02, -7.68427225e-03,  8.34859721e-03, -2.03929525e-02,\n",
       "         -6.22332748e-03,  4.50356398e-03, -2.68852096e-02, -2.04051584e-02,\n",
       "          2.59445366e-02, -3.47309969e-02,  2.99814660e-02, -4.07060459e-02,\n",
       "          7.51843164e-03,  2.99052317e-02, -1.49255088e-02,  2.13547915e-04,\n",
       "          1.08722476e-02, -5.55367325e-04,  1.87016129e-02, -2.60935072e-02,\n",
       "         -6.95336163e-02,  3.69004272e-02,  6.54916465e-02, -4.10055816e-02,\n",
       "         -4.96415459e-02, -3.19240913e-02, -1.00468555e-02, -3.12037133e-02,\n",
       "          1.50737120e-02, -3.61072458e-02,  1.95930637e-02,  1.08725065e-02,\n",
       "          3.95905785e-02, -8.26160517e-03, -1.36898039e-02, -1.07376138e-02,\n",
       "          6.00891560e-03,  4.24459614e-02, -2.34582881e-03, -2.42983811e-02,\n",
       "         -1.98402312e-02, -3.70562710e-02,  5.32052256e-02, -6.65756036e-03,\n",
       "          1.77915432e-02, -4.69058342e-02,  9.11871716e-03,  4.71826307e-02,\n",
       "          1.45058869e-03,  8.95327982e-03,  1.88950766e-02, -8.69615842e-03,\n",
       "          2.84743322e-07,  2.22975714e-03,  2.67361198e-02,  8.52322672e-03,\n",
       "         -9.63034630e-02, -1.46405678e-02, -4.90138270e-02, -1.09843565e-02,\n",
       "         -2.21838262e-02,  2.05979478e-02, -1.74932797e-02,  8.34650695e-02,\n",
       "         -3.68438810e-02, -5.55081153e-03, -1.92019325e-02, -1.18375309e-02,\n",
       "          2.01347005e-02, -4.50924359e-04, -8.21594670e-02,  1.50102554e-02,\n",
       "         -3.37932706e-02, -5.50026773e-03,  6.87319972e-03, -3.24962251e-02,\n",
       "         -1.99526083e-02,  1.58315636e-02, -5.31297475e-02, -9.68866516e-03,\n",
       "          1.90555174e-02,  1.84555016e-02, -5.97918639e-03, -8.28236155e-03,\n",
       "          5.18272957e-03,  2.16469336e-02,  3.07803992e-02, -6.16884753e-02,\n",
       "         -6.29784092e-02,  2.14595627e-02,  4.51647304e-03, -3.90107892e-02,\n",
       "         -4.43987688e-03, -5.72804436e-02,  1.16105508e-02, -3.81837459e-03,\n",
       "          1.58996414e-02, -5.44314384e-02,  5.65729104e-02, -2.01352164e-02,\n",
       "          3.45918238e-02, -2.03759074e-02,  1.62913778e-03,  5.40426336e-02,\n",
       "          3.40324305e-02, -1.22674590e-03,  3.00405454e-03, -5.03674597e-02,\n",
       "          4.98743728e-03,  2.26849541e-02,  7.89809227e-03,  7.58208036e-02,\n",
       "         -3.06545403e-02, -3.84979062e-02, -1.05863549e-02, -3.22082601e-02,\n",
       "         -2.73368545e-02,  8.98574479e-03,  5.44319153e-02,  1.17485300e-02,\n",
       "          2.08529346e-34, -3.11306398e-02, -4.25469428e-02,  5.81206754e-02,\n",
       "         -5.68449358e-03,  1.30696669e-02,  2.12708097e-02, -2.72747781e-02,\n",
       "          8.01951997e-03,  1.52058322e-02, -6.49141520e-02, -1.09106386e-02]),\n",
       "  'score': tensor(0.0193)},\n",
       " {'page_number': 550,\n",
       "  'sentence_chunk': 'Water-Soluble Vitamins UNIVERSITY OF HAWAII AT MNOA FOOD SCIENCE AND HUMAN NUTRITION PROGRAM AND HUMAN NUTRITION PROGRAM All water-soluble vitamins play a different kind of role in energy metabolism; they are required as functional parts of enzymes involved in energy release and storage. Vitamins and minerals that make up part of enzymes are referred to as coenzymes and cofactors, respectively. Coenzymes and cofactors are required by enzymes to catalyze a specific reaction. They assist in converting a substrate to an end-product. Coenzymes and cofactors are essential in catabolic pathways and play a role in many anabolic pathways too. In addition to being essential for metabolism, many vitamins and minerals are required for blood renewal and function. At insufficient levels in the diet these vitamins and minerals impair the health of blood and consequently the delivery of nutrients in and wastes out, amongst its many other functions. In this section we will focus on the vitamins that take part in metabolism and blood function and renewal. Figure 9.7 Enzyme Active Site for Cofactors 550 | Water-Soluble Vitamins',\n",
       "  'chunk_char_count': 1129,\n",
       "  'chunk_word_count': 176,\n",
       "  'chunk_token_count': 282.25,\n",
       "  'embedding': array([ 3.44977304e-02, -5.73528036e-02,  4.19655964e-02, -2.13542650e-03,\n",
       "          2.78809499e-02,  2.26335395e-02, -1.82942562e-02,  1.08353999e-02,\n",
       "          2.43804250e-02, -8.29460248e-02, -3.54524031e-02,  8.70856419e-02,\n",
       "          1.34557206e-03, -1.15103461e-02, -2.74515767e-02,  3.20066512e-02,\n",
       "          1.05833691e-02,  2.89709656e-03, -3.55004407e-02,  2.49088909e-02,\n",
       "         -2.26032399e-02,  1.62048247e-02,  1.61558501e-02,  2.31105499e-02,\n",
       "         -2.01375708e-02,  1.17336512e-02, -1.94608234e-03, -2.33253697e-03,\n",
       "          1.24812042e-02, -4.83271554e-02,  1.94779888e-03,  1.84912551e-02,\n",
       "          2.07915250e-02, -4.44020182e-02,  2.05775132e-06, -4.77656797e-02,\n",
       "         -4.08900902e-02, -2.25990452e-02, -7.06644803e-02,  3.91537212e-02,\n",
       "         -1.85535625e-02, -7.83546865e-02,  1.81679595e-02,  4.29686013e-04,\n",
       "          2.76332386e-02,  6.47929534e-02, -1.67517010e-02, -1.57728456e-02,\n",
       "         -4.06662514e-03, -1.12414332e-02, -2.41656881e-03,  6.07434884e-02,\n",
       "          5.61072379e-02,  5.34874089e-02,  2.58263052e-02, -3.51168178e-02,\n",
       "          4.71079163e-02,  8.00385252e-02,  8.05081706e-03,  4.49188091e-02,\n",
       "          2.04621255e-02, -4.67088161e-04, -1.38450544e-02,  3.20459493e-02,\n",
       "          5.65782748e-03,  5.54739088e-02, -4.54656817e-02, -4.27833274e-02,\n",
       "         -1.44196264e-02,  3.41856740e-02,  9.96376202e-03, -1.64667424e-02,\n",
       "         -7.07289670e-04, -1.20833172e-02, -3.93553451e-03,  4.82856855e-02,\n",
       "          1.63275339e-02, -9.20357835e-03,  4.76215919e-03,  1.49551816e-02,\n",
       "          4.45745401e-02,  1.16306888e-02,  2.02960614e-02, -3.97245586e-02,\n",
       "          5.42214364e-02, -2.04156507e-02, -4.60753171e-03,  1.40650086e-02,\n",
       "         -2.79053468e-02, -1.76862199e-02, -8.89333636e-02, -1.14383446e-02,\n",
       "          3.04329135e-02,  1.07000945e-02,  1.08536649e-02, -1.67737193e-02,\n",
       "         -6.56853914e-02,  1.17242774e-02,  6.02719188e-02, -3.04307118e-02,\n",
       "          3.39105800e-02, -2.16712225e-02, -7.78309628e-02,  3.37592214e-02,\n",
       "         -2.16003451e-02, -2.77244560e-02, -5.47654042e-03, -1.26758767e-02,\n",
       "         -4.28060070e-02, -3.16073447e-02, -1.42557202e-02, -3.17322128e-02,\n",
       "         -4.37001102e-02,  7.60401338e-02, -2.40074121e-03,  9.24164243e-03,\n",
       "         -8.26018769e-03, -8.87721370e-04, -2.72004865e-02,  9.95174865e-04,\n",
       "         -1.31825134e-02,  3.62111107e-02,  3.35161686e-02,  6.72797789e-04,\n",
       "         -7.00944364e-02,  6.74140826e-02, -1.02991774e-03, -5.42924888e-02,\n",
       "         -5.04536368e-02, -5.17910160e-02,  6.87164068e-02, -5.31465933e-02,\n",
       "          2.23213900e-03, -2.75596734e-02, -5.74917952e-03, -3.52112427e-02,\n",
       "          8.47141072e-02,  2.59341858e-03, -4.12877314e-02, -4.94574010e-02,\n",
       "         -2.72693355e-02,  7.16739940e-03, -4.63694036e-02,  1.80991907e-02,\n",
       "          3.70421261e-02, -3.56794358e-03,  1.34355286e-02,  7.06645334e-03,\n",
       "          1.71031188e-02,  3.21213603e-02,  2.74530500e-02,  9.74935573e-03,\n",
       "          1.25510655e-02,  4.26541567e-02,  5.29590212e-02, -5.41815907e-02,\n",
       "          3.46128047e-02, -3.26619223e-02,  4.14348580e-02, -1.89905185e-02,\n",
       "          4.02627140e-02, -3.00464239e-02,  2.75123641e-02, -7.49013796e-02,\n",
       "         -9.13287699e-03, -3.23636346e-02,  5.58620356e-02,  2.39309482e-02,\n",
       "         -3.20131853e-02, -2.76998314e-03,  2.01372188e-02, -5.59069812e-02,\n",
       "          4.23414893e-02, -3.57321464e-02, -3.49755995e-02,  2.87167914e-02,\n",
       "          2.77451426e-02,  4.69539352e-02, -5.04270894e-03, -3.84187396e-03,\n",
       "          3.10632978e-02,  1.06594767e-02,  5.86438458e-04,  2.87993588e-02,\n",
       "         -2.27684341e-02, -1.85264312e-02,  1.65512115e-02, -1.25389835e-02,\n",
       "         -1.24689564e-03, -1.09815253e-02,  2.75894180e-02, -1.77164038e-03,\n",
       "          3.09791509e-02, -4.53143343e-02, -1.42735271e-02, -2.15112306e-02,\n",
       "         -6.94889799e-02, -9.42815281e-03,  3.31118964e-02, -1.85680073e-02,\n",
       "          6.85369000e-02,  4.42967266e-02, -1.37474993e-02,  2.58067604e-02,\n",
       "         -3.40578854e-02,  3.17824595e-02,  1.43019371e-02,  3.35234255e-02,\n",
       "          3.97318341e-02, -4.63232845e-02, -9.32607334e-03, -1.57927368e-02,\n",
       "          1.84646714e-02, -3.39620076e-02, -6.25710469e-03, -8.35236460e-02,\n",
       "          1.89287234e-02, -2.27805655e-02, -6.32334873e-02,  8.81811380e-02,\n",
       "          2.30678869e-03,  8.90820846e-03, -4.21953462e-02, -3.77624445e-02,\n",
       "          6.17461577e-02, -3.41998786e-02, -3.37158479e-02,  4.51493785e-02,\n",
       "         -5.85413836e-02, -4.95891459e-03,  4.19959910e-02,  3.61295417e-02,\n",
       "          7.46581587e-04, -2.78086239e-03, -6.23674244e-02, -3.68230268e-02,\n",
       "          3.31379697e-02, -2.09469721e-02, -6.96913060e-03,  1.10469416e-01,\n",
       "          6.23642700e-04, -1.97568014e-02, -1.21693294e-02,  1.91270746e-02,\n",
       "         -1.08002303e-02,  5.18337339e-02, -6.15123026e-02,  4.66633886e-02,\n",
       "         -5.36363460e-02,  2.00811382e-02,  3.66757885e-02,  1.77712217e-02,\n",
       "         -3.76459956e-03,  1.60079338e-02, -2.34894007e-02,  3.42776030e-02,\n",
       "         -4.40177449e-04, -2.70322226e-02, -2.97035836e-02,  4.71114367e-02,\n",
       "          2.56636087e-03,  3.69948223e-02,  4.53194417e-02,  3.01959459e-02,\n",
       "         -6.84833080e-02, -2.61491351e-02,  1.44280577e-02,  2.82744598e-02,\n",
       "         -2.21757740e-02,  8.65682960e-03,  1.60756782e-02, -5.84909506e-03,\n",
       "          2.63122506e-02,  3.95863503e-02,  1.65916067e-02, -1.34585360e-02,\n",
       "         -7.19979778e-02, -1.19964201e-02,  7.21541001e-03,  9.69511736e-03,\n",
       "         -6.15343340e-02,  3.16081382e-02,  3.97130987e-03,  3.82595100e-02,\n",
       "          1.92695651e-02, -2.28577829e-03, -1.05087217e-02,  3.80251855e-02,\n",
       "          9.77545455e-02, -2.83322316e-02,  7.41166398e-02,  1.27422307e-02,\n",
       "          3.54137234e-02,  2.30546277e-02,  3.12267002e-02,  1.47937946e-02,\n",
       "         -1.29867727e-02,  1.27795022e-02,  2.15863753e-02, -9.10770614e-03,\n",
       "          3.13056633e-03, -4.38454486e-02, -1.85961444e-02,  5.69225959e-02,\n",
       "         -4.83727306e-02,  1.34562273e-02,  5.02217636e-02, -3.25620770e-02,\n",
       "          8.30067396e-02,  2.92124022e-02, -8.95413104e-03, -6.32712501e-04,\n",
       "         -2.68377215e-02, -1.60596110e-02, -2.32173912e-02, -3.74868256e-03,\n",
       "          8.80132988e-03, -2.28573717e-02,  1.86743252e-02, -4.14223038e-02,\n",
       "          1.69881862e-02,  9.52731725e-03,  2.87781795e-03, -2.63193473e-02,\n",
       "         -7.27686426e-03, -2.03113481e-02,  2.04614140e-02,  5.22079784e-03,\n",
       "         -9.29258466e-02, -3.45910005e-02, -4.80369814e-02, -7.21148700e-02,\n",
       "          1.09138340e-02,  3.83368619e-02, -1.56252030e-02,  2.20214035e-02,\n",
       "         -1.46026602e-02,  5.04286354e-03, -7.30343163e-03, -3.04269381e-02,\n",
       "         -5.06882854e-02,  4.00970504e-02,  1.03799012e-02,  7.88709968e-02,\n",
       "          3.81178930e-02,  2.10990850e-02, -2.33751838e-03,  5.73797058e-03,\n",
       "          4.60041277e-02,  5.25322929e-03,  1.09197050e-02, -1.78373531e-02,\n",
       "         -1.10435691e-02,  3.17625441e-02, -3.08617335e-02, -2.02817954e-02,\n",
       "         -2.27102321e-02,  1.29787475e-02, -3.88950817e-02, -1.37733258e-02,\n",
       "          1.14946134e-01,  4.39823195e-02, -2.82276385e-02, -5.92339598e-02,\n",
       "          5.53372949e-02,  7.00977072e-03, -4.20758538e-02,  1.83178652e-02,\n",
       "         -7.91281369e-03, -6.83984160e-02,  1.51289245e-02,  2.76684612e-02,\n",
       "          5.96982893e-03,  1.11717284e-02,  2.72523258e-02,  1.88221165e-03,\n",
       "          4.86405417e-02,  7.84650352e-03, -6.67953342e-02, -2.53185444e-02,\n",
       "         -1.95455607e-02,  2.63105221e-02,  1.02658719e-02,  1.60868037e-02,\n",
       "          3.27427052e-02,  1.10919485e-02, -5.28345294e-02, -3.50951143e-02,\n",
       "          9.64629129e-02, -2.03771442e-02,  2.45269854e-02,  5.54856099e-02,\n",
       "         -7.06491694e-02,  3.24043119e-03, -2.12489837e-03,  2.41111647e-02,\n",
       "          5.43416664e-02, -1.17018167e-02,  1.88113743e-04,  4.38978449e-02,\n",
       "          2.29025260e-02, -6.21635243e-02,  6.23562112e-02, -5.80216618e-03,\n",
       "          6.11078739e-03, -9.23865810e-02, -1.61831162e-03,  2.28726361e-02,\n",
       "          8.78257453e-02,  6.55837059e-02,  2.52177827e-02, -8.62630382e-02,\n",
       "          1.28546320e-02, -4.31767143e-02,  4.18146215e-02,  6.17528707e-03,\n",
       "          2.48628035e-02,  1.73155777e-02,  5.26862256e-02, -1.01532072e-01,\n",
       "         -6.26761392e-02,  2.10808869e-02, -3.83062125e-03, -7.01314351e-03,\n",
       "         -1.52125014e-02, -3.06108780e-02, -4.18353640e-02, -6.80110417e-03,\n",
       "         -1.93211287e-02,  1.73092764e-02, -4.21606638e-02, -3.34790722e-02,\n",
       "          4.77431715e-02,  3.32480157e-03, -6.69154944e-03, -7.20281713e-03,\n",
       "          3.70308980e-02, -4.94071171e-02,  1.63248777e-02, -3.40754837e-02,\n",
       "         -4.09809127e-02,  5.19315116e-02, -6.76999381e-03, -4.04809266e-02,\n",
       "          4.99559604e-02,  2.12009829e-02, -3.84701155e-02,  7.95961022e-02,\n",
       "          4.63353768e-02,  5.80594577e-02, -2.35974267e-02,  2.61550769e-02,\n",
       "         -1.64607186e-02,  2.15635374e-02, -2.43631694e-02, -5.10518625e-03,\n",
       "         -1.76760014e-02, -3.66000980e-02,  3.57244611e-02, -6.21603243e-02,\n",
       "         -3.60061228e-02, -2.33010948e-02,  1.68054253e-02, -2.30958802e-03,\n",
       "         -1.09009845e-02, -6.28755987e-03, -1.97737981e-02, -4.57375944e-02,\n",
       "         -4.31532376e-02, -2.59730127e-02,  4.29978482e-02,  4.54385616e-02,\n",
       "          1.65929797e-03, -4.41181436e-02, -4.10738885e-02, -7.07878247e-02,\n",
       "         -8.26720428e-03,  6.10515289e-02,  1.23639191e-02,  8.03921223e-02,\n",
       "         -5.07263541e-02, -6.42458024e-03,  3.54334107e-03, -2.00516335e-03,\n",
       "         -1.01387529e-02, -6.72682072e-04, -3.70240435e-02, -1.70775317e-02,\n",
       "         -1.85622945e-02,  1.28030255e-02,  4.73256484e-02,  5.18122241e-02,\n",
       "         -1.07832262e-02,  1.02289172e-03,  3.15449992e-03,  1.64564885e-02,\n",
       "         -1.78025067e-02,  4.08190563e-02,  1.56308059e-02,  2.54852399e-02,\n",
       "          5.26709966e-02, -1.87172387e-02,  1.78230908e-02,  1.29385628e-02,\n",
       "         -1.32546779e-02,  2.21133102e-02, -5.30199744e-02, -5.02326153e-03,\n",
       "         -6.57438710e-02,  9.00070090e-03, -4.80348654e-02, -2.69302651e-02,\n",
       "         -5.41422749e-03,  6.33885711e-02, -3.02270316e-02,  1.48839392e-02,\n",
       "          2.89649051e-03, -1.58447698e-02, -3.64485942e-02, -8.12769309e-03,\n",
       "         -2.88894936e-03, -1.66957136e-02, -3.83981597e-03,  4.23390456e-02,\n",
       "         -1.56923849e-02,  3.68287638e-02,  3.75124440e-02, -2.32324586e-03,\n",
       "         -4.56777327e-02,  3.56206112e-02,  5.63740209e-02, -9.30140074e-03,\n",
       "         -5.38791940e-02, -5.96008450e-02,  1.07175084e-02,  3.02426387e-02,\n",
       "         -9.16621163e-02,  5.23241684e-02, -3.02057154e-02, -4.54978202e-04,\n",
       "         -3.09849251e-03,  7.03975325e-03,  8.42225552e-03,  5.58953034e-03,\n",
       "          1.17671946e-02,  2.32326309e-03, -6.23842105e-02, -4.80815507e-02,\n",
       "          2.45178031e-04, -1.37822926e-01,  2.12324737e-03,  3.81663516e-02,\n",
       "          1.11564016e-02, -2.69258283e-02,  5.76336347e-02, -5.28804436e-33,\n",
       "          1.06980868e-01, -3.25296149e-02,  2.92455629e-02,  4.47776020e-02,\n",
       "          2.57634223e-02,  8.38274211e-02, -1.48980832e-03, -4.00364473e-02,\n",
       "          1.40130497e-03,  2.08439063e-02,  2.22428385e-02, -1.95176119e-03,\n",
       "          1.71667605e-03, -3.40256058e-02, -4.85980930e-03, -5.14985025e-02,\n",
       "          3.86671908e-02, -3.85425799e-02,  3.44938822e-02,  3.28363404e-02,\n",
       "         -4.04312052e-02, -2.76130643e-02,  1.04988529e-03,  3.06513309e-02,\n",
       "          2.33356319e-02, -3.90099548e-02,  3.12488042e-02, -2.47423537e-02,\n",
       "         -4.11416516e-02,  7.17803985e-02, -1.01363929e-02, -1.13413157e-02,\n",
       "         -3.40762585e-02,  4.07222249e-02, -1.95303187e-02, -4.52112313e-03,\n",
       "         -2.71185469e-02,  2.46156678e-02,  2.37132004e-03,  5.52434586e-02,\n",
       "         -9.68978480e-02, -1.18155591e-02,  1.93874147e-02,  9.14221536e-03,\n",
       "          8.82105494e-04,  5.11606820e-02, -6.23417534e-02,  1.39358817e-02,\n",
       "          2.34290753e-02,  2.02428084e-02, -1.18396720e-02, -7.85980280e-03,\n",
       "         -1.21969134e-02, -2.21347809e-02,  1.69464014e-02,  4.78479266e-02,\n",
       "          1.79158561e-02, -2.81091779e-02,  1.82051733e-02, -3.00808661e-02,\n",
       "         -3.04199345e-02,  3.84295210e-02,  3.94001650e-03,  2.73874309e-02,\n",
       "         -4.05010171e-02,  4.28314367e-03, -1.49504887e-02,  4.00564000e-02,\n",
       "         -4.29013185e-02,  4.06889543e-02, -1.24354959e-02, -2.39717122e-02,\n",
       "         -1.10878162e-02, -9.55429114e-03,  5.42727020e-03, -1.20174009e-02,\n",
       "         -3.80758755e-02,  7.72922346e-03, -1.23105403e-02, -2.40896083e-02,\n",
       "         -3.57697532e-02, -1.19745114e-03,  6.32664189e-03, -3.14230993e-02,\n",
       "         -3.46161649e-02,  2.45444011e-02, -1.89835113e-02, -7.78213423e-03,\n",
       "         -2.01433208e-02, -5.47088906e-02,  3.17759514e-02, -1.93914771e-02,\n",
       "          1.73285566e-02,  2.85932291e-02, -5.06299585e-02,  5.55427447e-02,\n",
       "          5.10711863e-04,  5.30091720e-03, -2.85489764e-03, -1.42347068e-02,\n",
       "         -4.45905924e-02,  1.11542763e-02,  5.78679331e-02, -8.94812942e-02,\n",
       "         -6.71752393e-02, -4.80380729e-02, -1.42981783e-02,  2.09192876e-02,\n",
       "          3.47929411e-02, -6.85471110e-03,  8.22128169e-03,  1.86074171e-02,\n",
       "          4.05195355e-02, -1.40756974e-03,  1.72097255e-02,  3.56380902e-02,\n",
       "          2.31686030e-02,  2.26755962e-02,  7.50329811e-03, -4.89033014e-02,\n",
       "         -1.67581197e-02,  2.14875061e-02,  7.37119168e-02,  1.62145775e-02,\n",
       "         -2.35138442e-02, -4.67909612e-02,  1.42398290e-02,  2.17619985e-02,\n",
       "         -1.43859014e-02,  8.88675451e-03, -5.67763695e-04,  1.93256177e-02,\n",
       "          2.92566511e-07, -1.78521350e-02, -1.29817640e-02,  7.55304797e-03,\n",
       "         -1.09389618e-01,  5.90182515e-03,  9.25638620e-03,  1.73448958e-02,\n",
       "         -2.65360177e-02,  1.73199810e-02, -3.59795578e-02,  1.55906184e-02,\n",
       "         -5.19429296e-02,  1.66162383e-02,  1.11612407e-02, -1.69324689e-03,\n",
       "          7.41923973e-02,  2.42363624e-02, -7.11623728e-02,  1.29953828e-02,\n",
       "         -4.20039408e-02, -8.00898671e-02, -1.72221940e-02, -3.79922651e-02,\n",
       "         -4.11370536e-04, -1.80592332e-02, -9.02687106e-03,  3.64262867e-03,\n",
       "          1.69484764e-02, -7.16921082e-03,  2.65361406e-02, -5.07816561e-02,\n",
       "          7.83752464e-03,  3.66340689e-02,  8.58003972e-04, -6.27769623e-03,\n",
       "          1.71749406e-02,  2.92729773e-02, -1.32647594e-02, -2.00900976e-02,\n",
       "         -4.80782837e-02, -2.68327184e-02,  1.68325081e-02, -1.99358687e-02,\n",
       "          7.01132119e-02, -7.59111941e-02,  2.15428323e-02, -1.15114748e-02,\n",
       "         -3.39537039e-02,  2.52887737e-02,  2.84869666e-03,  1.39015280e-02,\n",
       "          2.65850201e-02, -7.05391075e-03,  4.31993939e-02, -2.35109497e-02,\n",
       "          9.42145067e-04,  1.87067653e-03,  3.65157761e-02,  4.55666408e-02,\n",
       "         -2.73752604e-02,  9.19403881e-03, -6.11758849e-04, -1.81188565e-02,\n",
       "          1.12848170e-02,  3.71061601e-02,  7.16807172e-02,  1.67777184e-02,\n",
       "          2.56964282e-34, -2.64311023e-02, -5.16259409e-02,  3.17380019e-03,\n",
       "          3.47109623e-02,  1.08728223e-02,  4.71242797e-03, -9.57297757e-02,\n",
       "          1.48600955e-02,  2.87077716e-03, -2.49553341e-02, -3.30718197e-02]),\n",
       "  'score': tensor(0.0186)},\n",
       " {'page_number': 518,\n",
       "  'sentence_chunk': 'Image by Allison Calabrese / CC BY 4.0 One major difference between fat-soluble vitamins and water- soluble vitamins is the way they are absorbed in the body. Vitamins are absorbed primarily in the small intestine and their bioavailability is dependent on the food composition of the diet. Fat-soluble vitamins are absorbed along with dietary fat. Therefore, if a meal is very low in fat, the absorption of the fat-soluble vitamins will be impaired. Once fat-soluble vitamins have been absorbed in the small intestine, they are packaged and incorporated into chylomicrons along with other fatty acids and transported in the lymphatic system to the liver. Water-soluble vitamins on the other hand are absorbed in the small intestine but are transported to the liver through blood vessels. Figure 9.2 Absorption of Fat-Soluble and Water-Soluble Vitamins \\xa0 518 | Introduction',\n",
       "  'chunk_char_count': 873,\n",
       "  'chunk_word_count': 137,\n",
       "  'chunk_token_count': 218.25,\n",
       "  'embedding': array([ 3.24403904e-02, -7.45072812e-02,  2.06934679e-02,  1.61913820e-02,\n",
       "          6.14634864e-02, -1.53850783e-02, -2.24049576e-02, -1.44041460e-02,\n",
       "          7.56372064e-02, -6.72431104e-03, -2.66022962e-02,  4.36297841e-02,\n",
       "          1.11033057e-03, -6.14660680e-02,  2.73271697e-03, -1.46693634e-02,\n",
       "         -2.90795956e-02, -4.47562300e-02, -3.10942121e-02,  1.53067494e-02,\n",
       "          2.79331859e-02,  8.41288920e-03,  1.87524185e-02, -1.65166929e-02,\n",
       "          9.86215658e-04,  2.90437378e-02,  7.63962511e-03,  1.38788642e-02,\n",
       "         -3.54275592e-02, -2.71092635e-02,  1.08196186e-02,  7.99052883e-03,\n",
       "          7.62168095e-02,  2.59895176e-02,  1.82512247e-06,  9.11093224e-03,\n",
       "         -1.64907817e-02,  8.36366974e-03, -3.32904570e-02,  2.00633202e-02,\n",
       "          1.62449572e-02, -5.00287488e-02,  1.22901509e-02,  2.42404174e-02,\n",
       "          5.38276322e-02,  4.62485179e-02, -1.55924605e-02, -1.27192959e-02,\n",
       "         -1.02585936e-02, -1.17834071e-02,  1.56511348e-02,  3.58266719e-02,\n",
       "          3.97913903e-02,  8.04903954e-02, -3.83348437e-04, -7.41603971e-02,\n",
       "          1.16444388e-02,  1.28729478e-01, -4.04306836e-02,  5.11830375e-02,\n",
       "         -4.37530829e-03, -4.11208998e-03, -2.67325360e-02,  2.09479649e-02,\n",
       "          2.21421476e-03,  6.85065389e-02, -7.51338378e-02, -4.65407921e-03,\n",
       "         -1.52328946e-02,  2.97451336e-02, -3.75851104e-03,  7.64246751e-03,\n",
       "          7.01390347e-03, -2.15798561e-02, -1.07424408e-02,  6.39246427e-04,\n",
       "          2.75123268e-02, -2.87663452e-02,  9.69226181e-04,  7.84177240e-03,\n",
       "          8.13941378e-03, -1.66279618e-02,  3.43180038e-02, -2.16226932e-02,\n",
       "          6.03933411e-04,  1.61087885e-03,  1.99502297e-02, -5.75985666e-03,\n",
       "         -3.99503820e-02, -2.11844258e-02, -6.21281639e-02, -3.36364619e-02,\n",
       "          7.47634051e-03,  2.34682336e-02,  2.14158669e-02, -3.11472397e-02,\n",
       "         -3.01165469e-02,  2.84818020e-02,  4.91396040e-02, -2.16346923e-02,\n",
       "          4.24789004e-02, -4.93389145e-02, -1.28857866e-01,  3.47283743e-02,\n",
       "         -7.48947775e-03,  3.74777094e-02, -3.48776542e-02,  4.70100194e-02,\n",
       "         -3.21972184e-02, -2.20520440e-02, -2.04611849e-02, -3.76579575e-02,\n",
       "         -2.15476621e-02,  6.10792078e-02, -7.20953345e-02, -1.38777727e-02,\n",
       "         -5.74537106e-02, -2.80193463e-02, -2.94575691e-02, -4.36271280e-02,\n",
       "          3.85275967e-02, -8.92721955e-03,  3.00276764e-02,  2.09785309e-02,\n",
       "         -7.23023638e-02,  6.78405538e-02,  1.16714574e-02, -1.44561604e-02,\n",
       "          1.39401061e-03,  2.37222333e-02,  5.96734956e-02,  1.43610407e-02,\n",
       "          3.56785916e-02,  7.81813916e-03,  1.95589010e-02, -1.13497544e-02,\n",
       "          6.25113472e-02, -1.43735558e-02, -3.09925079e-02, -1.26339002e-02,\n",
       "         -3.69417109e-02, -2.27392875e-02, -1.71615686e-02,  1.58943590e-02,\n",
       "          1.82875786e-02, -3.12857330e-02,  5.27549861e-03,  2.51065101e-02,\n",
       "          1.08309500e-02,  6.78870156e-02,  2.08995249e-02,  9.31857154e-03,\n",
       "          5.14045320e-02,  4.33028676e-03,  7.99437985e-02, -5.18263578e-02,\n",
       "         -8.59577558e-05, -1.64294112e-02,  1.53523749e-02, -2.55484264e-02,\n",
       "          3.20044272e-02, -2.06734464e-02,  4.39290740e-02, -6.86382204e-02,\n",
       "         -1.30372646e-03, -6.20647054e-03,  3.03777605e-02,  2.79564429e-02,\n",
       "         -1.81660596e-02, -4.40068729e-03,  2.45062336e-02,  5.52579109e-03,\n",
       "          1.71784665e-02,  9.59880836e-03, -7.14388862e-03,  3.32460515e-02,\n",
       "          5.45767285e-02, -3.83445807e-02, -2.05318723e-03,  3.21864188e-02,\n",
       "          2.38364339e-02,  8.75188224e-03, -1.15408208e-02,  5.11122309e-03,\n",
       "         -3.05353990e-03,  1.96015332e-02,  2.21894830e-02, -2.23113280e-02,\n",
       "         -2.10593008e-02,  1.66107826e-02,  2.81757349e-03, -1.87808461e-02,\n",
       "          5.53502329e-02, -4.12946604e-02, -3.15208640e-03, -1.60384607e-02,\n",
       "         -7.84398913e-02,  1.48709081e-02,  5.87201081e-02,  1.44074922e-02,\n",
       "          2.28908937e-02,  4.46525328e-02, -4.59252521e-02,  7.36599835e-03,\n",
       "         -6.38734698e-02,  3.44516858e-02,  3.94741818e-02,  1.09937973e-03,\n",
       "          7.12781623e-02, -5.77223562e-02, -4.52939123e-02,  6.91344915e-03,\n",
       "         -8.38738494e-03, -1.77939665e-02,  2.16699895e-02, -6.04802370e-02,\n",
       "         -8.34153593e-03, -4.14092233e-03,  7.34398421e-03,  7.41617456e-02,\n",
       "         -3.15871313e-02, -1.46727292e-02, -3.27906176e-03, -2.06853319e-02,\n",
       "          7.49251097e-02, -3.05093694e-02, -6.69612689e-03,  2.99594868e-02,\n",
       "         -7.57624730e-02, -2.91730780e-02,  3.84360477e-02,  1.77332889e-02,\n",
       "         -4.73904684e-02,  3.64972092e-02, -6.32903799e-02, -8.54838118e-02,\n",
       "          9.78726801e-03,  1.39024537e-02,  1.25384321e-02,  6.42241538e-02,\n",
       "         -3.78791764e-02, -5.56905121e-02, -2.12848615e-02,  2.86963992e-02,\n",
       "         -1.76559966e-02, -2.62744445e-03, -1.01437137e-01,  5.38976230e-02,\n",
       "         -1.60363391e-02,  9.53218434e-03,  1.30092539e-02, -4.86032665e-03,\n",
       "         -1.41676208e-02, -3.32206190e-02, -2.77183298e-02,  4.00639288e-02,\n",
       "          3.26113589e-02, -9.02697351e-03, -2.96078492e-02,  4.64976728e-02,\n",
       "          3.79434563e-02,  2.38835206e-03,  4.38891985e-02,  2.05471069e-02,\n",
       "         -5.35133220e-02, -1.50673026e-02,  4.92437277e-03,  1.55098541e-02,\n",
       "         -8.65071826e-03,  1.78712811e-02,  1.62797980e-02, -2.22103503e-02,\n",
       "         -2.25636945e-03,  4.39309701e-02,  1.48166716e-02, -3.42003442e-03,\n",
       "         -4.16879132e-02, -2.51270328e-02,  2.18786206e-02,  2.09779590e-02,\n",
       "         -5.92787890e-03,  6.32864097e-03, -3.65753844e-02, -1.13470368e-02,\n",
       "          2.47492827e-02, -1.25486869e-02, -4.00417335e-02,  2.25307364e-02,\n",
       "          4.95968610e-02, -1.11234292e-01,  3.92496213e-02,  1.12783564e-02,\n",
       "          3.54862921e-02, -4.88836458e-03,  6.72468869e-03,  1.26904929e-02,\n",
       "         -1.13902311e-03,  3.12696695e-02, -7.36575713e-03,  6.74705133e-02,\n",
       "          3.41396742e-02, -2.53935973e-03,  5.09138592e-03,  5.37088253e-02,\n",
       "         -2.04596650e-02,  1.70246121e-02, -7.28985574e-03, -6.67238841e-03,\n",
       "          3.92971635e-02,  1.07376873e-02, -6.14036666e-03,  2.50856932e-02,\n",
       "         -4.87213721e-03,  4.18811515e-02, -4.33979519e-02,  3.46225016e-02,\n",
       "          3.04309726e-02,  9.12468135e-03, -1.52573874e-02, -7.19113648e-02,\n",
       "          1.03434233e-03,  2.22997088e-02,  1.67167373e-02, -2.15674508e-02,\n",
       "          1.09598157e-03,  2.57650134e-03,  9.06108040e-03, -7.33085768e-03,\n",
       "         -7.99708515e-02, -1.10887541e-02, -4.22189794e-02, -1.43076375e-01,\n",
       "         -1.70449801e-02,  2.87129078e-02, -1.09572606e-02,  7.28678256e-02,\n",
       "         -6.08173013e-03,  2.28368603e-02, -1.22217499e-02, -5.53308167e-02,\n",
       "         -4.18232232e-02,  2.79864501e-02,  3.70553732e-02,  4.20864038e-02,\n",
       "         -1.30461832e-03,  3.31296138e-02,  4.35413904e-02, -2.12643445e-02,\n",
       "          3.95503864e-02, -1.54662356e-02, -2.94060372e-02, -1.15270158e-02,\n",
       "         -2.52228584e-02,  1.08303744e-02, -2.98846583e-03,  7.98161700e-03,\n",
       "         -1.67518165e-02, -4.33151275e-02, -1.21331764e-02,  2.73113437e-02,\n",
       "          9.90141481e-02,  3.48057784e-02, -3.73242274e-02, -6.96228594e-02,\n",
       "          5.82346171e-02,  1.82005130e-02, -2.61475928e-02,  4.02679481e-02,\n",
       "         -2.83779260e-02,  6.24961127e-03,  2.54363604e-02, -2.05510855e-03,\n",
       "         -3.55111971e-03, -9.49381664e-03,  8.62743240e-03,  7.57107884e-02,\n",
       "          2.18649358e-02,  2.90558096e-02, -5.09485677e-02,  2.03032047e-02,\n",
       "         -2.41186190e-02,  1.45719638e-02, -1.69005916e-02,  7.35491747e-03,\n",
       "          2.13166848e-02,  1.63615514e-02, -2.25092620e-02, -3.70412283e-02,\n",
       "          1.17361180e-01,  2.79955752e-03,  1.04446029e-02,  2.14910414e-02,\n",
       "         -7.85141811e-02,  3.64906527e-03, -9.19320527e-03,  4.74105328e-02,\n",
       "          5.01179136e-02,  6.25796663e-03, -3.48766707e-03,  1.76993161e-02,\n",
       "          3.58095728e-02, -7.74255469e-02,  1.24339797e-01, -4.14491110e-02,\n",
       "          2.81292144e-02, -2.80361045e-02, -3.39075476e-02,  1.24622053e-02,\n",
       "          5.75383976e-02,  3.19806635e-02,  1.67341232e-02, -7.79256895e-02,\n",
       "         -2.76719909e-02, -9.56350565e-03,  2.92349737e-02,  1.44441295e-02,\n",
       "          3.88712399e-02, -1.26722334e-02,  6.73019327e-03, -2.17333436e-02,\n",
       "          8.96335486e-03,  1.79355927e-02,  4.70465496e-02,  3.31278481e-02,\n",
       "         -6.26011118e-02,  1.09709082e-02, -3.64415459e-02,  4.44507301e-02,\n",
       "         -2.01249830e-02, -2.51983590e-02, -1.47774455e-03, -4.06421125e-02,\n",
       "          4.03870642e-02, -2.23494787e-02,  7.73799568e-02, -2.28715409e-02,\n",
       "          4.80858386e-02, -6.03923239e-02,  3.46812047e-02, -1.95332468e-02,\n",
       "         -1.37329483e-02, -2.48200223e-02,  1.37402182e-02, -2.19952818e-02,\n",
       "          6.36257008e-02, -2.52694674e-02, -4.03735973e-02,  6.76652789e-02,\n",
       "          1.74101852e-02,  7.12445565e-03, -1.62099383e-03,  5.52840419e-02,\n",
       "         -1.02909617e-02,  2.69543696e-02,  2.65183114e-03,  5.01150591e-03,\n",
       "          8.06015264e-03, -3.16878818e-02,  3.67404595e-02, -3.91746126e-02,\n",
       "         -6.37466162e-02, -3.01771779e-02,  1.12212282e-02, -1.88754685e-02,\n",
       "          7.07345968e-03,  3.10434755e-02, -3.10965199e-02, -4.66797277e-02,\n",
       "         -3.12729515e-02, -1.57489348e-02,  3.61476578e-02,  8.75027254e-02,\n",
       "          1.19252643e-02, -4.98017408e-02, -6.79334551e-02, -3.77090238e-02,\n",
       "         -4.25418355e-02,  7.31173623e-03, -1.01935817e-02,  2.92323623e-02,\n",
       "         -5.16632684e-02, -2.26430874e-02,  2.74200551e-02, -1.93912052e-02,\n",
       "          7.80081656e-03,  2.93171816e-02, -6.32079393e-02, -4.90033887e-02,\n",
       "          2.16028281e-02, -4.09084484e-02,  4.85856868e-02,  3.41954306e-02,\n",
       "         -3.95996608e-02, -1.29843811e-02,  8.66218656e-03,  3.56349386e-02,\n",
       "          2.43584369e-03, -1.84558369e-02,  4.37903814e-02,  4.86435704e-02,\n",
       "          2.01114714e-02,  3.13220434e-02, -1.94682684e-02,  1.50703294e-02,\n",
       "         -5.31408638e-02,  2.74813287e-02, -5.32750413e-02,  1.31440330e-02,\n",
       "         -6.43874481e-02, -2.31638867e-02, -4.81808186e-02, -4.09059301e-02,\n",
       "         -3.16588953e-02,  3.36484872e-02, -8.67632916e-04,  4.80797142e-02,\n",
       "         -1.23305321e-02, -4.83053997e-02, -2.85690352e-02, -6.85196929e-03,\n",
       "         -1.08361035e-03, -2.75147464e-02, -2.58043539e-02,  2.06804760e-02,\n",
       "          1.83656684e-03,  8.56906027e-02,  1.93473476e-03,  1.37250042e-02,\n",
       "         -3.56950425e-02,  4.85933572e-03,  4.16071564e-02, -3.00085619e-02,\n",
       "         -2.43152529e-02, -4.28184196e-02,  3.91493132e-03,  3.94442752e-02,\n",
       "         -1.82613041e-02,  6.37335628e-02, -9.67004802e-03,  8.80389940e-03,\n",
       "          6.23381278e-03, -3.25813628e-04, -1.91536583e-02, -1.19950715e-02,\n",
       "          4.75890469e-03, -6.07421994e-03, -3.59027348e-02, -9.11817141e-03,\n",
       "          3.42966057e-02, -1.30294070e-01, -1.67199261e-02,  3.99311446e-02,\n",
       "          7.71480473e-03, -1.16753085e-02,  5.00733256e-02, -4.80219270e-33,\n",
       "          6.32702932e-02, -3.33115533e-02,  2.75391694e-02,  1.83639154e-02,\n",
       "          4.29421030e-02,  4.54866774e-02,  5.45671694e-02, -2.71603316e-02,\n",
       "          1.57573130e-02,  6.90949485e-02,  1.48437247e-02,  4.99003120e-02,\n",
       "          1.27446661e-02, -1.18701930e-04, -1.00930957e-02, -5.23866750e-02,\n",
       "          3.96725424e-02, -4.11320180e-02,  4.68134508e-02, -5.44452807e-04,\n",
       "         -3.53104956e-02, -6.74023405e-02, -1.86003260e-02, -3.43671106e-02,\n",
       "          2.37584021e-02, -5.55301234e-02,  8.79248753e-02, -8.69197305e-03,\n",
       "         -1.34354793e-02,  5.52330799e-02, -6.37278194e-03, -4.87564504e-03,\n",
       "         -3.09043080e-02,  1.45058911e-02, -3.15429680e-02, -9.71570611e-03,\n",
       "          3.85284121e-03,  1.77924186e-02, -1.32934963e-02,  5.65212592e-02,\n",
       "         -8.49827901e-02, -3.47923711e-02,  1.43561261e-02, -1.87995117e-02,\n",
       "          3.86824459e-02, -3.29651963e-03, -4.00185809e-02,  3.88686992e-02,\n",
       "          6.88268477e-03,  9.97340400e-03, -3.33503149e-02, -5.65494504e-03,\n",
       "         -4.76361401e-02, -3.20652574e-02,  3.08994986e-02,  1.86028574e-02,\n",
       "          1.55896861e-02, -1.66874900e-02,  1.72798689e-02, -3.81545201e-02,\n",
       "         -6.16557971e-02,  2.07388075e-03, -3.17408815e-02,  7.82065559e-03,\n",
       "         -5.01524583e-02,  1.39675355e-02, -5.68551430e-03, -2.91544423e-02,\n",
       "         -8.90719984e-03, -7.56536052e-02, -5.68948016e-02, -1.23251732e-02,\n",
       "         -4.86408882e-02, -4.31075841e-02, -3.63958813e-02, -2.96664401e-03,\n",
       "         -5.64226620e-02, -3.29859145e-02, -5.63233197e-02, -6.27885610e-02,\n",
       "         -6.08049110e-02,  2.03157570e-02, -1.52272144e-02, -1.02633154e-02,\n",
       "         -3.91286649e-02, -4.83940495e-03, -2.76780408e-02, -5.32865804e-03,\n",
       "         -1.88179165e-02, -4.32229079e-02,  4.17283624e-02, -4.23683785e-02,\n",
       "          2.24808455e-02,  1.79625563e-02, -6.15952387e-02, -1.14962179e-02,\n",
       "         -4.77405870e-03, -2.27782242e-02,  8.13703053e-03,  6.39512837e-02,\n",
       "         -4.70389798e-02,  5.52165806e-02,  4.48915102e-02, -4.89799604e-02,\n",
       "         -6.20869026e-02,  8.46898369e-03, -3.97021929e-03, -3.12597714e-02,\n",
       "          2.24510785e-02, -5.45699820e-02,  1.57201085e-02, -2.38144640e-02,\n",
       "         -4.45558177e-03, -2.80783493e-02,  3.13311629e-02,  5.27173392e-02,\n",
       "          5.40530495e-03,  3.46648321e-02,  5.32277673e-02, -3.42887430e-03,\n",
       "         -5.05346470e-02, -2.89987195e-02,  5.34199318e-03,  2.95221899e-02,\n",
       "         -1.23135615e-02, -2.00752784e-02, -1.64104607e-02,  8.60445574e-03,\n",
       "         -2.91470792e-02, -1.36720156e-02,  1.24087175e-02,  1.27688078e-02,\n",
       "          2.54862357e-07, -2.82724970e-04,  3.68257402e-03,  2.57094465e-02,\n",
       "         -8.31017867e-02,  1.21558281e-02,  4.50733723e-03,  4.71075112e-03,\n",
       "         -2.17504427e-02,  4.51866835e-02, -2.17507221e-02,  1.96955744e-02,\n",
       "         -8.81256443e-03, -4.13159318e-02, -3.49467881e-02, -2.10753120e-02,\n",
       "          5.39925359e-02,  4.64582108e-02, -5.53859621e-02,  2.00199708e-02,\n",
       "         -3.14941518e-02, -5.09104244e-02, -1.28574362e-02, -1.11577203e-02,\n",
       "         -5.02570346e-02, -1.74375232e-02,  4.18088995e-02,  7.28171039e-03,\n",
       "          5.06765544e-02,  2.14824937e-02,  9.01866984e-03,  9.36108548e-03,\n",
       "         -4.12934786e-03,  7.05188438e-02,  3.11454833e-02,  1.94797777e-02,\n",
       "         -6.88123796e-03,  3.07265185e-02,  8.91809538e-03, -3.82439382e-02,\n",
       "         -4.87585813e-02, -5.48210740e-02,  2.27582715e-02, -1.21503705e-02,\n",
       "          4.44756858e-02, -5.44480495e-02,  1.57394409e-02, -2.43706740e-02,\n",
       "         -3.37071419e-02,  5.22846915e-02, -2.76767425e-02,  2.70657465e-02,\n",
       "          5.14617898e-02, -1.21621706e-03,  4.80899997e-02, -1.91243794e-02,\n",
       "         -7.91603979e-03,  3.90577875e-02,  2.53714286e-02,  5.18534593e-02,\n",
       "         -1.93108385e-03,  3.31370458e-02,  4.31681285e-03, -2.69262716e-02,\n",
       "          1.11777363e-02,  6.45215958e-02,  3.10896039e-02,  3.54237785e-03,\n",
       "          1.56150431e-34, -2.66029499e-02, -3.10240351e-02,  2.74676085e-02,\n",
       "          1.19261090e-02,  8.06772895e-03,  5.98871848e-03, -1.75738577e-02,\n",
       "          1.55059965e-02,  2.05330625e-02, -3.29691693e-02,  5.42412000e-03]),\n",
       "  'score': tensor(0.0180)},\n",
       " {'page_number': 517,\n",
       "  'sentence_chunk': 'Image by Allison Calabrese / CC BY 4.0 To combat this issue the Island Food Community of Pohnpei has been instrumental in promoting the citizens of Pohnpei to increase local karat banana consumption. The karat banana is rich in beta- carotene (a source of vitamin A) and increasing consumption among the locals will decrease the prevalence of vitamin A deficiencies in Pohnpei.\\xa0For further information on this issue visit the Island Food Community of Pohnpeis website at http://www.islandfood.org/ and watch the video at https://www.youtube.com/ watch?v=DGVxnefqbTQ. Vitamins are organic compounds that are traditionally assigned to two groups fat-soluble (hydrophobic) or water-soluble (hydrophilic). This classification determines where they act in the body. Water-soluble vitamins act in the cytosol of cells or in extracellular fluids such as blood; fat-soluble vitamins are largely responsible for protecting cell membranes from free radical damage. The body can synthesize some vitamins, but others must be obtained from the diet. Figure 9.1 The Vitamins https://www.ncbi.nlm.nih.gov/pubmed/14984164. Accessed October 15, 2017. Introduction | 517',\n",
       "  'chunk_char_count': 1153,\n",
       "  'chunk_word_count': 163,\n",
       "  'chunk_token_count': 288.25,\n",
       "  'embedding': array([ 4.46678624e-02, -1.89871229e-02, -8.93632974e-03, -3.04892194e-03,\n",
       "          7.08175972e-02, -1.08474670e-02, -2.90521029e-02,  4.55462970e-02,\n",
       "          5.89639805e-02, -1.24899093e-02,  7.36082718e-03,  2.80761644e-02,\n",
       "          2.00692154e-02,  1.66364834e-02, -9.20568593e-03, -3.08989864e-02,\n",
       "          6.05768478e-03, -2.01596860e-02,  2.55887434e-02,  2.66060419e-02,\n",
       "         -9.07635083e-04,  2.18657572e-02,  1.38190622e-02,  3.26340757e-02,\n",
       "         -2.02508681e-02, -3.76143418e-02,  1.20297344e-02, -1.92390066e-02,\n",
       "         -1.87351257e-02, -7.44625330e-02, -1.17570069e-02, -1.70818018e-03,\n",
       "         -5.17598446e-03, -2.91079432e-02,  2.09312702e-06, -1.45163275e-02,\n",
       "         -3.53306308e-02, -1.64663885e-02, -4.57697101e-02,  9.19443592e-02,\n",
       "          3.16300653e-02, -6.39231727e-02, -7.75419315e-03,  3.37524456e-03,\n",
       "          3.47571447e-02,  4.38333079e-02,  3.31713855e-02, -6.67050714e-03,\n",
       "          2.76836194e-03,  7.96565320e-03,  2.51205917e-03,  8.29201788e-02,\n",
       "          5.76215498e-02,  5.16789593e-02,  5.00858240e-02, -3.04177552e-02,\n",
       "          2.68205814e-02,  8.56553093e-02,  2.91803712e-03,  4.21554111e-02,\n",
       "          4.62509273e-03, -9.80162527e-04, -5.18740807e-03,  2.05699876e-02,\n",
       "         -7.55030056e-03,  4.43179496e-02, -4.38624620e-02, -9.70692113e-02,\n",
       "         -1.58853643e-03,  3.11241318e-02,  3.54770542e-04, -1.25056142e-02,\n",
       "          2.71074511e-02,  3.26052010e-02, -9.41776577e-03,  3.76640707e-02,\n",
       "          2.20338050e-02,  1.95356552e-02, -2.31260341e-02, -1.44137070e-02,\n",
       "          4.72475365e-02, -2.58053490e-03,  8.89514980e-04, -4.51883022e-03,\n",
       "          2.87213139e-02,  1.20417541e-02,  8.05459265e-03, -1.66598370e-03,\n",
       "         -4.95130271e-02, -3.25950421e-02, -7.61252567e-02, -3.58983763e-02,\n",
       "          2.94523947e-02, -3.26176733e-03,  5.14246374e-02, -2.45631319e-02,\n",
       "          1.04587004e-02, -1.08807290e-03,  8.14712346e-02, -7.74227753e-02,\n",
       "          3.44199762e-02,  1.24747148e-02, -1.53720051e-01,  2.47757025e-02,\n",
       "          2.08299439e-02,  1.98287778e-02, -1.15679875e-02,  6.69242740e-02,\n",
       "         -1.34191206e-02,  1.58537310e-02, -2.86796745e-02, -3.08992323e-02,\n",
       "         -2.00635567e-02,  1.35486387e-02, -2.67909337e-02, -1.10737020e-02,\n",
       "         -3.19278911e-02,  4.54806664e-04, -8.76301154e-03, -7.82240853e-02,\n",
       "          3.49144917e-04,  2.73453724e-02,  1.27714984e-02, -6.62637735e-03,\n",
       "         -4.92595322e-02,  1.02384791e-01, -7.43862428e-03, -4.54820134e-02,\n",
       "         -3.56881171e-02, -6.05340526e-02,  2.56748777e-02, -2.19128355e-02,\n",
       "         -1.96569245e-02, -9.45637003e-03, -9.44852270e-03, -1.07919360e-02,\n",
       "          4.48088534e-02, -3.83785972e-03, -2.14478839e-03, -4.48067822e-02,\n",
       "         -6.18149005e-02, -7.01821363e-03, -4.85115908e-02,  2.50113998e-02,\n",
       "          5.09754829e-02, -4.36196737e-02,  3.76653410e-02, -1.75626215e-03,\n",
       "          1.16275856e-02,  6.58795461e-02,  8.79211351e-03,  1.07287066e-02,\n",
       "          6.42138645e-02,  2.53070649e-02,  7.81442374e-02,  2.45919172e-03,\n",
       "         -3.04371817e-03, -6.36892617e-02,  2.37529632e-02, -2.67419443e-02,\n",
       "          2.56061088e-02,  7.20756361e-04,  3.38118486e-02, -6.34736270e-02,\n",
       "         -9.32542782e-04,  2.59372871e-03,  6.45909160e-02,  1.72110759e-02,\n",
       "         -3.65298800e-02,  4.38614329e-03, -6.05746778e-03, -1.07772788e-02,\n",
       "          7.41418544e-03,  1.29922247e-02,  1.45418961e-02, -2.63812230e-03,\n",
       "          8.84296279e-03,  3.13257687e-02,  9.99276526e-03,  1.00939358e-02,\n",
       "          4.97138053e-02, -3.60559449e-02,  1.84003189e-02,  2.21053176e-02,\n",
       "         -6.29895255e-02,  2.30705999e-02, -2.70963162e-02,  1.78898107e-02,\n",
       "         -2.03592074e-03,  1.92197971e-02,  4.57249619e-02,  2.35455558e-02,\n",
       "          1.79772545e-02, -6.47091195e-02, -1.29927071e-02, -1.05533609e-02,\n",
       "         -2.14057192e-02,  3.04009067e-03,  4.64376025e-02, -4.12841141e-02,\n",
       "          2.75521316e-02,  5.23340702e-02,  1.54009452e-02, -1.03219617e-02,\n",
       "         -7.38471821e-02,  3.20128649e-02,  4.13799621e-02,  1.46343391e-02,\n",
       "          4.27861623e-02, -2.19120942e-02, -6.55136630e-02, -2.36487780e-02,\n",
       "         -5.43752313e-03, -1.96459703e-02,  4.38847654e-02, -5.05366065e-02,\n",
       "         -6.39678445e-03,  2.15818528e-02, -2.03544740e-02,  8.40499103e-02,\n",
       "          2.19695400e-02, -2.67222952e-02, -5.71241137e-03, -3.20198722e-02,\n",
       "          4.13310602e-02, -1.42615093e-02, -9.23503656e-03,  1.85564794e-02,\n",
       "         -3.48404311e-02,  7.27484701e-03,  3.70537788e-02,  9.92288440e-03,\n",
       "         -2.58308053e-02,  2.78867558e-02, -3.35734338e-02, -3.16314697e-02,\n",
       "          5.26379459e-02, -1.11492872e-02, -4.83788224e-03,  4.86406125e-02,\n",
       "         -1.55141586e-02, -2.62882411e-02, -2.37046974e-03,  1.16570452e-02,\n",
       "         -4.25902680e-02,  6.37626052e-02, -1.23984613e-01,  1.28094172e-02,\n",
       "         -3.91530879e-02,  3.17528285e-02,  1.42831057e-02,  1.88423730e-02,\n",
       "         -2.86414102e-02,  1.12779588e-02, -6.23473153e-02,  5.12452722e-02,\n",
       "         -3.46176773e-02, -5.80095733e-03, -5.73274866e-02,  5.12157716e-02,\n",
       "          2.30982769e-02, -7.13417493e-03,  2.14179009e-02, -2.71378830e-03,\n",
       "         -1.01168256e-03, -5.13732322e-02,  2.30656005e-02, -4.37515182e-03,\n",
       "         -3.94776240e-02,  1.15872221e-02, -1.11221345e-02, -1.77016184e-02,\n",
       "          5.80270682e-03,  5.36826476e-02, -5.97755751e-03, -1.42511325e-02,\n",
       "         -4.91239391e-02,  2.53952504e-03,  4.03777510e-02, -2.45805141e-02,\n",
       "         -1.94501020e-02,  1.72607806e-02, -1.03356140e-02,  3.65210325e-02,\n",
       "          3.59128006e-02, -3.22363176e-03,  8.49985343e-04,  3.62960063e-02,\n",
       "          7.91861415e-02, -1.02177598e-01,  2.56561507e-02, -6.99222088e-03,\n",
       "          2.44974103e-02,  2.29831133e-02,  1.93239283e-02,  1.95980892e-02,\n",
       "         -1.38534382e-02,  3.63777317e-02, -1.37299849e-02, -9.28991567e-03,\n",
       "          7.16539146e-03, -2.92038545e-02, -4.88425954e-04,  4.88994569e-02,\n",
       "         -4.42714058e-02,  3.03634536e-02,  4.12775576e-02, -3.57900374e-02,\n",
       "          1.82570666e-02, -1.04570622e-02,  3.15404721e-02,  4.52927649e-02,\n",
       "         -3.79341058e-02,  2.35225931e-02, -3.13029848e-02, -9.15901829e-03,\n",
       "          2.16218345e-02, -8.86615552e-03,  2.69670896e-02, -3.66412140e-02,\n",
       "         -6.85055880e-03,  1.15472339e-02,  3.64201237e-03, -4.00047339e-02,\n",
       "          5.25261927e-03, -5.99327944e-02,  2.52505243e-02,  2.84092352e-02,\n",
       "         -9.58522186e-02, -8.28785077e-03, -4.93192375e-02, -6.41350299e-02,\n",
       "          4.10709493e-02,  3.73088755e-02,  4.05375920e-02,  3.93457152e-03,\n",
       "          1.37180146e-02, -1.08707128e-02, -5.06694056e-02, -5.14769740e-02,\n",
       "         -4.42940854e-02, -1.30451368e-02,  1.92041341e-02,  4.65998650e-02,\n",
       "          2.20206063e-02, -3.06291636e-02,  3.01931682e-03,  1.60906017e-02,\n",
       "          8.56624395e-02,  2.45023170e-03, -3.98410112e-03, -4.52764742e-02,\n",
       "         -1.04736974e-02,  1.84249096e-02,  6.16391702e-03,  1.91852245e-02,\n",
       "         -3.75590241e-03, -3.25323306e-02, -4.09082212e-02,  1.31183313e-02,\n",
       "          7.25779906e-02,  1.64553449e-02, -2.47530825e-02, -6.32803589e-02,\n",
       "          1.05478846e-01,  3.36182974e-02, -4.34674099e-02,  2.38746461e-02,\n",
       "         -2.18576863e-02, -2.59007867e-02,  4.80019767e-03,  2.85271928e-02,\n",
       "         -2.93965861e-02, -7.26043386e-03,  6.18746364e-03,  1.98985264e-02,\n",
       "          4.26065177e-02, -3.52774572e-04, -7.82328919e-02, -3.64972241e-02,\n",
       "          1.51658412e-02, -8.54292046e-03,  6.48094490e-02, -1.15287537e-02,\n",
       "         -3.80223766e-02, -3.16300392e-02, -1.78904682e-02, -6.41369373e-02,\n",
       "          8.41971040e-02, -5.79540469e-02,  2.71748360e-02,  1.53003242e-02,\n",
       "         -1.15815336e-02,  5.87435029e-02, -2.65829172e-02,  1.26456050e-02,\n",
       "          4.82792035e-02, -1.70230982e-03,  1.95426457e-02,  5.44728190e-02,\n",
       "          3.58090876e-03, -9.19941291e-02,  8.56691226e-02,  3.31346095e-02,\n",
       "          5.37556261e-02, -6.26716241e-02, -9.58125666e-03,  1.15581853e-02,\n",
       "          6.19156212e-02,  1.99619383e-02,  8.39674380e-03, -6.44365996e-02,\n",
       "         -1.52837625e-02, -1.83481686e-02,  6.59029782e-02,  1.75330974e-02,\n",
       "          1.90497451e-02,  1.55624216e-02,  4.39059883e-02, -5.54475114e-02,\n",
       "         -3.62516940e-02,  4.00033332e-02,  9.28539899e-04,  3.21754143e-02,\n",
       "         -9.53818730e-04, -1.78214465e-03, -1.26600154e-02,  1.23592643e-02,\n",
       "         -8.43952671e-02,  1.20877540e-02,  2.97506549e-03, -2.72704512e-02,\n",
       "          9.00357962e-03, -8.96021631e-03, -1.68416742e-02,  3.04004503e-03,\n",
       "          7.08508790e-02, -4.84099723e-02, -1.97683219e-02, -5.58845885e-02,\n",
       "         -1.94533318e-02,  3.24746892e-02,  1.44586116e-02, -9.63669550e-03,\n",
       "         -1.38515551e-02, -3.34596299e-02, -1.15754958e-02,  1.03667304e-01,\n",
       "          3.73963602e-02,  3.48841995e-02,  1.00179883e-02,  5.95690496e-02,\n",
       "          2.62658042e-03,  3.55178230e-02, -7.43826060e-03, -1.14615289e-02,\n",
       "         -6.32323027e-02,  5.02457842e-03,  1.75850485e-02, -1.54333916e-02,\n",
       "         -1.15662646e-02, -4.43876628e-03,  3.31225758e-03, -5.49692370e-04,\n",
       "         -1.54980747e-02,  9.45852231e-03,  9.39060654e-03, -4.07158136e-02,\n",
       "         -3.83070596e-02, -3.01235169e-02,  3.61791961e-02,  6.46888018e-02,\n",
       "          2.71651316e-02, -4.02398147e-02, -8.42327718e-03, -4.35646586e-02,\n",
       "         -3.88106927e-02, -1.39919259e-02, -2.91816570e-04,  5.77274486e-02,\n",
       "         -2.92526353e-02,  4.29474562e-02,  2.99060135e-03, -2.29720976e-02,\n",
       "          1.11507336e-02, -1.42352255e-02, -8.04826170e-02, -1.27443923e-02,\n",
       "          1.12236068e-02, -1.52849965e-02,  4.81869169e-02,  4.18023616e-02,\n",
       "         -2.75825467e-02, -1.51401118e-03,  1.64054651e-02,  6.15698993e-02,\n",
       "          1.20605882e-02, -8.49187560e-03,  7.37664998e-02,  1.11579709e-02,\n",
       "          3.25079300e-02,  6.53113564e-03,  1.85813922e-02,  3.90368956e-03,\n",
       "         -4.70702760e-02,  1.51184052e-02, -4.14796919e-02,  4.47225273e-02,\n",
       "         -3.87481824e-02, -3.76683585e-02, -4.10805717e-02, -3.13210599e-02,\n",
       "         -1.79117620e-02,  4.56213579e-02, -3.95952985e-02,  4.46417592e-02,\n",
       "          5.23470808e-03, -2.90597938e-02,  1.07639153e-02,  2.80595617e-03,\n",
       "         -1.50665613e-02, -4.69864123e-02, -5.00132656e-03,  3.28439809e-02,\n",
       "         -3.40328440e-02,  4.39232737e-02,  3.24572213e-02, -2.52673682e-02,\n",
       "         -6.34769723e-02,  4.07231599e-02,  6.81066588e-02, -4.46269289e-02,\n",
       "         -1.08542219e-02, -1.37903420e-02, -2.76961662e-02,  8.36943984e-02,\n",
       "         -5.70731312e-02,  6.58064708e-02, -1.78613774e-02,  2.89713163e-02,\n",
       "          5.14385430e-03, -1.68349072e-02,  5.43755554e-02,  2.48908233e-02,\n",
       "          3.38916928e-02,  3.08270799e-03, -7.91548043e-02, -6.09237440e-02,\n",
       "          1.64932590e-02, -1.05643354e-01,  7.25491298e-03,  1.19931167e-02,\n",
       "         -5.40497042e-02, -8.76278244e-03,  6.72085956e-02, -5.53164168e-33,\n",
       "          4.67093401e-02, -3.67701389e-02,  1.96776073e-02,  6.44544736e-02,\n",
       "          2.97140479e-02,  3.41573432e-02,  1.62534770e-02, -3.79481204e-02,\n",
       "         -2.61512678e-02,  3.26986760e-02,  2.94237081e-02,  1.36098880e-02,\n",
       "          1.71406530e-02, -3.94225419e-02,  9.32683889e-03,  6.98659045e-04,\n",
       "          2.04683207e-02, -3.45550142e-02,  1.51649015e-02, -2.72132130e-03,\n",
       "         -4.34646606e-02, -2.42461432e-02, -1.15149403e-02,  1.95299666e-02,\n",
       "          4.13006730e-02, -2.06045769e-02,  2.94251349e-02, -1.61718018e-02,\n",
       "         -5.28719611e-02,  6.45473972e-02, -6.90803677e-03,  4.38356288e-02,\n",
       "         -1.23748528e-02,  6.92289770e-02, -1.39029846e-02, -2.72911843e-02,\n",
       "          4.75670863e-03,  1.12183327e-02, -8.29570368e-03,  3.59626450e-02,\n",
       "         -9.98825282e-02, -4.46744300e-02, -1.02465386e-02,  2.07592752e-02,\n",
       "          1.69090405e-02, -1.35812089e-02, -4.68030758e-02, -4.87875715e-02,\n",
       "         -2.36825515e-02,  1.04888268e-02, -7.41017284e-03, -4.12501488e-03,\n",
       "         -9.64755937e-03, -1.49476361e-02,  3.28275524e-02,  7.35121146e-02,\n",
       "         -5.36132231e-03, -2.55182441e-02,  1.47259319e-02, -1.86181441e-02,\n",
       "         -1.75164696e-02,  1.17804026e-02,  1.00975868e-03,  3.78590077e-02,\n",
       "         -3.21368836e-02, -2.53862813e-02,  1.90485269e-02, -1.02445995e-02,\n",
       "         -3.29605751e-02,  2.41573923e-03, -2.81553082e-02,  1.61939359e-04,\n",
       "         -1.93389598e-02,  1.69022977e-02, -2.93930359e-02, -2.79609095e-02,\n",
       "         -3.55803743e-02,  1.13849016e-02, -4.19914238e-02, -4.86901365e-02,\n",
       "         -6.24765120e-02,  4.97743953e-03,  2.42180489e-02, -1.14475200e-02,\n",
       "         -3.12303156e-02, -1.04101058e-02, -4.58226241e-02, -8.34843982e-03,\n",
       "          2.11490504e-02, -4.58170250e-02,  2.89636496e-02,  1.19314790e-02,\n",
       "          2.59242132e-02,  7.44988327e-04, -1.22561511e-02,  2.07864810e-02,\n",
       "         -5.47205023e-02, -7.30699720e-03,  1.28218457e-02, -1.57499164e-02,\n",
       "         -8.74071419e-02,  5.12192957e-02, -5.10315876e-03, -2.15736888e-02,\n",
       "         -5.77328801e-02, -4.26060259e-02, -4.23713261e-03, -6.12713397e-04,\n",
       "         -2.18155328e-03, -1.67999063e-02,  6.40619127e-03,  4.72626649e-03,\n",
       "          1.83971636e-02,  1.64660513e-02, -1.78252365e-02,  4.50420938e-02,\n",
       "          3.42555940e-02,  3.88389863e-02,  2.80193239e-02, -4.50382084e-02,\n",
       "         -2.49456987e-02,  1.80075392e-02,  3.87677774e-02, -3.61716226e-02,\n",
       "         -4.32601348e-02, -4.44550104e-02,  5.76360337e-02,  2.10294127e-02,\n",
       "         -5.18536568e-02, -1.60689689e-02,  2.30085831e-02,  2.81382538e-02,\n",
       "          2.92972629e-07, -2.70106420e-02,  2.28787847e-02,  1.93165168e-02,\n",
       "         -1.22074306e-01, -2.25654454e-03, -1.45911854e-02,  2.55356058e-02,\n",
       "         -2.42029596e-02,  2.29593124e-02, -4.54940386e-02,  6.60502762e-02,\n",
       "         -3.59499380e-02, -1.27078379e-02, -2.27943319e-03, -2.70851180e-02,\n",
       "          8.76985341e-02, -1.93082392e-02, -5.95864579e-02, -8.72310996e-03,\n",
       "         -4.13202345e-02, -2.33790232e-03,  1.54188359e-02, -2.42240131e-02,\n",
       "         -3.49319838e-02, -6.95318123e-03, -2.19892263e-02,  3.00867818e-02,\n",
       "          5.01249656e-02, -8.91727861e-03,  2.08520070e-02,  1.55396406e-02,\n",
       "         -1.49125401e-02,  2.68825106e-02,  1.36903930e-03, -1.39230350e-02,\n",
       "         -4.13594358e-02,  1.10574532e-02,  6.62831310e-03, -5.42735010e-02,\n",
       "          2.28212811e-02, -3.07104513e-02,  2.78080553e-02, -1.43255526e-02,\n",
       "          1.38155371e-02, -8.36973116e-02, -3.01951240e-03, -2.10165232e-02,\n",
       "         -3.48413214e-02,  7.68128410e-02, -2.19580587e-02,  2.99015976e-02,\n",
       "          1.77923311e-02, -6.57058880e-03,  2.08156910e-02, -5.17159924e-02,\n",
       "          1.16136139e-02,  3.55756767e-02,  2.96614598e-02,  3.82006727e-02,\n",
       "         -3.14986072e-02, -1.08200181e-02,  1.94905885e-02, -2.91209277e-02,\n",
       "          2.42418237e-02,  2.54588970e-03,  5.64991534e-02,  2.43023653e-02,\n",
       "          2.50825791e-34, -2.46101897e-02, -6.90181926e-02,  2.43901610e-02,\n",
       "         -2.84066312e-02,  2.03506239e-02, -1.48235413e-03,  8.44571833e-03,\n",
       "          3.98974586e-03, -2.87005361e-02, -4.76205051e-02, -2.28204299e-02]),\n",
       "  'score': tensor(0.0173)},\n",
       " {'page_number': 586,\n",
       "  'sentence_chunk': 'Accessed October 28, 2017. Choline Choline is a water-soluble substance that is not classified as a vitamin because it can be synthesized by the body. However, the synthesis of choline is limited and therefore it is recognized as an essential nutrient.\\xa0Choline is need to perform functions such as the synthesis of neurotransmitter acetylcholine, the synthesis of phospholipids used to make cell membranes, lipid transport, and also homocysteine metabolism.\\xa0A deficiency in choline may lead to interfered brain development in the fetus during pregnancy, and in adults cause fatty liver and muscle damage. 586 | Water-Soluble Vitamins',\n",
       "  'chunk_char_count': 633,\n",
       "  'chunk_word_count': 95,\n",
       "  'chunk_token_count': 158.25,\n",
       "  'embedding': array([ 1.61560941e-02,  2.00210232e-03,  1.79939773e-02, -3.53823081e-02,\n",
       "          3.73032205e-02,  2.38026008e-02, -3.79954875e-02,  4.18887176e-02,\n",
       "          9.67545733e-02,  3.30123468e-03,  1.64091922e-02,  8.61005187e-02,\n",
       "         -8.09513871e-03, -4.45872499e-03, -1.17467698e-02,  9.37459618e-03,\n",
       "         -8.63038003e-03,  2.18166169e-02, -6.43374547e-02,  7.40635814e-03,\n",
       "          5.60340763e-04, -9.93948081e-04,  3.27559486e-02, -3.46143842e-02,\n",
       "         -1.21910674e-02,  4.58961586e-03,  2.77188532e-02, -2.79241391e-02,\n",
       "         -2.30424237e-02, -4.26485911e-02, -8.71340279e-03,  2.83882078e-02,\n",
       "          8.62395111e-03, -4.69834358e-02,  2.16426270e-06, -9.46411118e-03,\n",
       "         -7.21738338e-02,  5.27085178e-03, -5.02871163e-02, -1.75258406e-02,\n",
       "          3.96708846e-02, -6.56725690e-02, -2.77315322e-02, -1.36324689e-02,\n",
       "          3.23037058e-02,  9.13362503e-02,  2.01777034e-02,  7.80647108e-03,\n",
       "         -4.03495599e-03,  6.42345985e-03,  1.62848849e-02,  9.71192792e-02,\n",
       "          5.12021258e-02,  6.48781806e-02,  4.13781255e-02, -1.44489966e-02,\n",
       "          1.74633991e-02,  1.46677941e-01, -6.51858049e-03,  6.11414621e-03,\n",
       "         -1.40987067e-02,  3.04516293e-02,  2.47195289e-02,  4.18616878e-03,\n",
       "          5.31296432e-02,  6.49714246e-02, -1.13981783e-01, -7.15117827e-02,\n",
       "          8.33938178e-03,  6.19104840e-02, -4.34540473e-02, -8.73349421e-03,\n",
       "          5.53644784e-02,  2.30480824e-02, -2.65392661e-02, -3.23632397e-02,\n",
       "          1.29025709e-02, -8.09675921e-03,  1.88004524e-02,  1.64233742e-03,\n",
       "          1.94268171e-02, -8.47921334e-03,  1.93796828e-02, -2.06625219e-02,\n",
       "          9.06983986e-02,  2.21631286e-04,  2.96250312e-03,  5.32425288e-03,\n",
       "         -2.47488283e-02, -2.16886625e-02, -1.02620311e-01, -2.41230875e-02,\n",
       "          3.82858664e-02,  2.54457025e-03,  1.04781520e-02, -3.46627422e-02,\n",
       "         -5.41211618e-03,  1.65533032e-02,  6.66131005e-02, -4.07958031e-02,\n",
       "          8.46275538e-02, -8.40406399e-03, -9.60155427e-02,  1.86499413e-02,\n",
       "          6.99519413e-03,  3.33244936e-03, -2.96559441e-03,  2.78268885e-02,\n",
       "         -4.52172123e-02,  2.88021397e-02, -3.91955348e-03, -4.91055800e-03,\n",
       "         -3.72724980e-02,  5.89703172e-02,  7.24745973e-04, -2.26658806e-02,\n",
       "         -9.02832765e-03,  2.10737120e-02, -2.35713795e-02, -3.01315468e-02,\n",
       "          1.03519605e-02,  4.69177067e-02,  2.62001865e-02,  2.15395559e-02,\n",
       "         -4.67809327e-02,  5.10912538e-02,  3.99953201e-02, -4.50684987e-02,\n",
       "          4.22024401e-03, -3.69924679e-02,  3.55084799e-02, -1.30759319e-02,\n",
       "         -1.03261974e-02,  4.09472771e-02,  3.88348848e-03, -1.18851685e-03,\n",
       "          5.35846464e-02, -1.90825108e-02, -6.08951924e-03, -5.70345782e-02,\n",
       "         -3.50415073e-02, -2.76125669e-02, -2.01285798e-02,  2.20199861e-02,\n",
       "          1.51964603e-02, -3.61604244e-02,  2.11355649e-02, -3.51739526e-02,\n",
       "          4.02368698e-03,  4.34127562e-02,  2.61645764e-02,  2.92338361e-03,\n",
       "          1.70810632e-02, -1.25876470e-02,  5.13943844e-02, -9.79291182e-03,\n",
       "          2.44606510e-02, -6.21663481e-02,  4.89874557e-03, -2.78729945e-03,\n",
       "          4.46205176e-02, -2.96475575e-03, -1.17531437e-02, -9.40914005e-02,\n",
       "          8.83547310e-03, -1.00311274e-02,  6.22265087e-03,  2.40055658e-02,\n",
       "         -6.10191897e-02, -5.40948422e-05,  3.45874578e-03, -3.20144780e-02,\n",
       "          2.34775105e-03, -2.37589665e-02,  1.70839708e-02,  5.10147512e-02,\n",
       "          2.36570947e-02,  3.32546979e-02, -7.77327456e-03,  1.13240955e-03,\n",
       "          1.46116214e-02,  6.62813615e-03, -7.81585090e-03,  2.99500003e-02,\n",
       "         -2.41360106e-02, -4.08478081e-03,  1.10700391e-02,  2.45655049e-02,\n",
       "          3.44958864e-02, -2.50334442e-02,  6.11116737e-02, -1.37140485e-03,\n",
       "          3.71447057e-02, -1.91469174e-02, -5.77799324e-03, -2.23936676e-03,\n",
       "         -3.56812216e-02, -5.71608804e-02, -3.07965488e-03, -3.20376530e-02,\n",
       "          1.97151825e-02,  8.54272544e-02,  2.93668583e-02,  2.09156238e-02,\n",
       "         -7.16418698e-02,  4.54091690e-02,  1.15434211e-02,  2.79617626e-02,\n",
       "          5.16926721e-02, -2.27586478e-02, -9.32656415e-03, -4.84746508e-02,\n",
       "          4.23006341e-03, -2.84834374e-02,  5.22004925e-02, -9.99258598e-04,\n",
       "          1.47718415e-02, -2.33555934e-03, -4.97891530e-02,  2.33397875e-02,\n",
       "          1.84606202e-02,  1.29346466e-02, -5.81759326e-02, -3.69119644e-02,\n",
       "          6.20482676e-02, -5.14028780e-02,  3.61869950e-03,  3.37341540e-02,\n",
       "         -2.38401759e-02, -1.70311946e-02,  3.57199088e-02,  1.61854252e-02,\n",
       "          1.82762556e-02,  6.21103756e-02, -7.24977925e-02, -7.77481571e-02,\n",
       "         -2.66077854e-02, -1.85571462e-02, -2.16445532e-02,  8.35957080e-02,\n",
       "         -5.29134087e-03, -4.22354564e-02, -1.14430031e-02,  2.27946267e-02,\n",
       "         -3.90221784e-03,  6.27547652e-02, -8.19204375e-02, -2.90515576e-03,\n",
       "          3.77405458e-03,  2.92327292e-02,  3.43565131e-03, -8.84530507e-03,\n",
       "         -9.71392263e-03,  9.96039901e-03, -1.66566968e-02,  3.04095484e-02,\n",
       "          5.05629182e-03,  2.92129791e-03, -4.64840084e-02,  3.92785519e-02,\n",
       "          3.19398120e-02,  1.97412129e-02,  5.83467297e-02,  1.29421549e-02,\n",
       "         -1.05326222e-02, -4.58899997e-02,  1.75465159e-02,  2.95196306e-02,\n",
       "          5.42417914e-03,  6.40778895e-03,  1.64312646e-02, -1.22250523e-02,\n",
       "          2.91033778e-02,  4.31756563e-02, -1.03723481e-02, -3.56719858e-04,\n",
       "         -5.24384230e-02, -3.76094319e-03,  6.72779232e-03,  5.17399702e-03,\n",
       "         -2.43226867e-02,  2.90037263e-02,  1.81943420e-02,  2.33464744e-02,\n",
       "          2.18021739e-02, -1.02995289e-02, -4.59805280e-02,  1.13629133e-01,\n",
       "          6.61263764e-02, -2.59595998e-02,  2.54793409e-02,  5.96052371e-02,\n",
       "          1.00328960e-02,  3.22114974e-02,  4.48865211e-03,  4.52464446e-02,\n",
       "         -7.66734872e-03,  7.23863617e-02, -2.32276320e-02, -1.50408633e-02,\n",
       "         -1.47041595e-02, -1.70204155e-02,  1.76324323e-03,  1.87327750e-02,\n",
       "         -3.86268608e-02,  7.09281042e-02,  3.22738737e-02,  1.43569172e-03,\n",
       "          4.74584810e-02,  3.49669997e-03,  8.34195036e-03,  1.11869518e-02,\n",
       "         -3.67037132e-02, -6.71541644e-03, -4.41379175e-02,  9.37875453e-03,\n",
       "         -9.38928220e-03, -3.93970609e-02, -2.70058066e-02, -6.46770373e-02,\n",
       "         -1.40956894e-03, -1.85422078e-02, -1.88541077e-02, -3.25614363e-02,\n",
       "         -1.19997058e-02, -3.94494012e-02, -1.03586093e-02,  3.17104012e-02,\n",
       "         -1.01747878e-01, -1.09347310e-02, -4.40120958e-02, -4.15227935e-02,\n",
       "         -3.51167051e-03,  4.76755053e-02, -1.19574764e-03,  7.03339232e-03,\n",
       "         -2.72274632e-02, -1.21569904e-02, -5.33185527e-02, -6.08298182e-02,\n",
       "         -2.26803850e-02, -1.14660067e-02, -5.68424957e-03,  9.19983760e-02,\n",
       "          6.05184138e-02,  1.22147463e-02,  4.40333858e-02, -1.14807691e-02,\n",
       "          1.83383040e-02, -4.73958924e-02, -2.72186100e-02,  6.46771397e-03,\n",
       "         -2.41710916e-02,  2.38149278e-02, -2.55437121e-02,  3.69057851e-03,\n",
       "         -6.00189669e-03, -4.15076539e-02, -1.00643374e-02,  2.04635179e-03,\n",
       "          8.36536512e-02,  3.79589088e-02, -4.29793932e-02, -5.93018457e-02,\n",
       "          5.04089892e-02,  6.13643713e-02, -2.34875400e-02,  3.03885564e-02,\n",
       "         -5.13477027e-02, -7.40223145e-03,  1.88335516e-02, -1.33959111e-02,\n",
       "         -1.41386231e-02,  7.98652123e-04, -8.09048302e-03,  5.93928620e-03,\n",
       "          1.41041903e-02,  4.72489148e-02, -6.56822994e-02, -2.14774087e-02,\n",
       "          1.36746960e-02,  5.82548268e-02,  4.40641157e-02,  2.32171807e-02,\n",
       "          1.88011210e-02,  3.22192122e-04, -4.20587026e-02, -4.76250499e-02,\n",
       "          9.96146351e-02, -2.31252648e-02, -2.13843700e-03,  3.97165818e-03,\n",
       "         -4.57547083e-02,  1.90253910e-02, -4.02803235e-02,  1.39005603e-02,\n",
       "          2.76107211e-02,  9.59599391e-03, -2.60187946e-02,  2.98180822e-02,\n",
       "          4.74365205e-02, -8.61615390e-02,  4.86228280e-02,  2.84075141e-02,\n",
       "          5.23935594e-02, -4.59048757e-03, -5.70984371e-03,  3.93851986e-03,\n",
       "          9.08298194e-02, -4.19426104e-03,  6.86834008e-02, -6.49078488e-02,\n",
       "         -4.73516807e-02, -9.95967910e-03,  2.75779907e-02, -8.93138722e-03,\n",
       "          6.47736667e-03, -5.96474223e-02,  2.80832592e-02, -4.69652191e-02,\n",
       "          8.25014524e-03, -3.04375142e-02,  1.67214684e-02,  8.14813096e-03,\n",
       "          1.87125318e-02,  2.20114291e-02, -8.51076748e-03, -5.94265573e-03,\n",
       "         -1.78495441e-02, -1.31201148e-02,  2.11747736e-03, -1.85518395e-02,\n",
       "          4.21116650e-02, -2.01550256e-02, -2.70130783e-02, -4.83243242e-02,\n",
       "          5.76180033e-02, -8.54594186e-02, -3.27307209e-02, -3.05804200e-02,\n",
       "         -3.44319493e-02,  3.00376490e-02,  9.77613032e-03, -2.94761751e-02,\n",
       "          1.35484692e-02,  1.30729340e-02, -5.92135079e-02,  4.05125581e-02,\n",
       "          3.86501551e-02,  3.99095118e-02, -2.40873788e-02, -1.76136885e-02,\n",
       "          1.87093727e-02,  1.74536481e-02, -5.14415698e-03, -2.56229173e-02,\n",
       "         -8.43492337e-03, -3.15153375e-02,  5.69140352e-02, -4.19958979e-02,\n",
       "         -4.00544554e-02, -1.58617850e-02, -3.08485255e-02, -1.37016019e-02,\n",
       "          8.30272026e-03, -7.70635670e-03,  2.79232617e-02, -1.12561435e-01,\n",
       "         -4.03125323e-02, -1.75414719e-02, -2.89628282e-03,  1.49827739e-02,\n",
       "          1.75914553e-03, -1.80949864e-03, -2.08916445e-03, -5.85770160e-02,\n",
       "         -9.29920748e-03,  3.58477123e-02,  5.56178056e-02,  9.40904319e-02,\n",
       "         -4.50159013e-02,  5.61584812e-03,  2.04590969e-02,  5.39988093e-02,\n",
       "          4.98082908e-03, -5.92459552e-03, -2.58094892e-02, -7.98540842e-03,\n",
       "          2.00941078e-02,  1.94380637e-02,  2.10770350e-02,  3.11624333e-02,\n",
       "         -1.59845967e-02, -2.96211545e-03,  9.21580475e-04,  8.50714445e-02,\n",
       "          7.99991470e-03,  3.56250349e-03,  3.09497248e-02, -4.18040669e-04,\n",
       "          5.15904315e-02,  7.89779797e-03,  1.90040898e-02,  1.25003532e-02,\n",
       "         -1.94080025e-02,  5.14508821e-02, -4.86605316e-02,  6.07090630e-03,\n",
       "         -4.38983627e-02,  2.32938342e-02, -8.08161944e-02, -2.42069997e-02,\n",
       "         -1.28391273e-02,  4.11131866e-02, -1.52970245e-02,  3.03167608e-02,\n",
       "         -1.14781279e-02, -2.84028277e-02,  7.22988369e-03,  3.71559965e-03,\n",
       "         -3.04779615e-02, -2.89483666e-02, -6.04601987e-02,  5.32874875e-02,\n",
       "         -5.14415838e-02,  2.14800052e-02,  7.80999009e-03, -2.14538304e-03,\n",
       "         -8.98413807e-02,  3.14618722e-02,  4.79251593e-02, -1.09424209e-02,\n",
       "         -4.86602597e-02, -3.31611447e-02, -2.27817483e-02,  3.15087475e-02,\n",
       "         -4.50839987e-03,  7.64505863e-02, -2.81256959e-02, -3.92679684e-03,\n",
       "          6.49137248e-04, -1.13006458e-02, -7.44944066e-03,  2.43229754e-02,\n",
       "          4.77067344e-02,  3.17530474e-03, -6.44892007e-02, -6.43545436e-03,\n",
       "          6.99397968e-03, -1.55931562e-01, -1.85896251e-02, -2.79286448e-02,\n",
       "          1.92770623e-02, -1.97945870e-02,  4.07641381e-02, -5.83144197e-33,\n",
       "          5.77020943e-02, -2.53480915e-02,  4.37501743e-02,  6.18992411e-02,\n",
       "          1.82981845e-02,  8.83992463e-02,  2.28854944e-03, -2.30427925e-02,\n",
       "         -1.59993046e-03,  3.42572294e-02,  1.83378235e-02,  1.33181410e-02,\n",
       "          1.03927888e-02, -2.37502679e-02,  3.43737267e-02, -3.84902246e-02,\n",
       "          4.13530581e-02, -5.31161129e-02,  1.65380761e-02,  1.27875581e-02,\n",
       "         -3.32982019e-02, -3.49008963e-02,  2.46763453e-02, -1.62741523e-02,\n",
       "          2.43882556e-02, -2.03412566e-02,  3.01210452e-02, -5.19221500e-02,\n",
       "         -1.19051319e-02,  6.62191957e-02, -2.54398305e-02,  3.30755971e-02,\n",
       "         -2.64314972e-02, -3.46425944e-03, -4.40565348e-02, -1.75737198e-02,\n",
       "         -4.71462868e-02,  1.91331655e-02,  1.38934534e-02,  9.65239946e-03,\n",
       "         -1.34657890e-01, -1.64103154e-02,  2.78365519e-03, -7.85674620e-03,\n",
       "          2.21487731e-02,  3.49004120e-02, -3.41888331e-02, -1.19944918e-03,\n",
       "          1.90856326e-02, -3.96462381e-02,  1.08865800e-03,  7.22520798e-03,\n",
       "         -1.95455290e-02, -3.85722145e-02, -1.35650048e-02,  5.36298007e-02,\n",
       "         -6.47551892e-03, -5.17080575e-02,  1.91377923e-02,  2.63876077e-02,\n",
       "         -5.16176932e-02,  4.74961400e-02, -2.04665121e-02, -1.56124420e-02,\n",
       "          7.08699133e-03,  6.23117108e-03, -2.83536110e-02, -7.59811420e-03,\n",
       "         -1.20449709e-02, -1.60129927e-02, -2.65755341e-03, -1.75971705e-02,\n",
       "         -2.48098541e-02, -6.74474761e-02, -8.76733847e-03, -2.84510516e-02,\n",
       "          3.66985128e-04, -2.01219898e-02, -2.71947752e-03, -3.81318964e-02,\n",
       "         -2.58607678e-02, -2.23500412e-02, -2.25931797e-02, -1.43792909e-02,\n",
       "         -2.53513567e-02,  5.26150968e-03, -1.42565172e-03, -1.50023252e-02,\n",
       "          1.13106482e-02,  1.70005392e-03,  3.85135822e-02,  9.38181113e-03,\n",
       "          1.28855081e-02,  2.14383993e-02,  6.30473159e-03, -1.03155887e-02,\n",
       "         -1.54742170e-02, -1.79873314e-02,  2.14999518e-03, -2.47421265e-02,\n",
       "         -5.24106659e-02,  4.39338535e-02,  5.69731668e-02, -7.16337934e-02,\n",
       "         -5.98368943e-02, -2.12173220e-02,  5.95217897e-03,  1.37588512e-02,\n",
       "         -8.75125173e-03, -1.40239848e-02, -9.74530634e-03,  2.90004071e-02,\n",
       "          8.06629732e-02,  4.12956811e-02, -2.11856123e-02,  1.50932744e-02,\n",
       "          2.29747090e-02,  3.43790837e-02, -1.58567186e-02, -5.84305963e-03,\n",
       "         -4.97097149e-02,  5.57312854e-02,  9.77561064e-03,  2.48399414e-02,\n",
       "         -2.69866362e-02, -1.94870327e-02,  8.75848066e-03,  5.72667830e-02,\n",
       "         -1.88179947e-02,  5.04733995e-03,  1.88054419e-05,  4.45115838e-05,\n",
       "          2.97585927e-07, -2.58118939e-02,  2.15367302e-02, -6.76417490e-03,\n",
       "         -5.28235994e-02,  2.38965396e-02, -3.66790965e-02,  1.42458417e-02,\n",
       "         -2.14779191e-02, -2.16208417e-02, -4.43950593e-02,  2.26807762e-02,\n",
       "         -5.91930840e-03, -1.77469272e-02, -1.99259091e-02,  1.14386547e-02,\n",
       "          3.24623175e-02,  2.28625983e-02, -1.20275095e-01, -2.48679775e-03,\n",
       "         -2.89654508e-02, -1.89430602e-02, -2.33281348e-02, -7.60751450e-03,\n",
       "         -4.51372713e-02,  6.68428326e-03,  3.40937972e-02, -1.67424232e-02,\n",
       "          3.02299373e-02,  6.48112893e-02,  2.71249283e-02,  8.05956963e-03,\n",
       "          4.13722880e-02,  2.35433858e-02,  3.95646654e-02, -1.19372392e-02,\n",
       "         -6.08534999e-02,  2.26793177e-02, -1.00585688e-02, -1.44101186e-02,\n",
       "         -3.36376019e-02, -4.28244881e-02,  1.83359422e-02, -3.15423347e-02,\n",
       "          3.50877829e-02, -5.53266667e-02,  3.58955786e-02, -2.94129830e-03,\n",
       "         -5.56095615e-02, -5.53097716e-03, -1.20298211e-02,  2.77662072e-02,\n",
       "          4.67124172e-02,  1.71348490e-02,  2.37364713e-02, -3.60644199e-02,\n",
       "         -6.39552251e-03,  3.74707915e-02,  9.88707971e-03,  5.78919761e-02,\n",
       "         -1.16841570e-02,  7.52778770e-03,  2.39748526e-02, -2.19954941e-02,\n",
       "          1.20953536e-02, -3.25701060e-03,  1.50652304e-02, -2.37814095e-02,\n",
       "          2.80297363e-34,  1.11585092e-02, -2.68477965e-02,  1.26849869e-02,\n",
       "         -4.03565504e-02,  2.50129681e-02, -5.06627187e-03, -1.60889830e-02,\n",
       "         -5.11207841e-02, -3.60140502e-02, -1.29216369e-02, -4.45698798e-02]),\n",
       "  'score': tensor(0.0171)}]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = random.choice(query_list)\n",
    "print(f\"Query: {query}\")\n",
    "\n",
    "# Answer query with context and return context\n",
    "answer, context_items = ask(query=query,\n",
    "                            temperature=0.7,\n",
    "                            max_new_tokens=512,\n",
    "                            return_answer_only=False)\n",
    "\n",
    "print(f\"Answer:\\n\")\n",
    "print_wrapped(answer)\n",
    "print(f\"Context items:\")\n",
    "context_items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcW2Qor-3jeU"
   },
   "source": [
    "## Try the model with different number of examples in prompt_formatter(Multiple shot prompt engineering) and comment on the answers retrieved by RAG. You can also try different examples but its optional.\n",
    "\n",
    "In this example, we have set to output the context, and we can see that context are ranked in the order of cosine similarity score. We set temperature raletive high as 0.7, so the output is more deterministic. We can change it to see the differencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NQzfN1fkmBih"
   },
   "source": [
    "# 2. LoRA Adapter FineTuning (30 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OyCKwVfhmWEY"
   },
   "source": [
    "[LoRA](https://huggingface.co/docs/peft/conceptual_guides/lora) was introduced at the end of 2021 in the paper [LoRA: Low-rank adaptation of Large Language Models.](https://arxiv.org/pdf/2106.09685.pdf)\n",
    "\n",
    "To make fine-tuning more efficient, LoRAs approach is to represent the weight updates with two smaller matrices (called update matrices) through low-rank decomposition. These new matrices can be trained to adapt to the new data while keeping the overall number of changes low. The original weight matrix remains frozen and doesnt receive any further adjustments. To produce the final results, both the original and the adapted weights are combined.\n",
    "\n",
    "Models that want to support LoRA fine-tuning need to provide adapters. There is a list on [Hugging Face](https://huggingface.co/docs/peft/index#supported-models) with the supported models which covers popular open source models and it is growing.\n",
    "\n",
    "You can also read [Efficient Large Language Model training with LoRA and Hugging Face](https://www.philschmid.de/fine-tune-flan-t5-peft)\n",
    "\n",
    "Advantages of LoRA-\n",
    "There are some additional advantages that LoRA provides. One is to use quantization which leads to memory reduction. Read the blog post on Hugging Face [Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes) for more details.\n",
    "\n",
    "Some people claim that LoRA might also help to reduce [catastrophic forgetting](https://www.pnas.org/doi/10.1073/pnas.1611835114) which happens when too much fine-tuning has been done. However, while the original weights are frozen, at inference time the additional parameters are utilized which lead to other results which is the whole point of fine-tuning.\n",
    "\n",
    "![LoRA](lora.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16202,
     "status": "ok",
     "timestamp": 1745902770346,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "YFGPx8lPnGED",
    "outputId": "5f8c99a1-5dce-4962-965f-24c71980f91f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.51.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.6.0)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.30.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2025.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub>=0.25.0->peft) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2025.1.31)\n",
      "[INFO] Running in Google Colab, installing requirements.\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
      "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.51.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (2.6.0+cu124)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (0.30.2)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence-transformers) (11.2.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.6.0)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.0.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.6.0+cu124)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.30.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n"
     ]
    }
   ],
   "source": [
    "## DONOT CHANGE THE CODE\n",
    "\n",
    "!pip install datasets\n",
    "import os\n",
    "!pip install peft\n",
    "if \"COLAB_GPU\" in os.environ:\n",
    "    print(\"[INFO] Running in Google Colab, installing requirements.\")\n",
    "    # !pip install -U torch # requires torch 2.1.1+ (for efficient sdpa implementation)\n",
    "    !pip install tqdm # for progress bars\n",
    "    !pip install sentence-transformers # for embedding models\n",
    "    !pip install accelerate # for quantization model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1qZDiokGmGUN"
   },
   "outputs": [],
   "source": [
    "## DONOT CHANGE THE CODE\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, default_data_collator, get_linear_schedule_with_warmup\n",
    "from peft import get_peft_config, get_peft_model, PromptTuningInit, PromptTuningConfig, TaskType, PeftType\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W0R9W0BxmeKb"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model_name_or_path = \"bigscience/bloomz-560m\"\n",
    "tokenizer_name_or_path = \"bigscience/bloomz-560m\"\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    prompt_tuning_init=PromptTuningInit.TEXT,\n",
    "    num_virtual_tokens=20,\n",
    "    prompt_tuning_init_text=\"Classify if the tweet is a complaint or not:\",\n",
    "    tokenizer_name_or_path=model_name_or_path,\n",
    ")\n",
    "\n",
    "dataset_name = \"twitter_complaints\"\n",
    "checkpoint_name = f\"{dataset_name}_{model_name_or_path}_{peft_config.peft_type}_{peft_config.task_type}_v1.pt\".replace(\n",
    "    \"/\", \"_\"\n",
    ")\n",
    "text_column = \"Tweet text\"\n",
    "label_column = \"text_label\"\n",
    "max_length = 64\n",
    "lr = 5e-2\n",
    "num_epochs = 10\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3940,
     "status": "ok",
     "timestamp": 1745902792175,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "_ZDExgJamjQb",
    "outputId": "1e5bba5e-7a82-4632-9e5f-ec2672d6a2c2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "## DONOT CHANGE THE CODE\n",
    "\n",
    "#load data\n",
    "dataset = load_dataset(\"ought/raft\", dataset_name)\n",
    "dataset[\"train\"][0]\n",
    "\n",
    "classes = [k.replace(\"_\", \" \") for k in dataset[\"train\"].features[\"Label\"].names]\n",
    "dataset = dataset.map(\n",
    "    lambda x: {\"text_label\": [classes[label] for label in x[\"Label\"]]},\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    ")\n",
    "dataset[\"train\"][0]\n",
    "{\"Tweet text\": \"@HMRCcustomers No this is my first job\", \"ID\": 0, \"Label\": 2, \"text_label\": \"no complaint\"}\n",
    "\n",
    "## preprocess\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "target_max_length = max([len(tokenizer(class_label)[\"input_ids\"]) for class_label in classes])\n",
    "print(target_max_length)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uBOjLWOMm4PO"
   },
   "outputs": [],
   "source": [
    "## DONOT CHANGE THE CODE\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    batch_size = len(examples[text_column])\n",
    "    inputs = [f\"{text_column} : {x} Label : \" for x in examples[text_column]]\n",
    "    targets = [str(x) for x in examples[label_column]]\n",
    "    model_inputs = tokenizer(inputs)\n",
    "    labels = tokenizer(targets)\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i] + [tokenizer.pad_token_id]\n",
    "        # print(i, sample_input_ids, label_input_ids)\n",
    "        model_inputs[\"input_ids\"][i] = sample_input_ids + label_input_ids\n",
    "        labels[\"input_ids\"][i] = [-100] * len(sample_input_ids) + label_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [1] * len(model_inputs[\"input_ids\"][i])\n",
    "    # print(model_inputs)\n",
    "    for i in range(batch_size):\n",
    "        sample_input_ids = model_inputs[\"input_ids\"][i]\n",
    "        label_input_ids = labels[\"input_ids\"][i]\n",
    "        model_inputs[\"input_ids\"][i] = [tokenizer.pad_token_id] * (\n",
    "            max_length - len(sample_input_ids)\n",
    "        ) + sample_input_ids\n",
    "        model_inputs[\"attention_mask\"][i] = [0] * (max_length - len(sample_input_ids)) + model_inputs[\n",
    "            \"attention_mask\"\n",
    "        ][i]\n",
    "        labels[\"input_ids\"][i] = [-100] * (max_length - len(sample_input_ids)) + label_input_ids\n",
    "        model_inputs[\"input_ids\"][i] = torch.tensor(model_inputs[\"input_ids\"][i][:max_length])\n",
    "        model_inputs[\"attention_mask\"][i] = torch.tensor(model_inputs[\"attention_mask\"][i][:max_length])\n",
    "        labels[\"input_ids\"][i] = torch.tensor(labels[\"input_ids\"][i][:max_length])\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81,
     "referenced_widgets": [
      "ed0371b55f744e9293aeb8422e3028a6",
      "5ba474ceda984fc2b917e0724cab2c05",
      "d2a2490d7ab249e19e934bc84b17a437",
      "d0133831eee443f28d4da61450fe4c47",
      "f0d82491057b4b0185ebef7100448c09",
      "c1edd10883d746bab0ff52edd6871a9d",
      "9b538b6909594ebb99e3dffb660adee7",
      "5ddc6ad9367f40aab5b53200b95d875b",
      "d445d328c20943c18ca7d9785a9ef402",
      "13a3ba3841f247b98a861868d80b8897",
      "88ebe024f13848a988b772fc81947d1e",
      "7b14ebc99dc4403487d22a0de5577018",
      "053b2dde775d4ec882373b24440ddb4a",
      "e0e2692b71824152a3370905b2af04e0",
      "f34d5f41efbb4d5ea87edfe193a62eb8",
      "b2299abf79cb4a978f50841edfc3f77e",
      "37894cf275e84d3ca35144c4fe499342",
      "db787461faed4c49929aad7b4a7b1f1e",
      "970e33278e1e4ab686aea3d4a3ad32f2",
      "59f75ef2ea864e5cb7079522961edb1f",
      "f62ccc4a65f543608d21c53edc4e1cec",
      "ca492b002baa4b97bde54f30493f9b95"
     ]
    },
    "executionInfo": {
     "elapsed": 3900,
     "status": "ok",
     "timestamp": 1745902796110,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "BdG8zpfHm4_Q",
    "outputId": "31382f01-0db5-4e65-ab25-6aa5e3f84f56"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed0371b55f744e9293aeb8422e3028a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/50 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b14ebc99dc4403487d22a0de5577018",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/3399 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## DONOT CHANGE THE CODE\n",
    "\n",
    "processed_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    load_from_cache_file=False,\n",
    "    desc=\"Running tokenizer on dataset\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IlMkAYRhm6-t"
   },
   "outputs": [],
   "source": [
    "train_dataset = processed_datasets[\"train\"]\n",
    "eval_dataset = processed_datasets[\"test\"]\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, shuffle=True, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True\n",
    ")\n",
    "eval_dataloader = DataLoader(eval_dataset, collate_fn=default_data_collator, batch_size=batch_size, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5185,
     "status": "ok",
     "timestamp": 1745902801326,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "mmy80lgim9h2",
    "outputId": "536d3073-afb6-4166-a5fa-a222c836f6a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,480 || all params: 559,235,072 || trainable%: 0.0037\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path)\n",
    "model = get_peft_model(model, peft_config)\n",
    "print(model.print_trainable_parameters())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QZxuczmbnrth"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "lr_scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=(len(train_dataloader) * num_epochs),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1139638,
     "status": "ok",
     "timestamp": 1745903940963,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "IVeZRGNknwW8",
    "outputId": "825eaeef-9075-4a40-ea25-97a30610e712"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:03<00:00,  1.15it/s]\n",
      "100%|| 213/213 [01:51<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0: train_ppl=tensor(1.2259e+11, device='cuda:0') train_epoch_loss=tensor(25.5321, device='cuda:0') eval_ppl=tensor(9584.4043, device='cuda:0') eval_epoch_loss=tensor(9.1679, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:03<00:00,  1.26it/s]\n",
      "100%|| 213/213 [01:51<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1: train_ppl=tensor(340.2736, device='cuda:0') train_epoch_loss=tensor(5.8298, device='cuda:0') eval_ppl=tensor(36118.0781, device='cuda:0') eval_epoch_loss=tensor(10.4945, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:03<00:00,  1.27it/s]\n",
      "100%|| 213/213 [01:50<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=2: train_ppl=tensor(87.3750, device='cuda:0') train_epoch_loss=tensor(4.4702, device='cuda:0') eval_ppl=tensor(61377.9531, device='cuda:0') eval_epoch_loss=tensor(11.0248, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:03<00:00,  1.27it/s]\n",
      "100%|| 213/213 [01:50<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=3: train_ppl=tensor(63.1506, device='cuda:0') train_epoch_loss=tensor(4.1455, device='cuda:0') eval_ppl=tensor(74731.7031, device='cuda:0') eval_epoch_loss=tensor(11.2217, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:03<00:00,  1.28it/s]\n",
      "100%|| 213/213 [01:50<00:00,  1.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=4: train_ppl=tensor(48.5964, device='cuda:0') train_epoch_loss=tensor(3.8835, device='cuda:0') eval_ppl=tensor(61023.3320, device='cuda:0') eval_epoch_loss=tensor(11.0190, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:03<00:00,  1.27it/s]\n",
      "100%|| 213/213 [01:50<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=5: train_ppl=tensor(36.4739, device='cuda:0') train_epoch_loss=tensor(3.5966, device='cuda:0') eval_ppl=tensor(65545.3203, device='cuda:0') eval_epoch_loss=tensor(11.0905, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:03<00:00,  1.26it/s]\n",
      "100%|| 213/213 [01:50<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=6: train_ppl=tensor(34.9229, device='cuda:0') train_epoch_loss=tensor(3.5531, device='cuda:0') eval_ppl=tensor(88039.0938, device='cuda:0') eval_epoch_loss=tensor(11.3855, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:03<00:00,  1.27it/s]\n",
      "100%|| 213/213 [01:50<00:00,  1.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=7: train_ppl=tensor(28.6202, device='cuda:0') train_epoch_loss=tensor(3.3541, device='cuda:0') eval_ppl=tensor(108361.4219, device='cuda:0') eval_epoch_loss=tensor(11.5932, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:03<00:00,  1.27it/s]\n",
      "100%|| 213/213 [01:50<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=8: train_ppl=tensor(26.5144, device='cuda:0') train_epoch_loss=tensor(3.2777, device='cuda:0') eval_ppl=tensor(88194.3047, device='cuda:0') eval_epoch_loss=tensor(11.3873, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 4/4 [00:03<00:00,  1.27it/s]\n",
      "100%|| 213/213 [01:50<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=9: train_ppl=tensor(24.8403, device='cuda:0') train_epoch_loss=tensor(3.2125, device='cuda:0') eval_ppl=tensor(82103.1797, device='cuda:0') eval_epoch_loss=tensor(11.3157, device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for step, batch in enumerate(tqdm(train_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.detach().float()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    model.eval()\n",
    "    eval_loss = 0\n",
    "    eval_preds = []\n",
    "    for step, batch in enumerate(tqdm(eval_dataloader)):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        eval_loss += loss.detach().float()\n",
    "        eval_preds.extend(\n",
    "            tokenizer.batch_decode(torch.argmax(outputs.logits, -1).detach().cpu().numpy(), skip_special_tokens=True)\n",
    "        )\n",
    "\n",
    "    eval_epoch_loss = eval_loss / len(eval_dataloader)\n",
    "    eval_ppl = torch.exp(eval_epoch_loss)\n",
    "    train_epoch_loss = total_loss / len(train_dataloader)\n",
    "    train_ppl = torch.exp(train_epoch_loss)\n",
    "    print(f\"{epoch=}: {train_ppl=} {train_epoch_loss=} {eval_ppl=} {eval_epoch_loss=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 491,
     "status": "ok",
     "timestamp": 1745904585389,
     "user": {
      "displayName": "jiheng zhang",
      "userId": "16676062713803155186"
     },
     "user_tz": 420
    },
    "id": "rrP8_f4Bn0EP",
    "outputId": "fa58e689-b808-46ea-f184-ed7658856835"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tweet text : @united not happy with this delay from Newark to Manchester tonight :( only 30 mins free Wi-fi sucks ... Label : complaint']\n"
     ]
    }
   ],
   "source": [
    "# Test the Tuned Model\n",
    "inputs = tokenizer(\n",
    "    f'{text_column} : {\"@united not happy with this delay from Newark to Manchester tonight :( only 30 mins free Wi-fi sucks ...\"} Label : ',\n",
    "    return_tensors=\"pt\",\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_new_tokens=10, eos_token_id=3\n",
    "    )\n",
    "    print(tokenizer.batch_decode(outputs.detach().cpu().numpy(), skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g-sDhzaE1XFE"
   },
   "source": [
    "# Train with 3 different Hyper-parameter Settings and explain the differences in output generation. Also try the best model with 5 different texts from https://huggingface.co/datasets/ought/raft/viewer?views%5B%5D=ade_corpus_v2_train\n",
    "\n",
    "text_column = \"Tweet text\"\n",
    "label_column = \"text_label\"\n",
    "max_length = 64\n",
    "lr = 3e-2\n",
    "num_epochs = 10\n",
    "batch_size = 8\n",
    "\n",
    "epoch=9: train_ppl=tensor(34.3882, device='cuda:0') train_epoch_loss=tensor(3.5377, device='cuda:0') eval_ppl=tensor(19200.8535, device='cuda:0') eval_epoch_loss=tensor(9.8627, device='cuda:0')\n",
    "\n",
    "['Tweet text : @nationalgridus I have no water and the bill is current and paid. Can you do something about this? Label : no complaintestado/no complaintestado/no complaint']\n",
    "\n",
    "\\\\\n",
    "text_column = \"Tweet text\"\n",
    "label_column = \"text_label\"\n",
    "max_length = 64\n",
    "lr = 5e-2\n",
    "num_epochs = 10\n",
    "batch_size = 8\n",
    "\n",
    "epoch=9: train_ppl=tensor(28.3167, device='cuda:0') train_epoch_loss=tensor(3.3435, device='cuda:0') eval_ppl=tensor(38194.5625, device='cuda:0') eval_epoch_loss=tensor(10.5504, device='cuda:0')\n",
    "\n",
    "['Tweet text : @nationalgridus I have no water and the bill is current and paid. Can you do something about this? Label : no complaintno complaintno complaintno']\n",
    "\n",
    "\\\\\n",
    "text_column = \"Tweet text\"\n",
    "label_column = \"text_label\"\n",
    "max_length = 64\n",
    "lr = 5e-2\n",
    "num_epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "epoch=9: train_ppl=tensor(24.8403, device='cuda:0') train_epoch_loss=tensor(3.2125, device='cuda:0') eval_ppl=tensor(82103.1797, device='cuda:0') eval_epoch_loss=tensor(11.3157, device='cuda:0')\n",
    "\n",
    "['Tweet text : @nationalgridus I have no water and the bill is current and paid. Can you do something about this? Label : complaint (no complaint) no complaint (compl']\n",
    "\n",
    "I first use the provided default hyper-parameter setting and the result was not correct. I change the learning rate to increase, we can see the results are getting better, but the prediction is still wrong. I increase the batch size since 8 was too small for LLM and was able to get correct prediction on this input. But we can still see wrong predictions in the following results when I try different input.\n",
    "\n",
    "\\\\\n",
    "# 5 different text input\n",
    "['Tweet text : Couples wallpaper, so cute. :) #BrothersAtHome Label : no complaintSErrorNo complaintSErrorNo complaintSErrorNo']\n",
    "\n",
    "\\\\\n",
    "['Tweet text : @EE On Rosneath Arial having good upload and download speeds but terrible latency 200ms. Why is this. Label : no complaintSErrorNo complaintSErrorNo complaintSErrorNo']\n",
    "\n",
    "\\\\\n",
    "['Tweet text : @Lin_Manuel @jmessinaphoto @VAMNit Omg a little squish!!!!! Enjoy and congrats!!!! I miss mine being so young!  Label : no complaint (complaint) no complaint (compl']\n",
    "\n",
    "\\\\\n",
    "['Tweet text : @greateranglia Could I ask why the Area in front of BIC Station was not gritted withh all the snow. Label : no complaintSErrorNo complaintSErrorNo complaintSErrorNo']\n",
    "\n",
    "\\\\\n",
    "['Tweet text : @united not happy with this delay from Newark to Manchester tonight :( only 30 mins free Wi-fi sucks ... Label : complaint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UE29Odj_1Y1x"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "053b2dde775d4ec882373b24440ddb4a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_37894cf275e84d3ca35144c4fe499342",
      "placeholder": "",
      "style": "IPY_MODEL_db787461faed4c49929aad7b4a7b1f1e",
      "value": "Runningtokenizerondataset:100%"
     }
    },
    "13a3ba3841f247b98a861868d80b8897": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d54c779b9ac4240a885daff808a6821": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37894cf275e84d3ca35144c4fe499342": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3c87ccdf392c4799b146c5f15631c13f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "549e8192fd7b4184a6a2c1e1eec21dc0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "59f75ef2ea864e5cb7079522961edb1f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5ba474ceda984fc2b917e0724cab2c05": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c1edd10883d746bab0ff52edd6871a9d",
      "placeholder": "",
      "style": "IPY_MODEL_9b538b6909594ebb99e3dffb660adee7",
      "value": "Runningtokenizerondataset:100%"
     }
    },
    "5c5c9276b5f34ce7b87e1194e765321b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5ddc6ad9367f40aab5b53200b95d875b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5f13868b073d41fbb6f9afb07c2f9441": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f21c25b5d304b179353abe2b42837a5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70cdb00e808348dcaddaa7ec6f7fd9ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f12a051b04e5425da01a7babc4929454",
       "IPY_MODEL_7313fdcf34764175bb092a488ff76fc7",
       "IPY_MODEL_b32a9d3278074e22b652c588fc13ae1e"
      ],
      "layout": "IPY_MODEL_5f13868b073d41fbb6f9afb07c2f9441"
     }
    },
    "7313fdcf34764175bb092a488ff76fc7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6f21c25b5d304b179353abe2b42837a5",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_8c5e430043d140c4b64f73b5ff67da4c",
      "value": 2
     }
    },
    "74ca64aa476c4727b5a9fd513a8fe01c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a7d5f82ecb04c6990e24facb2b9df18": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7b14ebc99dc4403487d22a0de5577018": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_053b2dde775d4ec882373b24440ddb4a",
       "IPY_MODEL_e0e2692b71824152a3370905b2af04e0",
       "IPY_MODEL_f34d5f41efbb4d5ea87edfe193a62eb8"
      ],
      "layout": "IPY_MODEL_b2299abf79cb4a978f50841edfc3f77e"
     }
    },
    "7ee4e027800e496eb6b684bc71946bf1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "88ebe024f13848a988b772fc81947d1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8c5e430043d140c4b64f73b5ff67da4c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "91265efea71343bea7a3195666f075a0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "96d56c3bfa80490986a2246c362aa78f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "970e33278e1e4ab686aea3d4a3ad32f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9a077f71782f4fd6adcd212edf706bfc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3c87ccdf392c4799b146c5f15631c13f",
      "placeholder": "",
      "style": "IPY_MODEL_7ee4e027800e496eb6b684bc71946bf1",
      "value": "Fetching12files:100%"
     }
    },
    "9b538b6909594ebb99e3dffb660adee7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b2299abf79cb4a978f50841edfc3f77e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b32a9d3278074e22b652c588fc13ae1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_549e8192fd7b4184a6a2c1e1eec21dc0",
      "placeholder": "",
      "style": "IPY_MODEL_7a7d5f82ecb04c6990e24facb2b9df18",
      "value": "2/2[00:31&lt;00:00,13.11s/it]"
     }
    },
    "b594d2e620d44d959bb8e3cf5245184e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c1edd10883d746bab0ff52edd6871a9d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ca492b002baa4b97bde54f30493f9b95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d0133831eee443f28d4da61450fe4c47": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_13a3ba3841f247b98a861868d80b8897",
      "placeholder": "",
      "style": "IPY_MODEL_88ebe024f13848a988b772fc81947d1e",
      "value": "50/50[00:00&lt;00:00,557.34examples/s]"
     }
    },
    "d2a2490d7ab249e19e934bc84b17a437": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5ddc6ad9367f40aab5b53200b95d875b",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d445d328c20943c18ca7d9785a9ef402",
      "value": 50
     }
    },
    "d445d328c20943c18ca7d9785a9ef402": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d8982a44da304f9481062f2f4019c3d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_74ca64aa476c4727b5a9fd513a8fe01c",
      "placeholder": "",
      "style": "IPY_MODEL_5c5c9276b5f34ce7b87e1194e765321b",
      "value": "12/12[00:00&lt;00:00,37.71it/s]"
     }
    },
    "db787461faed4c49929aad7b4a7b1f1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e0e2692b71824152a3370905b2af04e0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_970e33278e1e4ab686aea3d4a3ad32f2",
      "max": 3399,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_59f75ef2ea864e5cb7079522961edb1f",
      "value": 3399
     }
    },
    "ed0371b55f744e9293aeb8422e3028a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5ba474ceda984fc2b917e0724cab2c05",
       "IPY_MODEL_d2a2490d7ab249e19e934bc84b17a437",
       "IPY_MODEL_d0133831eee443f28d4da61450fe4c47"
      ],
      "layout": "IPY_MODEL_f0d82491057b4b0185ebef7100448c09"
     }
    },
    "ee39d84b949543509713a50094805ac1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f0d82491057b4b0185ebef7100448c09": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f12a051b04e5425da01a7babc4929454": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ee39d84b949543509713a50094805ac1",
      "placeholder": "",
      "style": "IPY_MODEL_b594d2e620d44d959bb8e3cf5245184e",
      "value": "Loadingcheckpointshards:100%"
     }
    },
    "f34d5f41efbb4d5ea87edfe193a62eb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f62ccc4a65f543608d21c53edc4e1cec",
      "placeholder": "",
      "style": "IPY_MODEL_ca492b002baa4b97bde54f30493f9b95",
      "value": "3399/3399[00:02&lt;00:00,1328.28examples/s]"
     }
    },
    "f62ccc4a65f543608d21c53edc4e1cec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f99d220ca20547179be152853a02ec68": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_96d56c3bfa80490986a2246c362aa78f",
      "max": 12,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_91265efea71343bea7a3195666f075a0",
      "value": 12
     }
    },
    "fda3c7a15eef4d60901797750420c7e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_9a077f71782f4fd6adcd212edf706bfc",
       "IPY_MODEL_f99d220ca20547179be152853a02ec68",
       "IPY_MODEL_d8982a44da304f9481062f2f4019c3d4"
      ],
      "layout": "IPY_MODEL_2d54c779b9ac4240a885daff808a6821"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
