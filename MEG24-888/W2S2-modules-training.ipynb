{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a neural network\n",
    "\n",
    "Last week, we learned about tensors and modules in PyTorch. This week, we will go over all the steps to define a model for a certain learning task for a dataset and train it. You will learn to use PyTorch to:\n",
    "\n",
    "- define or download the dataset and inspect it to understand the shape and type of input and output of the learning task (usually called x and y).\n",
    "- define the model with the correct layers according to the shape and dtype of the input and output.\n",
    "- define a loss function that is a quantitative metric for the performance of the model\n",
    "- perform backpropagation to calculate the gradient of the loss with respect to all parameters\n",
    "- update the model parameters using the calculated gradients\n",
    "- automate the whole procedure in a training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# For interactive plotting:\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "try:\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()\n",
    "except ImportError:\n",
    "    pass\n",
    "try:\n",
    "    %matplotlib widget\n",
    "except:\n",
    "    os.system('pip install ipympl -qq')\n",
    "    %matplotlib widget\n",
    "\n",
    "# tqdm is a nice library to create progress bars (we'll use it at the end)\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import optim\n",
    "\n",
    "# for datasets provided by pytorch:\n",
    "from torchvision import datasets\n",
    "# for transformations (preprocessing, augmentation, ...) we can apply to the datasets:\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "\n",
    "# our very useful function to inspect a tensor:\n",
    "def print_tensor_info(\n",
    "        name: str, \n",
    "        tensor: torch.Tensor,\n",
    "        ):\n",
    "    print(f'{name}')\n",
    "    print(20*'-')\n",
    "    if not isinstance(tensor, torch.Tensor):\n",
    "        print(f'It is {type(tensor).__name__}!')\n",
    "        print(20*'='+'\\n')\n",
    "        return\n",
    "    # print name, shape, dtype, device, require_grad\n",
    "    print(f'shape: {tensor.shape}')\n",
    "    print(f'dtype: {tensor.dtype}')\n",
    "    print(f'device: {tensor.device}')\n",
    "    print(f'requires_grad: {tensor.requires_grad}')\n",
    "    print(20*'='+'\\n')\n",
    "\n",
    "\n",
    "# Check if cuda (GPU) is available\n",
    "Device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Using {Device} device')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "In this notebook, we use a dataset from `torchvision`. You can find the available datasets in `torchvision.datasets` [here](https://pytorch.org/vision/0.8/datasets.html). As an example, we are going to download and use the USPS dataset. For each dataset, carefully read the documentation on how to download and use it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.USPS(\n",
    "    root = 'week2-data', # where the data will be downloaded and stored\n",
    "    train = True, # if you want the train data. pass False if you want the test data\n",
    "    download = True, # download the data if you don't have it\n",
    "    transform = v2.Compose([\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "    ]) # data preprocessing and augmentation\n",
    ")\n",
    "\n",
    "test_data = datasets.USPS(\n",
    "    root = 'week2-data', # where the data will be downloaded and stored\n",
    "    train = False, # if you want the train data. pass False if you want the test data\n",
    "    download = True, # download the data if you don't have it\n",
    "    transform = v2.Compose([\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "    ]) # data preprocessing and augmentation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The base class for a Dataset is `torch.nn.utils.data.Dataset` which we have imported as `Dataset` for convenience. All these available datasets are an instance of this class.\n",
    "\n",
    "Spoiler: In this week's assignment, you will have to write your custom subclass of `Dataset`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isinstance(train_data, Dataset), isinstance(test_data, Dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get the number of samples using `len(your_dataset)`. You can also index it to get a specific sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples: 7291\n",
      "Number of test  samples: 2007\n"
     ]
    }
   ],
   "source": [
    "n_train = len(train_data)\n",
    "n_test = len(test_data)\n",
    "\n",
    "print(f\"Number of train samples: {n_train}\")\n",
    "print(f\"Number of test  samples: {n_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample\n",
      "--------------------\n",
      "It is tuple!\n",
      "====================\n",
      "\n",
      "x\n",
      "--------------------\n",
      "shape: torch.Size([1, 16, 16])\n",
      "dtype: torch.float32\n",
      "device: cpu\n",
      "requires_grad: False\n",
      "====================\n",
      "\n",
      "y\n",
      "--------------------\n",
      "It is int!\n",
      "====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's look at a sample from the training data\n",
    "sample = train_data[0]\n",
    "print_tensor_info('sample', sample)\n",
    "x, y = sample\n",
    "print_tensor_info('x', x)\n",
    "print_tensor_info('y', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's ignore the first dimension for now. Each sample appears to be of size `(16,16)`, which means it is a 2D tensor, i.e. a matrix.Actually, the input is an image. Let's visualize the data using the main plotting library in python `matplotlib` and the `ipywidgets` library that is helpful to create an interactive interface to visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ImageDataViz:\n",
    "    \"\"\"\n",
    "    An interactive image data visualzation tool inside Juptyer Notebook.\n",
    "    Make sure to use the magic command: %matplotlib widget\n",
    "    \"\"\"\n",
    "    def __init__(self, data: Dataset):\n",
    "        self.data = data\n",
    "        self.n_samples = len(data)\n",
    "        self.index = widgets.IntSlider(\n",
    "            value=0, \n",
    "            min=0, \n",
    "            max=self.n_samples-1, \n",
    "            step=1, \n",
    "            description='Index', \n",
    "            continuous_update=True,\n",
    "            layout=widgets.Layout(width='40%'),\n",
    "        )\n",
    "\n",
    "    def update(self, index: int):\n",
    "        x, y = self.data[index]\n",
    "        image = x.moveaxis(0, -1).squeeze().numpy()\n",
    "        self.img.set_data(image)\n",
    "        self.ax.set_title(f'Label: {y}')\n",
    "\n",
    "    def show(self):\n",
    "        self.fig, self.ax = plt.subplots()\n",
    "        x, y = self.data[0]\n",
    "        image = x.moveaxis(0, -1).squeeze().numpy()\n",
    "        self.img = self.ax.imshow(image)\n",
    "        self.ax.axis('off')\n",
    "        self.ax.set_title(f'Label: {y}')\n",
    "        widgets.interact(self.update, index=self.index)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58a361feec8a409186f304248279693a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='Index', layout=Layout(width='40%'), max=7290), Output())â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b78f4e002d4c2596cd736ec2ec735a",
       "version_major": 2,
       "version_minor": 0
      },
      "image/png": "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",
      "text/html": [
       "\n",
       "            <div style=\"display: inline-block;\">\n",
       "                <div class=\"jupyter-widgets widget-label\" style=\"text-align: center;\">\n",
       "                    Figure\n",
       "                </div>\n",
       "                <img src='data:image/png;base64,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' width=640.0/>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "viz = ImageDataViz(train_data)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that this dataset consists of `(16, 16)` grayscale images of digits with their corresponding digit as a label saved as an integer. The first dimension that we ignored before is known as the channel dimension. You will get more familiar with this concept in the upcoming weeks when learning more about image data and convilutional neural networks. For now, know that grayscale images have one channel and color images (RGB) usually have 3 channels for Red, Green and Blue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloader for batching the data\n",
    "\n",
    "As you have learned, data is usually used as mini-batches in deep learning. In the code, we simply call them batches. You can choose the batch_size to be 1 (a single sample at a time), n_train (all the dataset), or a more reasonable number in between. Usually batch size is set to 32, 64, or 128. Choosing a power of two is best to optimize hardware utilization.\n",
    "\n",
    "The tool that PyTorch provides us to batch the data and go over the batches one by one and feed them to our model is `torch.utils.data.DataLoader` which we imported as `DataLoader`. Using it is pretty simple: you pass in your dataset, specify the batch_size, whether you want to shuffle it in each epoch (one full pass over the dataset), and some other optional choices. Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch\n",
      "--------------------\n",
      "It is list!\n",
      "====================\n",
      "\n",
      "batched x\n",
      "--------------------\n",
      "shape: torch.Size([32, 1, 16, 16])\n",
      "dtype: torch.float32\n",
      "device: cpu\n",
      "requires_grad: False\n",
      "====================\n",
      "\n",
      "batched y\n",
      "--------------------\n",
      "shape: torch.Size([32])\n",
      "dtype: torch.int64\n",
      "device: cpu\n",
      "requires_grad: False\n",
      "====================\n",
      "\n",
      "batch\n",
      "--------------------\n",
      "It is list!\n",
      "====================\n",
      "\n",
      "batched x\n",
      "--------------------\n",
      "shape: torch.Size([32, 1, 16, 16])\n",
      "dtype: torch.float32\n",
      "device: cpu\n",
      "requires_grad: False\n",
      "====================\n",
      "\n",
      "batched y\n",
      "--------------------\n",
      "shape: torch.Size([32])\n",
      "dtype: torch.int64\n",
      "device: cpu\n",
      "requires_grad: False\n",
      "====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size = 32,\n",
    "    shuffle = True, # shuffle the data before each epoch\n",
    "    drop_last = True, # The last batch may not be of the same size as the previous ones. If you want to drop it, set this to True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size = 32,\n",
    "    shuffle = False, # no need to shuffle the test data\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Typically, the way to use the dataloader is to loop over it to get the batches one by one.\n",
    "Here is how you can do it:\n",
    "\n",
    "\n",
    "The full loop is called an epoch. It is the number of times you loop over the entire dataset.\n",
    "For now, we'll just use one batch and show how it works and what it gives us.\n",
    "\"\"\"\n",
    "for batch in train_loader:\n",
    "    x, y = batch\n",
    "    break # We don't to loop over all the batches for now. This was just for demonstration\n",
    "\n",
    "print_tensor_info('batch', batch)\n",
    "print_tensor_info('batched x', x)\n",
    "print_tensor_info('batched y', y)\n",
    "\n",
    "\n",
    "# Another way to get the next batch without the for loop ane the break:\n",
    "# We'll work with this single batch for now to go over the steps\n",
    "batch = next(iter(train_loader))\n",
    "x, y = batch\n",
    "\n",
    "print_tensor_info('batch', batch)\n",
    "print_tensor_info('batched x', x)\n",
    "print_tensor_info('batched y', y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "You need important information from the dataset before you can implement your model, most importantly the shape and dtype of input and output. In our example, we will use a simple softmax regression (one layer only) to classify the image as the correct digit it represents. But how do we apply a linear layer to a 2D image? An easy way is to flatten the whole tensor as a long vector (1D tensor) first. After flattening, we will have a `16*16` vector, and that is going to be the `in_features` for the linear module. The `out_features` should be the number of classes (why?). The softmax is not explicitly applied, because we are going to use `nn.CrossEntropyLoss()`, which takes logits (the outputs before softmax). Since the argmax of the output does not change if we apply softmax, we can directly get the predicted class from the logits and we do not need the softmax. `nn.CrossEntropyLoss()` combines the softmax and the loss into one module because of some benefits in terms of the numerical stability and computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_pred\n",
      "--------------------\n",
      "shape: torch.Size([32, 10])\n",
      "dtype: torch.float32\n",
      "device: cuda:0\n",
      "requires_grad: True\n",
      "====================\n",
      "\n",
      "y\n",
      "--------------------\n",
      "shape: torch.Size([32])\n",
      "dtype: torch.int64\n",
      "device: cpu\n",
      "requires_grad: False\n",
      "====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "    # First, we flatten the 2D image into a long vector\n",
    "    nn.Flatten(start_dim=1), # because dim=0 is the batch dimension\n",
    "    nn.Linear(16*16, 10), # 16*16 is the size of the image, and 10 is the number of classes\n",
    ").to(Device) # don't forget to move the model to the device\n",
    "\n",
    "\n",
    "x, y = next(iter(train_loader))\n",
    "# always remember to move the data to the correct device and dtype\n",
    "\n",
    "\"\"\"\n",
    "Move the data to the device and the correct dtype.\n",
    "Then, pass the input to the model to get the output.\n",
    "\"\"\"\n",
    "\n",
    "# Test the model with the batch. You should not get an error.\n",
    "x = x.to(device=Device, dtype=torch.float32) # the input should be on the same device as the model.\n",
    "y_pred = model(x)\n",
    "\n",
    "print_tensor_info('y_pred', y_pred)\n",
    "\n",
    "# For comparison:\n",
    "print_tensor_info('y', y)\n",
    "\n",
    "# You can inspect the content of these tensors to see what they look like as well!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function\n",
    "\n",
    "The next step is to define the objective you want to optimize. Usually, the objective is defined as a loss function (the lower, the better) that is minimized in training. Since this is a multi-class classification, we need a cross entropy loss. You can define your loss function anyway you want, as long as it represents a metric measuring the divergence of the model prediction from the output. However, some specific loss functions are better suited for some properties and have desirable convergence properties and so on. For example, a commonly used loss function for multi-class classification is the cross entropy loss, and it's already available in PyTorch. Let's see how to define and use it: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss\n",
      "--------------------\n",
      "shape: torch.Size([])\n",
      "dtype: torch.float32\n",
      "device: cuda:0\n",
      "requires_grad: True\n",
      "====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#######################################################################\n",
    "# Components:\n",
    "\n",
    "model = nn.Sequential(\n",
    "    # First, we flatten the 2D image into a long vector\n",
    "    nn.Flatten(\n",
    "        start_dim=1, # because dim=0 is the batch dimension\n",
    "    ),\n",
    "    nn.Linear(16*16, 10), # 16*16 is the size of the image, and 10 is the number of classes\n",
    ").to(Device) # don't forget to move the model to the device\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(\n",
    "    reduction = 'mean', # the loss is averaged over the batch. You can also choose 'sum' or 'none'.\n",
    ")\n",
    "\n",
    "#######################################################################\n",
    "# Training: \n",
    "\n",
    "x, y = next(iter(train_loader))\n",
    "x = x.to(device=Device, dtype=torch.float32) # the input should be on the same device as the model.\n",
    "y = y.to(device=Device, dtype=torch.long) # device should be the same as the model output for the loss function to work\n",
    "# The dtype should also be correct. In this case, the loss function expects a long tensor\n",
    "\n",
    "# forward pass\n",
    "y_pred = model(x)\n",
    "\n",
    "\"\"\"\n",
    "Let's compute the loss and see what it looks like\n",
    "\"\"\"\n",
    "\n",
    "# compute the loss\n",
    "# y_pred and y should be on the same device, and their shape and dtype should be correct for the loss function (look at the documentation)\n",
    "loss = loss_fn(y_pred, y)\n",
    "print_tensor_info('loss', loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation (backward pass)\n",
    "\n",
    "Backpropagation was the difficult part in the old days. Now, it is done in one line. In general, all your calculations in PyTorch are tracked as a **computational graph**, which is a data structure used for automatic differentiation and executing backpropagation using the chain rule. All tensors that have `requires_grad=True` will have their `grad` attribute updated once you perform the backpropagation. After that, you can use the gradients to update your paramters using Stochastic Gradient Descent or any optimizer you fancy (Adam, AdamW, ...). \n",
    "\n",
    "Performing backpropagation is simple. Just call `.backward()` on the quantity that you want to take the derivate of (with respect to all parameters involved in its calculation that have `requires_grad=True`). Let's go over an example and look at a specific parameter of the model and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model[1].weight.grad before backprop\n",
      "--------------------\n",
      "It is NoneType!\n",
      "====================\n",
      "\n",
      "model[1].weight.grad after backprop\n",
      "--------------------\n",
      "shape: torch.Size([10, 256])\n",
      "dtype: torch.float32\n",
      "device: cuda:0\n",
      "requires_grad: False\n",
      "====================\n",
      "\n",
      "model[1].weight\n",
      "--------------------\n",
      "shape: torch.Size([10, 256])\n",
      "dtype: torch.float32\n",
      "device: cuda:0\n",
      "requires_grad: True\n",
      "====================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "##################################################\n",
    "# Components:\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(\n",
    "        start_dim=1, # because dim=0 is the batch dimension\n",
    "    ),\n",
    "    nn.Linear(16*16, 10)\n",
    ").to(Device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "##################################################\n",
    "# Training:\n",
    "\n",
    "x, y = next(iter(train_loader))\n",
    "x = x.to(device=Device, dtype=torch.float32)\n",
    "y = y.to(device=Device, dtype=torch.long)\n",
    "\n",
    "# forward pass\n",
    "y_pred = model(x)\n",
    "\n",
    "# compute the loss\n",
    "loss = loss_fn(y_pred, y)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Let's do backprop and see what happens!\n",
    "\"\"\"\n",
    "\n",
    "# Since the backpropagation is not done yet, the gradients are None\n",
    "print_tensor_info('model[1].weight.grad before backprop', model[1].weight.grad)\n",
    "\n",
    "# Let's do the backpropagation\n",
    "loss.backward() # this simple line takes care of the backpropagation! Cool, right?\n",
    "\n",
    "# Now let's look at the gradient\n",
    "print_tensor_info('model[1].weight.grad after backprop', model[1].weight.grad)\n",
    "\n",
    "# Let's look at the parameter itself:\n",
    "print_tensor_info('model[1].weight', model[1].weight)\n",
    "\n",
    "# grad is the same shape as the parameter itself! and the same dtype and device!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## update parameters using the gradients\n",
    "\n",
    "After you have performed the backpropagation (also called backward pass), you can use the gradients to update the parameters. This is done using the `optim` class in PyTorch. To use an optimizer, you have to give it the parameters it has to optimize, as well as optional optimizer hyperparameters. You can find more about all different optimizers and their configurations and how to use them [here](https://pytorch.org/docs/stable/optim.html). For now, let's use the simple stochastic gradient descent (SGD) as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both did the same thing!\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "# Components\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(\n",
    "        start_dim=1, # because dim=0 is the batch dimension\n",
    "    ),\n",
    "    nn.Linear(16*16, 10)\n",
    ").to(Device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "\"\"\"\n",
    "The final component is the optimizer. Let's use SGD (normal stochastic gradient descent).\n",
    "\"\"\"\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(), # pass the parameters of the model to the optimizer\n",
    "    lr = 0.001, # Optional: learning rate. Default is 0.001 (can be found in the documentation)\n",
    ")\n",
    "\n",
    "####################################################\n",
    "# Training\n",
    "\n",
    "x, y = next(iter(train_loader))\n",
    "x = x.to(device=Device, dtype=torch.float32)\n",
    "y = y.to(device=Device, dtype=torch.long)\n",
    "\n",
    "# forward pass\n",
    "y_pred = model(x)\n",
    "\n",
    "# compute the loss\n",
    "loss = loss_fn(y_pred, y)\n",
    "\n",
    "# backward pass\n",
    "loss.backward() \n",
    "\n",
    "\"\"\"\n",
    "Now it's time to update the parameters using SGD.\n",
    "The optimizer has a method called step() that does the update for you.\n",
    "To show you what happens, we'll also do the update manually and compare the results.\n",
    "\"\"\"\n",
    "\n",
    "# let's choose a certain weight and implement the update manually. Then we'll compare the results!\n",
    "\n",
    "# MANUALLY:\n",
    "# We should use .clone() to not point to the actual tensors and create a copy instead!\n",
    "w_before = model[1].weight.clone()\n",
    "w_before_grad = model[1].weight.grad.clone()\n",
    "\n",
    "# The manual gradient step\n",
    "w_after = w_before - 0.001 * w_before_grad\n",
    "\n",
    "# Now let's do the update using the optimizer.\n",
    "optimizer.step()\n",
    "\n",
    "# Now let's compare the results\n",
    "if torch.allclose(w_after, model[1].weight):\n",
    "    print('Both did the same thing!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting everything together\n",
    "\n",
    "Now that we have everything we need, let's define our model and write a full training epoch. Some new things are added here. Look carefully to spot them and understand what they do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(\n",
    "        start_dim=1, # because dim=0 is the batch dimension\n",
    "    ),\n",
    "    nn.Linear(16*16, 10)\n",
    ").to(Device)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size = 8, # let's use a small batch size\n",
    "    shuffle = True,\n",
    "    drop_last = True,\n",
    ")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "optimizer = optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr = 0.001,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a train_epoch function that trains the model for one epoch\n",
    "# An epoch is a full pass over the entire dataset\n",
    "\n",
    "@torch.enable_grad()\n",
    "def train_epoch(\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        loss_fn: callable,\n",
    "        optimizer: optim.Optimizer,\n",
    "        device: str,\n",
    "        ) -> float:\n",
    "\n",
    "    # Always set the model to train mode first.\n",
    "    # Some layers behave differently in train and test mode.\n",
    "    # You will learn about these layers later (example: dropout, batchnorm)\n",
    "    model.train()\n",
    "\n",
    "    # define a progress bar using tqdm:\n",
    "    batch_pbar = tqdm(\n",
    "        train_loader, # The iterable to loop over\n",
    "        desc = 'training', # the description of the progress bar\n",
    "        unit = 'batch', # the unit to display\n",
    "        leave = True, # if you want the progress bar to disappear after it's done, set leave=False.\n",
    "        )\n",
    "\n",
    "    # Loop over the batches using the progress bar\n",
    "    for x, y in batch_pbar:\n",
    "        # Move the data to the device and dtype\n",
    "        x = x.to(device=device, dtype=torch.float32)\n",
    "        y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "        # now we have to clear the gradients of all trainable tensors\n",
    "        # Why: because the gradients are accumulated for parameters that are used several times\n",
    "        # (example: recurrent neural networks)\n",
    "        # If you don't clear them, they will accumulate and affect the next batch\n",
    "        optimizer.zero_grad()\n",
    "        # you can also call .zero_grad() on the model itself\n",
    "\n",
    "        # forward pass\n",
    "        y_pred = model(x)\n",
    "\n",
    "        # compute the loss\n",
    "        loss = loss_fn(y_pred, y)\n",
    "\n",
    "        # backward pass (backpropagation)\n",
    "        loss.backward()\n",
    "\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # update the progress bar\n",
    "        # loss.item() returns the scalar value of a dimension 0 tensor as a Python number\n",
    "        # We can report the live loss value in the progress bar\n",
    "        # This is very useful to see if the model is converging\n",
    "        batch_pbar.set_postfix_str(f'loss={loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 911/911 [00:04<00:00, 191.92batch/s, loss=1.9583]\n"
     ]
    }
   ],
   "source": [
    "# Let's ry to train the model for an epoch:\n",
    "train_epoch(model, train_loader, loss_fn, optimizer, Device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is one epoch of training. Usually, training is done for a certain number of epochs, or a certain number of iterations (calls to `optimizer.step()`), which may be more fair when we want to compare models that were trained on different portions of the data. The same number of epochs would not be fair, but the same number of iterations give both models an equal opportunity to do their best with the avaialbe data to them to be trained.\n",
    "\n",
    "Usually, you need additional code to keep track of your training loss to see if the model is converging! You also should keep track of your validation loss to look out for overfitting and tuning your hyperparameters! You will learn more in this week's assignment!\n",
    "\n",
    "\n",
    "Finally, let's write a full loop that trains the model for multiple epochs. We also introduce another tool here, the learning rate scheduler. You will get to explore different optimizers and learning rate schedulers in your assignments. You will get more familiar with different optimizers next week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "        model: nn.Module,\n",
    "        train_loader: DataLoader,\n",
    "        loss_fn: callable,\n",
    "        optimizer: optim.Optimizer,\n",
    "        lr_scheduler: optim.lr_scheduler._LRScheduler,\n",
    "        device: str,\n",
    "        n_epochs: int,\n",
    "        ) -> None:\n",
    "    \n",
    "    epoch_pbar = tqdm(\n",
    "        range(n_epochs),\n",
    "        desc = 'training',\n",
    "        unit = 'epoch',\n",
    "        leave = False,\n",
    "        )\n",
    "    \n",
    "    for epoch in epoch_pbar:\n",
    "\n",
    "        running_avg_train_loss = 0.0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "\n",
    "            x = x.to(device=device, dtype=torch.float32)\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_avg_train_loss += loss.item() * x.shape[0]\n",
    "\n",
    "        running_avg_train_loss /= len(train_loader.dataset)\n",
    "\n",
    "        if isinstance(lr_scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            lr_scheduler.step(running_avg_train_loss)\n",
    "        elif lr_scheduler is not None:\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        epoch_pbar.set_postfix_str(f'running avg train loss = {running_avg_train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                             \r"
     ]
    }
   ],
   "source": [
    "# define the model, loss function, optimizer, and learning rate scheduler\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Flatten(start_dim=1),\n",
    "    nn.Linear(16*16, 10),\n",
    ").to(Device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='mean')\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
    "# optimizer = optim.AdamW(model.parameters(), lr=0.004)\n",
    "\n",
    "lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "# lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "# lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 20, 30], gamma=0.1)\n",
    "\n",
    "# train the model for 100 epochs\n",
    "train(\n",
    "    model = model,\n",
    "    train_loader = train_loader,\n",
    "    loss_fn = loss_fn,\n",
    "    optimizer = optimizer,\n",
    "    lr_scheduler = lr_scheduler,\n",
    "    device = Device,\n",
    "    n_epochs = 20,\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
